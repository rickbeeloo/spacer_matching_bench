{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tool results comparison on IMG/VR4 (subsampled)\n",
    "Following up on the prep work in [`subsample_prep.ipynb`](./subsample_prep.ipynb), and [scaling_cheack_imgvr4_subsamples.ipynb](scaling_cheack_imgvr4_subsamples.ipynb), here we test the different aligners/search outputs. \n",
    "Unlike the similar notebook for simulated data, here we do not have a \"ground truth\" (we can't tell false and true positives (spacer-protospacer pairs) are not real or spurious).  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will use the 5% sample size - the largest for which all tools finished (sassy and indelfree_bruteforce timed out for the 10% sample size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %load_ext autoreload\n",
    "# %autoreload 2\n",
    "import os\n",
    "os.chdir('/clusterfs/jgi/scratch/science/metagen/neri/code/blits/spacer_bench/')\n",
    "from src.bench.utils.functions import *\n",
    "from src.bench.commands.generate_scripts import load_tool_configs\n",
    "import matplotlib.pyplot as plt\n",
    "import upsetplot as up\n",
    "import numpy as np\n",
    "import polars as pl\n",
    "import json\n",
    "pl.Config(tbl_rows=50)\n",
    "\n",
    "# need to disable future deprecation warrnings\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\", category=FutureWarning)\n",
    "\n",
    "from bench import *\n",
    "from bench.utils.functions import *\n",
    "\n",
    "TOOL_STYLES = json.load(open('notebooks/tool_styles.json', 'r'))\n",
    "MAX_MISMATCHES = 3\n",
    "base_dir = \"/clusterfs/jgi/scratch/science/metagen/neri/code/blits/spacer_bench/results/real_data/subsamples/fraction_0.001/\"\n",
    "spacers_file = \"/clusterfs/jgi/scratch/science/metagen/neri/code/blits/spacer_bench/imgvr4_data/spacers/All_CRISPR_spacers_nr_clean.fna\"\n",
    "contigs_file=\"/clusterfs/jgi/scratch/science/metagen/neri/code/blits/spacer_bench/results/real_data/subsamples/fraction_0.001/subsampled_data/subsampled_contigs.fa\"\n",
    "threads = 12\n",
    "spacers = read_fasta(spacers_file)\n",
    "spacer_lendf = pl.DataFrame({\"spacer_id\": spacers.keys(), \"length\": [len(seq) for seq in spacers.values()]})\n",
    "tools = load_tool_configs(\n",
    "    results_dir=base_dir,\n",
    "    contigs_file=contigs_file,\n",
    "    spacers_file=spacers_file,\n",
    "    threads=12)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we'll read all tool results, removing entries of unmmaped contigs or matches with more than 3 mismatches. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# results_df = read_results(\n",
    "#     tools=tools,\n",
    "#     max_mismatches=MAX_MISMATCHES+1, #tool reported, not validated for the scalling tests\n",
    "#     spacer_lendf=spacer_lendf,\n",
    "#     ref_file=contigs_file,\n",
    "#     threads=18,\n",
    "#     memory_limit=\"150GB\",\n",
    "#     output_parquet='results/real_data/subsamples_analysis/alignments_fraction_0.001.parquet'\n",
    "# )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## DuckDB-Based Workflow (Memory Efficient)\n",
    "Instead of loading the full dataset into memory and then validating, we'll use a streaming approach:\n",
    "1. Extract unique regions via DuckDB (streaming)\n",
    "2. Validate sequences in batches (controlled memory)\n",
    "3. Join back and filter via DuckDB (streaming)\n",
    "\n",
    "This avoids OOM errors by never loading the full dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "[Streaming Mismatch Recalculation]\n",
      "  Input: results/real_data/subsamples_analysis/alignments_fraction_0.001.parquet\n",
      "  Output: results/real_data/subsamples_analysis/alignments_fraction_0.001_validated.parquet\n",
      "  Batch size: 10,000,000\n",
      "  Memory limit: 150GB\n",
      "\n",
      "[Step 1/4] Extracting unique regions via DuckDB...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "362e4954e5c448f2a9c6d7b417e54e6e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, layout=Layout(width='auto'), style=ProgressStyle(bar_color='black'))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Found 975,312,127 unique regions\n",
      "\n",
      "[Step 2/4] Populating sequences in batches...\n",
      "  Processing batch 1/98 (offset=0)...\n",
      "Initial pldf shape: (10000000, 5)\n",
      "Unique entries in minipldf: (488005, 1)\n",
      "After filtering nulls: (488005, 1)\n",
      "Actual number of sequences in file: 3882812\n",
      "\n",
      "Processing chunk 500000/3882812\n",
      "Number of sequences in chunk: 500000\n",
      "Joining with nascent df\n",
      "Null count in seqcol after chunk: 425074\n",
      "\n",
      "Processing chunk 1000000/3882812\n",
      "Number of sequences in chunk: 500000\n",
      "Joining with nascent df\n",
      "Null count in seqcol after chunk: 361447\n",
      "\n",
      "Processing chunk 1500000/3882812\n",
      "Number of sequences in chunk: 500000\n",
      "Joining with nascent df\n",
      "Null count in seqcol after chunk: 303129\n",
      "\n",
      "Processing chunk 2000000/3882812\n",
      "Number of sequences in chunk: 500000\n",
      "Joining with nascent df\n",
      "Null count in seqcol after chunk: 240745\n",
      "\n",
      "Processing chunk 2500000/3882812\n",
      "Number of sequences in chunk: 500000\n",
      "Joining with nascent df\n",
      "Null count in seqcol after chunk: 177635\n",
      "\n",
      "Processing chunk 3000000/3882812\n",
      "Number of sequences in chunk: 500000\n",
      "Joining with nascent df\n",
      "Null count in seqcol after chunk: 115434\n",
      "\n",
      "Processing chunk 3500000/3882812\n",
      "Number of sequences in chunk: 500000\n",
      "Joining with nascent df\n",
      "Null count in seqcol after chunk: 46646\n",
      "\n",
      "Processing chunk 3882812/3882812\n",
      "Number of sequences in chunk: 382812\n",
      "Joining with nascent df\n",
      "Null count in seqcol after chunk: 0\n",
      "\n",
      "Final merge with original df\n",
      "Final null count in seqcol: 0\n",
      "Initial pldf shape: (10000000, 6)\n",
      "Unique entries in minipldf: (9334232, 4)\n",
      "After filtering nulls: (9334232, 4)\n",
      "Actual number of sequences in file: 808\n",
      "\n",
      "Processing chunk 808/808\n",
      "Number of sequences in chunk: 808\n",
      "Trimming sequences\n",
      "shape: (10,)\n",
      "Series: 'contig_seq' [str]\n",
      "[\n",
      "\t\"CATCTTTTGATATAGATCGATATGC\"\n",
      "\t\"AGCGGCGCCATGGTCTCTTGGCCTGCTGC\"\n",
      "\t\"GGCTGCAGCCGCATAGGTCACGCTTTCAT\"\n",
      "\t\"CCCTTTGGCGCCCAGTCGCCGCTGCGCTAC\"\n",
      "\t\"GTATGGCAACCTTTTTTTGTTGCCTTTTGGT\"\n",
      "\t\"GTCCCTTTTTTTTTCCGTCGCTATGGGCTTT\"\n",
      "\t\"CGTCTCTATCGTGGTGGTGGTCATGACAGTC\"\n",
      "\t\"GTATCTTGTGGCGCCGATTCCTGCGACACCG\"\n",
      "\t\"CCATCGACTCGATGCTGGTCAACGTCGCCTG\"\n",
      "\t\"GTCGGGCACCCAGGCGAGGATGGTCGCCACC\"\n",
      "]\n",
      "Reversing sequences\n",
      "Joining with nascent df\n",
      "Null count in seqcol after chunk: 0\n",
      "\n",
      "Final merge with original df\n",
      "Final null count in seqcol: 0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "parasail_nw_trace: s2Len must be > 0\n",
      "parasail_result_is_trace: missing result\n",
      "parasail_nw_trace: s2Len must be > 0\n",
      "parasail_result_is_trace: missing result\n",
      "parasail_nw_trace: s2Len must be > 0\n",
      "parasail_result_is_trace: missing result\n",
      "parasail_nw_trace: s2Len must be > 0\n",
      "parasail_result_is_trace: missing result\n",
      "parasail_nw_trace: s2Len must be > 0\n",
      "parasail_result_is_trace: missing result\n",
      "parasail_nw_trace: s2Len must be > 0\n",
      "parasail_result_is_trace: missing result\n",
      "parasail_nw_trace: s2Len must be > 0\n",
      "parasail_result_is_trace: missing result\n",
      "parasail_nw_trace: s2Len must be > 0\n",
      "parasail_result_is_trace: missing result\n",
      "parasail_nw_trace: s2Len must be > 0\n",
      "parasail_result_is_trace: missing result\n",
      "parasail_nw_trace: s2Len must be > 0\n",
      "parasail_result_is_trace: missing result\n",
      "parasail_nw_trace: s2Len must be > 0\n",
      "parasail_result_is_trace: missing result\n",
      "parasail_nw_trace: s2Len must be > 0\n",
      "parasail_result_is_trace: missing result\n",
      "parasail_nw_trace: s2Len must be > 0\n",
      "parasail_result_is_trace: missing result\n",
      "parasail_nw_trace: s2Len must be > 0\n",
      "parasail_result_is_trace: missing result\n",
      "parasail_nw_trace: s2Len must be > 0\n",
      "parasail_result_is_trace: missing result\n",
      "parasail_nw_trace: s2Len must be > 0\n",
      "parasail_result_is_trace: missing result\n",
      "parasail_nw_trace: s2Len must be > 0\n",
      "parasail_result_is_trace: missing result\n",
      "parasail_nw_trace: s2Len must be > 0\n",
      "parasail_result_is_trace: missing result\n",
      "parasail_nw_trace: s2Len must be > 0\n",
      "parasail_result_is_trace: missing result\n",
      "parasail_nw_trace: s2Len must be > 0\n",
      "parasail_result_is_trace: missing result\n",
      "parasail_nw_trace: s2Len must be > 0\n",
      "parasail_result_is_trace: missing result\n",
      "parasail_nw_trace: s2Len must be > 0\n",
      "parasail_result_is_trace: missing result\n",
      "parasail_nw_trace: s2Len must be > 0\n",
      "parasail_result_is_trace: missing result\n",
      "parasail_nw_trace: s2Len must be > 0\n",
      "parasail_result_is_trace: missing result\n",
      "parasail_nw_trace: s2Len must be > 0\n",
      "parasail_result_is_trace: missing result\n",
      "parasail_nw_trace: s2Len must be > 0\n",
      "parasail_result_is_trace: missing result\n",
      "parasail_nw_trace: s2Len must be > 0\n",
      "parasail_result_is_trace: missing result\n",
      "parasail_nw_trace: s2Len must be > 0\n",
      "parasail_result_is_trace: missing result\n",
      "parasail_nw_trace: s2Len must be > 0\n",
      "parasail_result_is_trace: missing result\n",
      "parasail_nw_trace: s2Len must be > 0\n",
      "parasail_result_is_trace: missing result\n",
      "parasail_nw_trace: s2Len must be > 0\n",
      "parasail_result_is_trace: missing result\n",
      "parasail_nw_trace: s2Len must be > 0\n",
      "parasail_result_is_trace: missing result\n",
      "parasail_result_free: attempted free of NULL result pointer\n",
      "parasail_result_free: attempted free of NULL result pointer\n",
      "parasail_result_free: attempted free of NULL result pointer\n",
      "parasail_result_free: attempted free of NULL result pointer\n",
      "parasail_result_free: attempted free of NULL result pointer\n",
      "parasail_result_free: attempted free of NULL result pointer\n",
      "parasail_result_free: attempted free of NULL result pointer\n",
      "parasail_result_free: attempted free of NULL result pointer\n",
      "parasail_result_free: attempted free of NULL result pointer\n",
      "parasail_result_free: attempted free of NULL result pointer\n",
      "parasail_result_free: attempted free of NULL result pointer\n",
      "parasail_result_free: attempted free of NULL result pointer\n",
      "parasail_result_free: attempted free of NULL result pointer\n",
      "parasail_result_free: attempted free of NULL result pointer\n",
      "parasail_result_free: attempted free of NULL result pointer\n",
      "parasail_result_free: attempted free of NULL result pointer\n",
      "parasail_result_free: attempted free of NULL result pointer\n",
      "parasail_result_free: attempted free of NULL result pointer\n",
      "parasail_result_free: attempted free of NULL result pointer\n",
      "parasail_result_free: attempted free of NULL result pointer\n",
      "parasail_result_free: attempted free of NULL result pointer\n",
      "parasail_result_free: attempted free of NULL result pointer\n",
      "parasail_result_free: attempted free of NULL result pointer\n",
      "parasail_result_free: attempted free of NULL result pointer\n",
      "parasail_result_free: attempted free of NULL result pointer\n",
      "parasail_result_free: attempted free of NULL result pointer\n",
      "parasail_result_free: attempted free of NULL result pointer\n",
      "parasail_result_free: attempted free of NULL result pointer\n",
      "parasail_result_free: attempted free of NULL result pointer\n",
      "parasail_result_free: attempted free of NULL result pointer\n",
      "parasail_result_free: attempted free of NULL result pointer\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Removed temp directory: results/real_data/subsamples_analysis/mismatch_recalc_temp\n"
     ]
    },
    {
     "ename": "AttributeError",
     "evalue": "'Result' object has no traceback",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_1259929/3218106005.py\u001b[0m in \u001b[0;36m?\u001b[0;34m()\u001b[0m\n\u001b[0;32m---> 11\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0msrc\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbench\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mutils\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfunctions\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mrecalculate_mismatches_streaming\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     12\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     13\u001b[0m \u001b[0;31m# This replaces all the manual steps of:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     14\u001b[0m \u001b[0;31m# 1. Extracting unique regions\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/clusterfs/jgi/scratch/science/metagen/neri/code/blits/spacer_bench/src/bench/utils/functions.py\u001b[0m in \u001b[0;36m?\u001b[0;34m(parquet_path, spacers_file, contigs_file, output_parquet, max_mismatches, batch_size, threads, memory_limit, ignore_region_strands)\u001b[0m\n\u001b[1;32m   3206\u001b[0m         \u001b[0;31m# Cleanup temp files\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3207\u001b[0m         \u001b[0;32mimport\u001b[0m \u001b[0mshutil\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3208\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexists\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtemp_dir\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3209\u001b[0m             \u001b[0mshutil\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrmtree\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtemp_dir\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 3210\u001b[0;31m             \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"  Removed temp directory: {temp_dir}\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   3211\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3212\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"\\n[Complete] Results written to: {output_parquet}\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3213\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/clusterfs/jgi/scratch/science/metagen/neri/code/blits/spacer_bench/src/bench/utils/functions.py\u001b[0m in \u001b[0;36m?\u001b[0;34m(results, ignore_region_strands, **kwargs)\u001b[0m\n\u001b[1;32m   2988\u001b[0m     \"\"\"\n\u001b[1;32m   2989\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0;34m\"spacer_seq\"\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mresults\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcolumns\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0;34m\"contig_seq\"\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mresults\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcolumns\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2990\u001b[0m         \u001b[0;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"spacer_seq and contig_seq columns are required\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2991\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2992\u001b[0;31m     results = results.with_columns(\n\u001b[0m\u001b[1;32m   2993\u001b[0m         pl.struct(\n\u001b[1;32m   2994\u001b[0m             \u001b[0mpl\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcol\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"spacer_seq\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2995\u001b[0m             \u001b[0mpl\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcol\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"contig_seq\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/clusterfs/jgi/scratch/science/metagen/neri/code/blits/spacer_bench/.pixi/envs/default/lib/python3.9/site-packages/polars/dataframe/frame.py\u001b[0m in \u001b[0;36m?\u001b[0;34m(self, *exprs, **named_exprs)\u001b[0m\n\u001b[1;32m  10310\u001b[0m         \"\"\"\n\u001b[1;32m  10311\u001b[0m         \u001b[0;32mfrom\u001b[0m \u001b[0mpolars\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlazyframe\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mopt_flags\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mQueryOptFlags\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m  10312\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m  10313\u001b[0m         return (\n\u001b[0;32m> 10314\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlazy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m  10315\u001b[0m             \u001b[0;34m.\u001b[0m\u001b[0mwith_columns\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mexprs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mnamed_exprs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m  10316\u001b[0m             \u001b[0;34m.\u001b[0m\u001b[0mcollect\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moptimizations\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mQueryOptFlags\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_eager\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m  10317\u001b[0m         )\n",
      "\u001b[0;32m/clusterfs/jgi/scratch/science/metagen/neri/code/blits/spacer_bench/.pixi/envs/default/lib/python3.9/site-packages/polars/_utils/deprecation.py\u001b[0m in \u001b[0;36m?\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     93\u001b[0m                     \u001b[0mkwargs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"engine\"\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m\"in-memory\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     94\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     95\u001b[0m                 \u001b[0;32mdel\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"streaming\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     96\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 97\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mfunction\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m/clusterfs/jgi/scratch/science/metagen/neri/code/blits/spacer_bench/.pixi/envs/default/lib/python3.9/site-packages/polars/lazyframe/opt_flags.py\u001b[0m in \u001b[0;36m?\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    324\u001b[0m                     \u001b[0missue_warning\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmessage\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mDeprecationWarning\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    325\u001b[0m                     \u001b[0moptflags\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcb\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moptflags\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpop\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[no-untyped-call,unused-ignore]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    326\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    327\u001b[0m             \u001b[0mkwargs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"optimizations\"\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0moptflags\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 328\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mfunction\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m/clusterfs/jgi/scratch/science/metagen/neri/code/blits/spacer_bench/.pixi/envs/default/lib/python3.9/site-packages/polars/lazyframe/frame.py\u001b[0m in \u001b[0;36m?\u001b[0;34m(self, type_coercion, predicate_pushdown, projection_pushdown, simplify_expression, slice_pushdown, comm_subplan_elim, comm_subexpr_elim, cluster_with_columns, collapse_joins, no_optimization, engine, background, optimizations, **_kwargs)\u001b[0m\n\u001b[1;32m   2425\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mInProcessQuery\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mldf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcollect_concurrently\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2426\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2427\u001b[0m         \u001b[0;31m# Only for testing purposes\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2428\u001b[0m         \u001b[0mcallback\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_kwargs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"post_opt_callback\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcallback\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2429\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mwrap_df\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mldf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcollect\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mengine\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcallback\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m/clusterfs/jgi/scratch/science/metagen/neri/code/blits/spacer_bench/.pixi/envs/default/lib/python3.9/site-packages/polars/functions/lazy.py\u001b[0m in \u001b[0;36m?\u001b[0;34m(self, sl, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1090\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0;34m\"unexpected keyword argument 'return_dtype'\"\u001b[0m \u001b[0;32min\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1091\u001b[0m                 \u001b[0mkwargs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpop\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"return_dtype\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1092\u001b[0m                 \u001b[0mrv\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfunction\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mslp\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1093\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1094\u001b[0;31m                 \u001b[0;32mraise\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1095\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1096\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0m_check_for_numpy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrv\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrv\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mndarray\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1097\u001b[0m             \u001b[0mrv\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpl\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mSeries\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrv\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mreturn_dtype\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/clusterfs/jgi/scratch/science/metagen/neri/code/blits/spacer_bench/.pixi/envs/default/lib/python3.9/site-packages/polars/expr/expr.py\u001b[0m in \u001b[0;36m?\u001b[0;34m(sl, *args, **kwargs)\u001b[0m\n\u001b[1;32m   4654\u001b[0m         \u001b[0;32mdef\u001b[0m \u001b[0m_wrap\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msl\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mSequence\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mpl\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mSeries\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mAny\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mAny\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mpl\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mSeries\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 4655\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mfunction\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msl\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m/clusterfs/jgi/scratch/science/metagen/neri/code/blits/spacer_bench/.pixi/envs/default/lib/python3.9/site-packages/polars/expr/expr.py\u001b[0m in \u001b[0;36m?\u001b[0;34m(x, **kwargs)\u001b[0m\n\u001b[1;32m   4875\u001b[0m                 \u001b[0mreturn_dtype\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"return_dtype\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   4876\u001b[0m                 \u001b[0;32mwith\u001b[0m \u001b[0mwarnings\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcatch_warnings\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   4877\u001b[0m                     \u001b[0mwarnings\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msimplefilter\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"ignore\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mPolarsInefficientMapWarning\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   4878\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 4879\u001b[0;31m                     return x.map_elements(\n\u001b[0m\u001b[1;32m   4880\u001b[0m                         \u001b[0mfunction\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreturn_dtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mreturn_dtype\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mskip_nulls\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mskip_nulls\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   4881\u001b[0m                     )\n",
      "\u001b[0;32m/clusterfs/jgi/scratch/science/metagen/neri/code/blits/spacer_bench/.pixi/envs/default/lib/python3.9/site-packages/polars/series/series.py\u001b[0m in \u001b[0;36m?\u001b[0;34m(self, function, return_dtype, skip_nulls)\u001b[0m\n\u001b[1;32m   5834\u001b[0m             \u001b[0mpl_return_dtype\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mparse_into_dtype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mreturn_dtype\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   5835\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   5836\u001b[0m         \u001b[0mwarn_on_inefficient_map\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfunction\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcolumns\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmap_target\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"series\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   5837\u001b[0m         return self._from_pyseries(\n\u001b[0;32m-> 5838\u001b[0;31m             self._s.map_elements(\n\u001b[0m\u001b[1;32m   5839\u001b[0m                 \u001b[0mfunction\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreturn_dtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mpl_return_dtype\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mskip_nulls\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mskip_nulls\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   5840\u001b[0m             )\n\u001b[1;32m   5841\u001b[0m         )\n",
      "\u001b[0;32m/clusterfs/jgi/scratch/science/metagen/neri/code/blits/spacer_bench/src/bench/utils/functions.py\u001b[0m in \u001b[0;36m?\u001b[0;34m(x)\u001b[0m\n\u001b[0;32m-> 3001\u001b[0;31m             \u001b[0;32mlambda\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mtest_row\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mignore_region_strands\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mignore_region_strands\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m/clusterfs/jgi/scratch/science/metagen/neri/code/blits/spacer_bench/src/bench/utils/functions.py\u001b[0m in \u001b[0;36m?\u001b[0;34m(row, ignore_region_strands)\u001b[0m\n\u001b[1;32m   2964\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mtest_row\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrow\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mignore_region_strands\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2965\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mignore_region_strands\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2966\u001b[0;31m         return test_alignment(\n\u001b[0m\u001b[1;32m   2967\u001b[0m             \u001b[0mrow\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"spacer_seq\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2968\u001b[0m             \u001b[0mrow\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"contig_seq\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2969\u001b[0m             \u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/clusterfs/jgi/scratch/science/metagen/neri/code/blits/spacer_bench/src/bench/utils/functions.py\u001b[0m in \u001b[0;36m?\u001b[0;34m(spacer_seq, contig_seq, strand, start, end, gap_cost, extend_cost, gaps_as_mismatch, **kwargs)\u001b[0m\n\u001b[1;32m   2839\u001b[0m     \u001b[0;31m# '|' = match\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2840\u001b[0m     \u001b[0;31m# '.' = mismatch (substitution)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2841\u001b[0m     \u001b[0;31m# ' ' (space) = gap/indel\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2842\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2843\u001b[0;31m     \u001b[0mcomp_string\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0malignment\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtraceback\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcomp\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2844\u001b[0m     \u001b[0mmatches\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcomp_string\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcount\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"|\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2845\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2846\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mgaps_as_mismatch\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/clusterfs/jgi/scratch/science/metagen/neri/code/blits/spacer_bench/.pixi/envs/default/lib/python3.9/site-packages/parasail/bindings_v2.py\u001b[0m in \u001b[0;36m?\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    420\u001b[0m     \u001b[0;34m@\u001b[0m\u001b[0mproperty\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    421\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mtraceback\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 422\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_traceback\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m/clusterfs/jgi/scratch/science/metagen/neri/code/blits/spacer_bench/.pixi/envs/default/lib/python3.9/site-packages/parasail/bindings_v2.py\u001b[0m in \u001b[0;36m?\u001b[0;34m(self, mch, sim, neg, case_sensitive, alphabet_aliases)\u001b[0m\n\u001b[1;32m    404\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mget_traceback\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmch\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'|'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msim\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m':'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mneg\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'.'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcase_sensitive\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0malphabet_aliases\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    405\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0;36m0\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0m_lib\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mparasail_result_is_trace\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpointer\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 406\u001b[0;31m             \u001b[0;32mraise\u001b[0m \u001b[0mAttributeError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"'Result' object has no traceback\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    407\u001b[0m         \u001b[0mcase\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcase_sensitive\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0m_case_sensitive\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    408\u001b[0m         \u001b[0malias\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0malphabet_aliases\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0m_alphabet_aliases\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    409\u001b[0m         \u001b[0margs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m''\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mmch\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0msim\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mneg\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcase\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0malias\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'Result' object has no traceback"
     ]
    }
   ],
   "source": [
    "from src.bench.utils.functions import recalculate_mismatches_streaming\n",
    "\n",
    "# This replaces all the manual steps of:\n",
    "# 1. Extracting unique regions\n",
    "# 2. Populating spacer sequences  \n",
    "# 3. Populating contig sequences\n",
    "# 4. Running test_alignment_polars\n",
    "# 5. Joining back and filtering\n",
    "\n",
    "# Instead, everything is done in batches without loading the full dataset\n",
    "recalculate_mismatches_streaming(\n",
    "    parquet_path='results/real_data/subsamples_analysis/alignments_fraction_0.001.parquet',\n",
    "    spacers_file=spacers_file,\n",
    "    contigs_file=contigs_file,\n",
    "    output_parquet='results/real_data/subsamples_analysis/alignments_fraction_0.001_validated.parquet',\n",
    "    max_mismatches=3,  # Only keep alignments with ≤3 mismatches after recalculation\n",
    "    batch_size=10000000,  # Process 10M unique regions at a time (adjust based on memory)\n",
    "    threads=18,\n",
    "    memory_limit=\"150GB\",\n",
    "    ignore_region_strands=True\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load Results for Analysis (DuckDB)\n",
    "Now we set up a DuckDB connection to query the validated results without loading into memory."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import duckdb\n",
    "\n",
    "# Create DuckDB connection for querying without loading into memory\n",
    "con = duckdb.connect(database=\":memory:\")\n",
    "con.execute(\"SET threads TO 12;\")\n",
    "con.execute(\"SET memory_limit = '100GB';\")\n",
    "\n",
    "# Get tool list\n",
    "tools_list = con.execute(\"\"\"\n",
    "    SELECT DISTINCT tool \n",
    "    FROM read_parquet('results/real_data/subsamples_analysis/alignments_fraction_0.001_validated.parquet')\n",
    "    ORDER BY tool\n",
    "\"\"\").pl()['tool'].to_list()\n",
    "\n",
    "print(f\"Tools: {tools_list}\")\n",
    "\n",
    "# Create a view with renamed columns for consistency\n",
    "# 'mismatches' (tool-reported) → 'tool_reported_mismatches'\n",
    "# 'alignment_test' (parasail-validated) → 'mismatches'\n",
    "con.execute(\"\"\"\n",
    "    CREATE VIEW tools_results AS\n",
    "    SELECT \n",
    "        spacer_id,\n",
    "        contig_id,\n",
    "        strand,\n",
    "        start,\n",
    "        end,\n",
    "        tool,\n",
    "        spacer_length,\n",
    "        mismatches as tool_reported_mismatches,\n",
    "        alignment_test as mismatches,\n",
    "        spacer_seq,\n",
    "        contig_seq\n",
    "    FROM read_parquet('results/real_data/subsamples_analysis/alignments_fraction_0.001_validated.parquet')\n",
    "\"\"\")\n",
    "\n",
    "print(\"Created view: tools_results (with recalculated mismatches)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Summary Statistics (DuckDB)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.bench.utils.functions import get_summary_stats_duckdb\n",
    "\n",
    "summary_stats = get_summary_stats_duckdb(\n",
    "    'results/real_data/subsamples_analysis/alignments_fraction_0.001_validated.parquet',\n",
    "    threads=12,\n",
    "    memory_limit=\"100GB\"\n",
    ")\n",
    "\n",
    "print(summary_stats)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Matched Contigs Summary (DuckDB)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "matched_contigs = con.execute(\"\"\"\n",
    "    SELECT \n",
    "        contig_id,\n",
    "        COUNT(DISTINCT spacer_id) as n_spacers,\n",
    "        LIST(DISTINCT tool) as tools,\n",
    "        COUNT(DISTINCT tool) as n_tools\n",
    "    FROM tools_results\n",
    "    GROUP BY contig_id\n",
    "    ORDER BY n_spacers DESC\n",
    "    LIMIT 10\n",
    "\"\"\").pl()\n",
    "\n",
    "print(matched_contigs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Upset Plots (DuckDB version)\n",
    "Generate upset plots using DuckDB to create the contig-tool mappings without loading full dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_mismatches = [0, 1, 2, 3]\n",
    "\n",
    "for n_mismatches in plot_mismatches:\n",
    "    print(f\"\\nProcessing upset plot for {n_mismatches} mismatches...\")\n",
    "    \n",
    "    # Use DuckDB to create contig-tools mapping (streaming, no memory load)\n",
    "    contig_tool_table = con.execute(f\"\"\"\n",
    "        SELECT \n",
    "            contig_id,\n",
    "            LIST(DISTINCT tool) as tools\n",
    "        FROM tools_results\n",
    "        WHERE mismatches = {n_mismatches}\n",
    "        GROUP BY contig_id\n",
    "        ORDER BY contig_id\n",
    "    \"\"\").pl()\n",
    "    \n",
    "    # Create upset plot from the small aggregated result\n",
    "    test_upset = up.from_memberships(contig_tool_table['tools'])\n",
    "    print(f\"n mismatches: {n_mismatches}\")\n",
    "    \n",
    "    plt.figure(figsize=(10, 6))\n",
    "    up.plot(test_upset, subset_size='count', sort_by='cardinality')\n",
    "    plt.title(f'Matches with == {n_mismatches} mismatches')\n",
    "    plt.savefig(f'results/real_data/plots/upset_{n_mismatches}.pdf')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tool Comparison Matrix (DuckDB version)\n",
    "Generate tool comparison matrices using streaming DuckDB queries."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.bench.utils.functions import create_tool_comparison_matrix_duckdb\n",
    "\n",
    "all_charts = []\n",
    "\n",
    "for n_mismatches in [0, 1, 2, 3]:\n",
    "    print(f\"\\nCreating matrix for {n_mismatches} mismatches...\")\n",
    "    \n",
    "    matrix = create_tool_comparison_matrix_duckdb(\n",
    "        parquet_path='results/real_data/subsamples_analysis/alignments_fraction_0.001_validated.parquet',\n",
    "        tools_list=tools_list,\n",
    "        n_mismatches=n_mismatches,\n",
    "        output_csv=f'results/real_data/results/matrix_{n_mismatches}.tsv',\n",
    "        threads=12,\n",
    "        memory_limit=\"100GB\"\n",
    "    )\n",
    "    \n",
    "    print(matrix)\n",
    "    \n",
    "    # Create heatmap\n",
    "    heatmap_filename = f'results/real_data/plots/matrix_{n_mismatches}'\n",
    "    chart = plot_matrix(matrix, f\"Matrix for {n_mismatches} mismatches\", heatmap_filename)\n",
    "    all_charts.append(chart)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Spacer Counts for Recall Analysis (DuckDB version)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.bench.utils.functions import create_spacer_counts_with_tools_duckdb\n",
    "\n",
    "# Create spacer counts for recall vs occurrences analysis\n",
    "spacer_counts_with_tools = create_spacer_counts_with_tools_duckdb(\n",
    "    parquet_path='results/real_data/subsamples_analysis/alignments_fraction_0.001_validated.parquet',\n",
    "    tools_list=tools_list,\n",
    "    mismatches=3,\n",
    "    exact_or_max=\"max\",\n",
    "    threads=12,\n",
    "    memory_limit=\"100GB\"\n",
    ")\n",
    "\n",
    "print(f\"Created spacer counts: {spacer_counts_with_tools.height:,} rows\")\n",
    "print(spacer_counts_with_tools.head())\n",
    "\n",
    "# Save for later use\n",
    "spacer_counts_with_tools.write_parquet('results/real_data/results/spacer_counts_max3.parquet')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Distribution Data (DuckDB version)\n",
    "Get aggregated distribution data for plotting without loading the full dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get distributions using DuckDB (small aggregated results)\n",
    "length_distribution = con.execute(\"\"\"\n",
    "    SELECT \n",
    "        spacer_length,\n",
    "        COUNT(*) as count\n",
    "    FROM (\n",
    "        SELECT DISTINCT spacer_id, spacer_length\n",
    "        FROM tools_results\n",
    "    )\n",
    "    GROUP BY spacer_length\n",
    "    ORDER BY spacer_length\n",
    "\"\"\").pl()\n",
    "\n",
    "mismatch_distribution = con.execute(\"\"\"\n",
    "    SELECT \n",
    "        mismatches,\n",
    "        COUNT(*) as count\n",
    "    FROM (\n",
    "        SELECT DISTINCT spacer_id, contig_id, strand, start, end, mismatches\n",
    "        FROM tools_results\n",
    "    )\n",
    "    GROUP BY mismatches\n",
    "    ORDER BY mismatches\n",
    "\"\"\").pl()\n",
    "\n",
    "occurrence_distribution = con.execute(\"\"\"\n",
    "    SELECT \n",
    "        n_occurrences,\n",
    "        COUNT(*) as count\n",
    "    FROM (\n",
    "        SELECT \n",
    "            spacer_id,\n",
    "            COUNT(DISTINCT contig_id) as n_occurrences\n",
    "        FROM tools_results\n",
    "        WHERE mismatches <= 3\n",
    "        GROUP BY spacer_id\n",
    "    )\n",
    "    GROUP BY n_occurrences\n",
    "    ORDER BY n_occurrences\n",
    "\"\"\").pl()\n",
    "\n",
    "print(f\"Length distribution: {length_distribution.height} unique lengths\")\n",
    "print(f\"Mismatch distribution: {mismatch_distribution.height} mismatch levels\")\n",
    "print(f\"Occurrence distribution: {occurrence_distribution.height} occurrence levels\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tool Performance by Mismatches (DuckDB version)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate total possible matches and per-tool matches using DuckDB\n",
    "total_matches = con.execute(\"\"\"\n",
    "    SELECT \n",
    "        mismatches,\n",
    "        COUNT(*) as total_possible\n",
    "    FROM (\n",
    "        SELECT DISTINCT spacer_id, contig_id, mismatches\n",
    "        FROM tools_results\n",
    "    )\n",
    "    GROUP BY mismatches\n",
    "    ORDER BY mismatches\n",
    "\"\"\").pl()\n",
    "\n",
    "tool_matches = con.execute(\"\"\"\n",
    "    SELECT \n",
    "        mismatches,\n",
    "        tool,\n",
    "        COUNT(*) as tool_matches\n",
    "    FROM (\n",
    "        SELECT DISTINCT spacer_id, contig_id, tool, mismatches\n",
    "        FROM tools_results\n",
    "    )\n",
    "    GROUP BY mismatches, tool\n",
    "    ORDER BY mismatches, tool\n",
    "\"\"\").pl()\n",
    "\n",
    "# Create all combinations\n",
    "all_combinations = pl.DataFrame({\n",
    "    'mismatches': np.repeat(range(4), len(tools_list)),\n",
    "    'tool': tools_list * 4\n",
    "})\n",
    "\n",
    "# Calculate recall\n",
    "mismatch_performance = all_combinations\\\n",
    "    .join(total_matches, on='mismatches')\\\n",
    "    .join(tool_matches, on=['mismatches', 'tool'], how='left')\\\n",
    "    .with_columns([\n",
    "        pl.col('tool_matches').fill_null(0),\n",
    "        (pl.col('tool_matches') / pl.col('total_possible')).alias('recall')\n",
    "    ])\n",
    "\n",
    "print(mismatch_performance)\n",
    "\n",
    "# Save results\n",
    "mismatch_performance.write_csv('results/real_data/results/tool_recall_by_mismatches.tsv', separator='\\t')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Cleanup\n",
    "Close the DuckDB connection when analysis is complete."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Close DuckDB connection\n",
    "con.close()\n",
    "print(\"DuckDB connection closed\")\n",
    "print(\"\\nMemory-efficient workflow complete!\")\n",
    "print(\"All analyses performed without loading full dataset into memory.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Filter alignments using DuckDB\n",
    "Since the combined parquet file is large and causes memory issues with Polars, we'll use DuckDB to filter for mismatches <= 3 and save a smaller filtered file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Use Polars streaming mode - processes the data in chunks without loading everything into memory, cause it's too big (raw sassy tsv output is ~4.7tb...)\n",
    "# pl.scan_parquet('results/real_data/subsamples_analysis/alignments_fraction_0.001.parquet') \\\n",
    "#     .filter(pl.col('mismatches') <= 3) \\\n",
    "#     .sink_parquet(\n",
    "#         'results/real_data/subsamples_analysis/alignments_fraction_0.001_maxmis_3.parquet',\n",
    "#         compression='snappy'\n",
    "#     )\n",
    "# no need to rerun - next cell will read the filtered parquet"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we can load the filtered results using Polars (much smaller file, so memory-friendly)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To reduce memory usage and time to access matched contigs/spacers, we'll index the spacers fasta file, and also craete and index a fasta file containing only the matched contigs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tools_results = pl.scan_parquet('results/real_data/subsamples_analysis/alignments_fraction_0.001_maxmis_3.parquet')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tools_results.collect_schema()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next -  indexing the contigs fasta file for faster access."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%bash\n",
    "pyfastx index results/real_data/subsamples/fraction_0.1/subsampled_data/subsampled_contigs.fa"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### summary \n",
    "reminder - loading of pre-combined results:  \n",
    "`tools_results = pl.read_parquet('results/real_data/results/tools_results.parquet')`\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we print some summary statistics\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tools_results = tools_results.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "summary_stats = tools_results.group_by('tool').agg(\n",
    "     pl.col('mismatches').mean().alias('mean_mismatches'),\n",
    "     pl.col('spacer_id').n_unique().alias('n_spacers'),\n",
    "     pl.col('contig_id').n_unique().alias('n_contigs'),\n",
    "     pl.col('strand').value_counts().alias('strand_counts'),\n",
    "     )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Summary about the matched contigs\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "matched_contigs = tools_results.group_by('contig_id').agg(\n",
    "    pl.col('spacer_id').n_unique().alias('n_spacers'),\n",
    "    pl.col('tool').unique().alias('tools'),\n",
    "    pl.col('tool').n_unique().alias('n_tools'),\n",
    ")\n",
    "matched_contigs.sort('n_spacers',descending=True).head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Let's closely examine all the contigs that were only detected by a single tool with 0 mismatches."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "one_tool_only_n0 = tools_results.filter(pl.col('mismatches') == 0).group_by('contig_id').agg(\n",
    "    pl.col('spacer_id').n_unique().alias('n_spacers'),\n",
    "    pl.col('tool').unique().alias('tools'),\n",
    "    pl.col('tool').n_unique().alias('n_tools'),\n",
    "    ).filter(pl.col('n_tools') == 1)\n",
    "results_n0 = tools_results.filter(pl.col('mismatches') < 1).filter(pl.col('contig_id').is_in(one_tool_only_n0['contig_id']))\n",
    "results_n0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Validation/correction of the results\n",
    "To avoid relying on the tool-reported mismatches, we'll recalculate the mismatches in a consistent way.   \n",
    "To do this, we'll use the parasail library, on a set of pairs of spacer & unique aligned-to regions from contigs "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "unique_regions = tools_results.select([\"spacer_id\",\"contig_id\",\"strand\",\"start\",\"end\"]).unique()\n",
    "unique_regions.write_parquet('results/real_data/results/unique_regions.parquet')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First we'll populate the unique regions with the spacer sequences.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "unique_regions = populate_pldf_withseqs_needletail(seqfile=spacers_file, pldf=unique_regions,chunk_size=2000000, reverse_by_strand_col=False,trim_to_region=False, idcol=\"spacer_id\",seqcol=\"spacer_seq\")\n",
    "unique_regions.write_parquet('results/real_data/results/unique_regions_with_spacer_seqs.parquet')\n",
    "unique_regions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next we'll populate the unique regions with the contig sequences, this will take a while."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "contigs_file  = \"results/real_data/results/matched_contigs.fna\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "unique_regions = populate_pldf_withseqs_needletail(seqfile=contigs_file, trim_to_region=True,reverse_by_strand_col=True, chunk_size=200000, pldf=unique_regions, idcol=\"contig_id\",start_col=\"start\",end_col=\"end\",strand_col=\"strand\",seqcol=\"contig_seq\")\n",
    "unique_regions.write_parquet('results/real_data/results/unique_regions_with_contig_seqs.parquet')\n",
    "unique_regions\n",
    "### quick check to see if how many contigs were only detected by each tool with n mismatches"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next we'll use parasail to recalculate the mismatches between the spacer and the contig seqs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "unique_regions = pl.read_parquet('results/real_data/results/unique_regions_with_contig_seqs.parquet')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test3 = unique_regions\n",
    "test4 = test_alignment_polars(\n",
    "    results=test3, \n",
    "    return_deviations=False,\n",
    "    ignore_region_strands=True \n",
    ")\n",
    "test4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test4.write_parquet('results/real_data/results/unique_regions_mm_recalced.parquet')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we merge the recalculated mismatches with the original results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tools_results = tools_results.join(test4[[\"spacer_id\", \"contig_id\", \"strand\", \"start\", \"end\", \"spacer_seq\",\"contig_seq\"]], on=['spacer_id', 'contig_id', 'strand', 'start', 'end'], how='left')\n",
    "tools_results.write_parquet('results/real_data/results/tools_results_mm_recalced.parquet')\n",
    "tools_results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next we'll test the reported alignments, and further filter them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tools_results = tools_results.with_columns((\n",
    "    pl.col(\"alignment_test\") -  pl.col(\"mismatches\")  ).alias(\"deviation\")\n",
    ")\n",
    "\n",
    "deviated_rows = tools_results.filter(pl.col(\"deviation\") != 0)# .filter(pl.col(\"tool\") == \"vsearch\")\n",
    "deviated_rows = deviated_rows.sort(\"deviation\", descending=False)\n",
    "deviated_rows"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "maybe this is a tool parsing issue?, let's get the frequency of deviations per tool\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "deviation_counts = deviated_rows.group_by(\"tool\").agg(\n",
    "    pl.col(\"deviation\").count().alias(\"deviation_count\"),\n",
    "    pl.col(\"deviation\").mean().alias(\"mean_deviation\")\n",
    "    ).sort(\"deviation_count\", descending=True)\n",
    "print(deviation_counts)\n",
    "deviation_counts.write_csv('results/real_data/results/deviation_counts.csv')\n",
    "for tool in deviation_counts['tool']:\n",
    "    tmp = deviated_rows.filter(pl.col('tool') == tool).sort(\"alignment_test\", descending=False)\n",
    "    print(f\"{tool} had: min {tmp['alignment_test'].min()} max {tmp['alignment_test'].max()} mean {tmp['alignment_test'].mean()} std {tmp['alignment_test'].std()}\")\n",
    "    print(f\"worst 5 rows for {tool}:\")\n",
    "    for row in tmp.tail(10).iter_rows(named=True):\n",
    "        print(prettify_alignment(row['spacer_seq'], row['contig_seq'], None,None, None))\n",
    "        print(f\"mismatches: {row['mismatches']} recalc: {row['alignment_test']}\")\n",
    "        print(\"\\n\")\n",
    "    for row in tmp.head(10).iter_rows(named=True):\n",
    "        print(prettify_alignment(row['spacer_seq'], row['contig_seq'], None,None, None))\n",
    "        print(f\"mismatches: {row['mismatches']} recalc: {row['alignment_test']}\")\n",
    "        print(\"\\n\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Overall results\n",
    "Next, we try to answer the question: \"Which is the single best tool?\".  \n",
    "For that, let's use 2 metrics to define \"best tool\":\n",
    "1.  Has the highest number of unique spacer-contig pairs.  \n",
    "2.  For every spacer, has the highest fraction of identified occurences (regardless of the number of unique contigs it was found in)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. Tool comparison matrixes - unique spacer-contig pairs "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tools_results = pl.read_parquet('results/real_data/results/tools_results_mm_recalced.parquet')\n",
    "tools_results = tools_results.filter(pl.col('alignment_test') < 4)\n",
    "tools_results = tools_results.rename({'mismatches': 'tool_reported_mismatches'}).rename({'alignment_test': 'mismatches'})\n",
    "tools_list = tools_results['tool'].unique().to_list() # might want to consider removing certain tools as they aren't very informative"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_mismatches = [0,1,2,3]\n",
    "tools_list"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Upset plots\n",
    "THis let's us examine the set() intersections of the results.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_mismatches = [0,1,2,3]\n",
    "for n_mismatches in plot_mismatches: \n",
    "    # first we create a table where each row is a contig and a 2nd column is a list of tools that matched the contig\n",
    "    nmism_tools_results = tools_results.filter(pl.col('mismatches') == n_mismatches)\n",
    "    contig_tool_table = nmism_tools_results.group_by('contig_id').agg(pl.col('tool').unique().alias('tools')).sort('contig_id')\n",
    "    test_upset = up.from_memberships(contig_tool_table['tools']) # need to disable future deprecation warrnings\n",
    "    print(\"n mismatches: \", n_mismatches)\n",
    "    plt.figure(figsize=(10, 6))\n",
    "    up.plot(test_upset, subset_size='count',sort_by='cardinality')\n",
    "    plt.title(f'Matches with == {n_mismatches} mismatches') #≤\n",
    "    plt.savefig(f'results/real_data/plots/upset_{n_mismatches}.pdf')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And as one figure for the supplementary material (I couldn't figure out how to merge upset plots, and I really do not want this to be done in inkscape manually, so using use svgutils)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# First create individual plots and save them\n",
    "for i, n_mismatches in enumerate(plot_mismatches):\n",
    "    # Create the data for the upset plot\n",
    "    nmism_tools_results = tools_results.filter(pl.col('mismatches') == n_mismatches)\n",
    "    contig_tool_table = nmism_tools_results.group_by('contig_id').agg(pl.col('tool').unique().alias('tools')).sort('contig_id')\n",
    "    test_upset = up.from_memberships(contig_tool_table['tools'])\n",
    "    \n",
    "    # Create individual plot\n",
    "    plt.figure(figsize=(12, 10))\n",
    "    up.plot(test_upset, subset_size='count', sort_by='cardinality')\n",
    "    plt.title(f'Matches with == {n_mismatches} mismatches')\n",
    "    \n",
    "    # Save individual plot\n",
    "    plt.savefig(f'results/real_data/plots/upset_{n_mismatches}.svg', bbox_inches='tight')\n",
    "    plt.close()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from svgutils.compose import Figure, Panel, SVG, Text\n",
    "# height = f\"2200\"\n",
    "# width = f\"7500\"\n",
    "# fig = Figure(width, height,\n",
    "#         SVG(\"results/real_data/plots/upset_0.svg\",fix_mpl=True),\n",
    "#         SVG(\"results/real_data/plots/upset_1.svg\",fix_mpl=True),\n",
    "#         SVG(\"results/real_data/plots/upset_2.svg\",fix_mpl=True),\n",
    "#         SVG(\"results/real_data/plots/upset_3.svg\",fix_mpl=True)\n",
    "#         ).tile(1, 4)\n",
    "# fig.save(\"results/real_data/plots/upset_combined.svg\")\n",
    "# I can't figure out how to make this look good, will just point to the plots seperatly"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Matrixes\n",
    "In a tool vs tool manner, it possible to get specific insights.  \n",
    "In the following tables A `cell(i,x)` is the number of unique spacer-contig pairs that are in tool `i` but not in tool `x`.   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_charts = []\n",
    "for n_mismatches in [0,1,2,3]:\n",
    "    print(f\"n_mismatches: {n_mismatches}\")\n",
    "    \n",
    "    # Filter for current mismatch level\n",
    "    tmp = tools_results.filter(pl.col('mismatches') == n_mismatches)\n",
    "    \n",
    "    # Create empty matrix\n",
    "    matrix = pl.DataFrame(data=np.zeros((len(tools_list), len(tools_list)), dtype=int), schema=tools_list)\n",
    "    \n",
    "    # Get unique pairs for each tool\n",
    "    tool_pairs = {}\n",
    "    for tool in tools_list:\n",
    "        tool_pairs[tool] = tmp.filter(pl.col('tool') == tool).select([\"contig_id\",\"spacer_id\",\"strand\",\"start\",\"end\"]).unique()\n",
    "    \n",
    "    # Fill matrix with counts\n",
    "    for i, tool_x in enumerate(tools_list):\n",
    "        for j, tool_y in enumerate(tools_list):\n",
    "            if tool_x == tool_y:\n",
    "                continue\n",
    "            # Count pairs in x but not in y\n",
    "            unique_pairs = tool_pairs[tool_x].join(tool_pairs[tool_y], on=['contig_id','spacer_id','strand','start','end'], how='anti')\n",
    "            matrix[i,j] = unique_pairs.height\n",
    "    \n",
    "    # Convert to DataFrame for better visualization\n",
    "    matrix = matrix.with_columns(pl.Series(name=\"tool1\", values=tools_list, dtype=pl.Utf8))\n",
    "    print(f\"Matrix for {n_mismatches} mismatches:\")\n",
    "    print(matrix)\n",
    "    heatmap_filename = f'results/real_data/plots/matrix_{n_mismatches}'\n",
    "    chart = plot_matrix(matrix, f\"Matrix for {n_mismatches} mismatches\", heatmap_filename)\n",
    "    all_charts.append(chart)\n",
    "    # plot_matrix(matrix, f\"Matrix for {n_mismatches} mismatches (% of tool's total matches)\", heatmap_filename+\"_percent\", as_percent=True)  # as percentages\n",
    "    matrix.write_csv(f'results/real_data/results/matrix_{n_mismatches}.tsv',separator='\\t')\n",
    "    print(\"\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's combine all the matrixes svgs into one for the manuscript"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import altair as alt\n",
    "\n",
    "row_1 = alt.hconcat(all_charts[0],all_charts[1])\n",
    "row_2 = alt.hconcat(all_charts[2],all_charts[3])\n",
    "chart = alt.vconcat(row_1,row_2)\n",
    "chart.save(\"results/real_data/plots/matrix_combined.svg\")\n",
    "chart.save(\"results/real_data/plots/matrix_combined.pdf\",format=\"pdf\")\n",
    "chart"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Summary of the matrixes: (one matrix with <=3 mismatches)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_mismatches = 3\n",
    "# Filter for current mismatch level\n",
    "tmp = tools_results.filter(pl.col('mismatches') <= n_mismatches)\n",
    "\n",
    "# Create empty matrix\n",
    "matrix = pl.DataFrame(data=np.zeros((len(tools_list), len(tools_list)), dtype=int), schema=tools_list)\n",
    "\n",
    "# Get unique pairs for each tool\n",
    "tool_pairs = {}\n",
    "for tool in tools_list:\n",
    "    tool_pairs[tool] = tmp.filter(pl.col('tool') == tool).select([\"contig_id\",\"spacer_id\",\"strand\",\"start\",\"end\"]).unique()\n",
    "\n",
    "# Fill matrix with counts\n",
    "for i, tool_x in enumerate(tools_list):\n",
    "    for j, tool_y in enumerate(tools_list):\n",
    "        if tool_x == tool_y:\n",
    "            continue\n",
    "        # Count pairs in x but not in y\n",
    "        unique_pairs = tool_pairs[tool_x].join(tool_pairs[tool_y], on=['contig_id','spacer_id','strand','start','end'], how='anti')\n",
    "        matrix[i,j] = unique_pairs.height\n",
    "\n",
    "# Convert to DataFrame for better visualization\n",
    "matrix = matrix.with_columns(pl.Series(name=\"tool1\", values=tools_list, dtype=pl.Utf8))\n",
    "print(f\"Matrix for {n_mismatches} mismatches:\")\n",
    "print(matrix)\n",
    "heatmap_filename = f'results/real_data/plots/matrix__less_or_equal_{n_mismatches}_mismatches'\n",
    "chart = plot_matrix(matrix, f\"Matrix <= {n_mismatches} mismatches\", heatmap_filename)\n",
    "# plot_matrix(matrix, f\"Matrix for {n_mismatches} mismatches (% of tool's total matches)\", heatmap_filename+\"_percent\", as_percent=True)  # as percentages\n",
    "matrix.write_csv(f'results/real_data/results/matrix_less_or_equal_{n_mismatches}_mismatches.tsv',separator='\\t')\n",
    "print(\"\\n\")\n",
    "chart"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Tool comparison - spacer-contig pairs as a function of the number of occurrences \n",
    "Next, we check if an increase in the number of occurrences (meaning more of the same spacer in the reference file) corrosponds to a lower true positive rate (in the sense that the tools do not find as much). The exact effect should be tool specific.  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, for each spacer, we add a column that specifies the fraction of the occurrences that tool identified\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_spacer_counts_with_tools(recalc_only, tools_list, mismatches=3, exact_or_max=\"exact\"):\n",
    "    # First get total occurrences per spacer across all tools\n",
    "    if exact_or_max == \"max\":\n",
    "        spacer_counts = recalc_only.filter(pl.col('mismatches') <= mismatches)\n",
    "    else:\n",
    "        spacer_counts = recalc_only.filter(pl.col('mismatches') == mismatches)\n",
    "    \n",
    "    spacer_counts = spacer_counts.select([\"spacer_id\", \"contig_id\"])\\\n",
    "        .unique()\\\n",
    "        .group_by('spacer_id')\\\n",
    "        .agg(pl.count('contig_id').alias('n_occurrences'))\n",
    "\n",
    "    # Calculate matches per tool and spacer without joining to spacer_counts yet\n",
    "    if exact_or_max == \"max\":\n",
    "        tool_matches = recalc_only.filter(pl.col('mismatches') <= mismatches)\n",
    "    else:\n",
    "        tool_matches = recalc_only.filter(pl.col('mismatches') == mismatches)\n",
    "    \n",
    "    tool_matches = tool_matches.select(['spacer_id', 'tool', 'contig_id'])\\\n",
    "        .unique()\\\n",
    "        .group_by(['spacer_id', 'tool'])\\\n",
    "        .agg(pl.count('contig_id').alias('tool_matches'))\n",
    "\n",
    "    # Create a cross join of all spacers with all tools\n",
    "    all_combinations = spacer_counts.select('spacer_id', 'n_occurrences')\\\n",
    "        .join(\n",
    "            pl.DataFrame({'tool': tools_list}),\n",
    "            how='cross'\n",
    "        )\n",
    "\n",
    "    # Join the actual matches and calculate fractions\n",
    "    complete_fractions = all_combinations\\\n",
    "        .join(\n",
    "            tool_matches,\n",
    "            on=['spacer_id', 'tool'],\n",
    "            how='left'\n",
    "        )\\\n",
    "        .with_columns([\n",
    "            pl.col('tool_matches').fill_null(0),\n",
    "            (pl.col('tool_matches') / pl.col('n_occurrences')).alias('fraction')\n",
    "        ])\n",
    "\n",
    "    # Pivot to get tools as columns\n",
    "    spacer_counts_with_tools = complete_fractions\\\n",
    "        .pivot(\n",
    "            index=['spacer_id', 'n_occurrences'],\n",
    "            on='tool',\n",
    "            values='fraction'\n",
    "        )\\\n",
    "        .fill_null(0)\n",
    "    \n",
    "    return spacer_counts_with_tools"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Recall plots vs occurrences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_combined_recall_vs_occurrences(recalc_only, tools_list, n_high_occ_bins=3, \n",
    "                         output_prefix='results/real_data/plots/recall_vs_occurrences', \n",
    "                         max_bin=3, n_bins=150, color_dict=None, marker_dict=None, exact_or_max=\"exact\",\n",
    "                         plot_mismatches=[1,3]):\n",
    "    \n",
    "    # Create color and marker dictionaries for consistent styling\n",
    "    import matplotlib.colors as mcolors\n",
    "    if color_dict is None:\n",
    "        color_dict = dict(zip(tools_list, mcolors.TABLEAU_COLORS))\n",
    "    if marker_dict is None:\n",
    "        marker_dict = dict(zip(tools_list, ['o', 's', '^', 'D', 'v', '<', '>', 'p','x']))\n",
    "\n",
    "    # Create figure with two subplots\n",
    "    fig, axes = plt.subplots(len(plot_mismatches), 1, figsize=(15, 36))\n",
    "    \n",
    "    for i, mismatches in enumerate(plot_mismatches):\n",
    "        # Create a new figure for the single plot\n",
    "        fig_single, ax_single = plt.subplots(figsize=(15, 12))\n",
    "        \n",
    "        # Plot on both the combined and single figures\n",
    "        plot_on_axis(axes[i], recalc_only, tools_list, n_high_occ_bins, n_bins, max_bin, \n",
    "                    mismatches=mismatches, exact_or_max=exact_or_max, color_dict=color_dict, \n",
    "                    marker_dict=marker_dict, output_prefix=output_prefix)\n",
    "        plot_on_axis(ax_single, recalc_only, tools_list, n_high_occ_bins, n_bins, max_bin, \n",
    "                    mismatches=mismatches, exact_or_max=exact_or_max, color_dict=color_dict, \n",
    "                    marker_dict=marker_dict, output_prefix=output_prefix)\n",
    "        \n",
    "        # Set titles\n",
    "        if exact_or_max == \"exact\":\n",
    "            title = f'Recall vs number of occurrences (mismatches == {mismatches})'\n",
    "        else:\n",
    "            title = f'Recall vs number of occurrences (mismatches ≤ {mismatches})'\n",
    "        axes[i].set_title(title)\n",
    "        ax_single.set_title(title)\n",
    "        \n",
    "        # Save single plot\n",
    "        plt.figure(fig_single.number)\n",
    "        plt.tight_layout()\n",
    "        fig_single.savefig(f'{output_prefix}_{exact_or_max}_nm_{mismatches}.pdf', bbox_inches='tight',format='pdf')\n",
    "        fig_single.savefig(f'{output_prefix}_{exact_or_max}_nm_{mismatches}.svg', bbox_inches='tight',format='svg')\n",
    "        plt.close(fig_single)\n",
    "    \n",
    "    # Save combined plot\n",
    "    plt.figure(fig.number)\n",
    "    plt.tight_layout()\n",
    "    plot_mismatches_str = \"_\".join([str(mismatch) for mismatch in plot_mismatches])\n",
    "    fig.savefig(f'{output_prefix}_{exact_or_max}_nm_{plot_mismatches_str}_combined.pdf', bbox_inches='tight')\n",
    "    fig.savefig(f'{output_prefix}_{exact_or_max}_nm_{plot_mismatches_str}_combined.svg', bbox_inches='tight')\n",
    "    plt.show()\n",
    "\n",
    "def plot_on_axis(ax: plt.Axes, recalc_only, tools_list, n_high_occ_bins, n_bins, max_bin, \n",
    "                 mismatches, exact_or_max, color_dict, marker_dict,output_prefix):\n",
    "    \n",
    "    spacer_counts_with_tools = create_spacer_counts_with_tools(recalc_only, tools_list, \n",
    "                                                             mismatches=mismatches, \n",
    "                                                             exact_or_max=exact_or_max)\n",
    "\n",
    "    # Create range bins for number of occurrences\n",
    "    bins = np.logspace(np.log10(1), max_bin, n_bins)\n",
    "\n",
    "    # Calculate mean fraction for each tool within each bin\n",
    "    bin_stats = []\n",
    "    for i in range(len(bins)-1):\n",
    "        mask = (spacer_counts_with_tools['n_occurrences'] >= bins[i]) & \\\n",
    "               (spacer_counts_with_tools['n_occurrences'] < bins[i+1])\n",
    "        bin_data = spacer_counts_with_tools.filter(mask)\n",
    "        if bin_data.height > 0:\n",
    "            stats = {\n",
    "                'bin_start': bins[i],\n",
    "                'bin_end': bins[i+1],\n",
    "                'n_spacers': bin_data.height\n",
    "            }\n",
    "            for tool in tools_list:\n",
    "                stats[tool] = bin_data[tool].mean()\n",
    "            bin_stats.append(stats)\n",
    "\n",
    "    # Add points for high occurrences in multiple bins\n",
    "    if n_high_occ_bins > 0:\n",
    "        high_occ_edges = np.logspace(3, 4, n_high_occ_bins + 1)\n",
    "        for i in range(n_high_occ_bins):\n",
    "            bin_start = high_occ_edges[i]\n",
    "            bin_end = high_occ_edges[i + 1]\n",
    "            \n",
    "            if i == n_high_occ_bins - 1:\n",
    "                high_occ_mask = (spacer_counts_with_tools['n_occurrences'] >= bin_start)\n",
    "            else:\n",
    "                high_occ_mask = (spacer_counts_with_tools['n_occurrences'] >= bin_start) & \\\n",
    "                               (spacer_counts_with_tools['n_occurrences'] < bin_end)\n",
    "            \n",
    "            high_occ_data = spacer_counts_with_tools.filter(high_occ_mask)\n",
    "            if high_occ_data.height > 0:\n",
    "                high_occ_stats = {\n",
    "                    'bin_start': bin_start,\n",
    "                    'bin_end': bin_end,\n",
    "                    'n_spacers': high_occ_data.height\n",
    "                }\n",
    "                for tool in tools_list:\n",
    "                    high_occ_stats[tool] = high_occ_data[tool].mean()\n",
    "                bin_stats.append(high_occ_stats)\n",
    "\n",
    "    # Plot on the provided axis\n",
    "    for tool in tools_list:\n",
    "        x = [(stat['bin_start'] + stat['bin_end'])/2 for stat in bin_stats]\n",
    "        y = [stat[tool] for stat in bin_stats]\n",
    "        ax.plot(x, y, label=tool, markersize=4, linewidth=1, \n",
    "                color=color_dict[tool], markerfacecolor=color_dict[tool],\n",
    "                marker=marker_dict[tool])\n",
    "\n",
    "    ax.set_xscale('log')\n",
    "    ax.set_xlabel('Number of occurrences (log scale)')\n",
    "    ax.set_ylabel('Mean Detection Fraction')\n",
    "    ax.legend(bbox_to_anchor=(1.05, 1), loc='upper left')\n",
    "    ax.grid(True, which=\"both\", ls=\"-\", alpha=0.2)\n",
    "    ax.grid(True, which=\"major\", ls=\"-\", alpha=0.5)\n",
    "    ax.minorticks_on()\n",
    "    ax.set_ylim(0, 1.05)\n",
    "    ax.set_xlim(1, 10**4)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_mismatches = [0,1,2,3]\n",
    "plot=plot_combined_recall_vs_occurrences(tools_results,\n",
    "                                        tools_list,\n",
    "                                        n_high_occ_bins=3,\n",
    "                                        output_prefix='results/real_data/plots/recall_vs_occurrences',\n",
    "                                        plot_mismatches=n_mismatches,\n",
    "                                        exact_or_max=\"exact\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And in a multi-panel plot with 2 plots (up to 1 and up to 3):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tools_list"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Similar plots, but with 1 and 3 mismatches (max, not exact), and in a multi-panel plot (for the main text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Call the plotting function combined\n",
    "plot_combined_recall_vs_occurrences(tools_results, tools_list, n_high_occ_bins=3, \n",
    "                                  output_prefix='results/real_data/plots/recall_vs_occurrences',\n",
    "                                  exact_or_max=\"max\",\n",
    "                                  plot_mismatches=[1,3])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we'll look at the performance of the tools as a function of the number of mismatches."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# First calculate the total matches per spacer-contig pair across all tools (per mismatch level)\n",
    "import altair as alt\n",
    "total_matches = tools_results\\\n",
    "    .select(['spacer_id', 'contig_id', 'mismatches'])\\\n",
    "    .unique()\\\n",
    "    .group_by(['mismatches'])\\\n",
    "    .agg(pl.len().alias('total_possible'))\n",
    "\n",
    "# Calculate matches per tool at each mismatch level\n",
    "tool_matches = tools_results\\\n",
    "    .select(['spacer_id', 'contig_id', 'tool', 'mismatches'])\\\n",
    "    .unique()\\\n",
    "    .group_by(['mismatches', 'tool'])\\\n",
    "    .agg(pl.len().alias('tool_matches'))\n",
    "\n",
    "# Create all combinations of mismatches (0-3) and tools\n",
    "all_combinations = pl.DataFrame({\n",
    "    'mismatches': np.repeat(range(4), len(tools_list)),\n",
    "    'tool': tools_list * 4\n",
    "})\n",
    "\n",
    "# Calculate fractions\n",
    "mismatch_performance = all_combinations\\\n",
    "    .join(\n",
    "        total_matches,\n",
    "        on='mismatches'\n",
    "    )\\\n",
    "    .join(\n",
    "        tool_matches,\n",
    "        on=['mismatches', 'tool'],\n",
    "        how='left'\n",
    "    )\\\n",
    "    .with_columns([\n",
    "        pl.col('tool_matches').fill_null(0),\n",
    "        (pl.col('tool_matches') / pl.col('total_possible')).alias('recall')\n",
    "    ])\n",
    "\n",
    "mismatch_performance.write_csv('results/real_data/results/tool_recall_by_mismatches.tsv',separator='\\t')\n",
    "\n",
    "metrics = {\n",
    "    # 'precision': 'true_positives / (true_positives + false_positives)',\n",
    "    'recall': 'true_positives / (true_positives + false_negatives)',\n",
    "    # 'f1_score': ' 2 * (precision * recall) / (precision + recall)'\n",
    "}\n",
    "\n",
    "charts = []\n",
    "\n",
    "for metric in metrics.keys():\n",
    "    base_chart = alt.Chart(mismatch_performance).mark_trail(color=\"tool:N\").encode(\n",
    "        x=alt.X(\"mismatches:Q\", title=\"Number of Mismatches\"),\n",
    "        y=alt.Y(f\"{metric}:Q\", \n",
    "                title=metrics[metric],\n",
    "                scale=alt.Scale(domain=[0, 1.05])\n",
    "                ),\n",
    "        color=alt.Color(\"tool:N\",\n",
    "                        legend=None\n",
    "                       ),  # Hide color legend\n",
    "        shape=alt.Shape(\"tool:N\",     # Shape legend will show both shape and color\n",
    "                       legend=alt.Legend(\n",
    "                           title=\"Tool\",\n",
    "                           orient=\"right\",\n",
    "                           symbolFillColor=\"tool:N\",  # Use the color encoding for fill\n",
    "                           symbolStrokeColor=\"tool:N\" # Use the color encoding for stroke\n",
    "                       )),\n",
    "        tooltip=['tool', 'mismatches', metric]\n",
    "    ).properties(\n",
    "        width=300,\n",
    "        height=300,\n",
    "        title=metric.title()\n",
    "    )\n",
    "    \n",
    "    charts.append(base_chart)\n",
    "\n",
    "# Combine charts horizontally\n",
    "combined_chart = alt.hconcat(*charts).configure_axis(\n",
    "    grid=True,\n",
    "    gridOpacity=0.9\n",
    ").configure_view(step=1,\n",
    "    strokeWidth=0.1\n",
    ").configure_title(\n",
    "    fontSize=16,\n",
    "    anchor='middle'\n",
    ")\n",
    "combined_chart.save('./results/real_data/plots/tool_performance_by_mismatches.html')\n",
    "combined_chart.save('./results/real_data/plots/tool_performance_by_mismatches.json',format='json')\n",
    "\n",
    "import json as json\n",
    "# Save the chart specification as JSON with the correct format\n",
    "chart_json = combined_chart.to_json(format=\"vega\")\n",
    "\n",
    "# Parse the JSON\n",
    "chart_spec = json.loads(chart_json)\n",
    "\n",
    "# Replace the legends in the specification\n",
    "# We'll need to traverse the spec to find where legends are defined damn it\n",
    "def update_legends_in_spec(spec):\n",
    "    # Define the new legend configuration\n",
    "    new_legend = {\n",
    "    \"orient\": \"right\",\n",
    "    \"symbolSize\": 190,\n",
    "    \"symbolOpacity\": 1,\n",
    "    \"symbolFillColor\": \"shape\",\n",
    "    \"symbolStrokeColor\": \"tool:N\",\n",
    "    \"title\": \"Tool\",\n",
    "    \"shape\": \"shape\",\n",
    "    \"fill\": \"color\",\n",
    "    \"offset\": 0,\n",
    "    \"encode\": {\n",
    "        \"symbols\": {\n",
    "            \"update\": {\n",
    "                \"fillOpacity\": {\"value\": 0.9}\n",
    "            }\n",
    "        }\n",
    "    }}\n",
    "    if isinstance(spec, dict):\n",
    "        if 'legends' in spec:\n",
    "            spec['legends'] = [new_legend]\n",
    "        for value in spec.values():\n",
    "            update_legends_in_spec(value)\n",
    "    elif isinstance(spec, list):\n",
    "        for item in spec:\n",
    "            update_legends_in_spec(item)\n",
    "    return spec\n",
    "\n",
    "new_chart_spec = update_legends_in_spec(chart_spec)\n",
    "\n",
    "# Write the modified specification to a file\n",
    "def change_chart_spec(new_chart_spec,output_path):\n",
    "        html_content = f\"\"\"\n",
    "        <!DOCTYPE html>\n",
    "        <html>\n",
    "        <head>\n",
    "        <script src=\"https://cdn.jsdelivr.net/npm/vega@5\"></script>\n",
    "        <script src=\"https://cdn.jsdelivr.net/npm/vega-lite@5\"></script>\n",
    "        <script src=\"https://cdn.jsdelivr.net/npm/vega-embed@6\"></script>\n",
    "        </head>\n",
    "        <body>\n",
    "        <div id=\"vis\"></div>\n",
    "        <script type=\"text/javascript\">\n",
    "            var spec = {json.dumps(new_chart_spec)};\n",
    "            vegaEmbed('#vis', spec);\n",
    "        </script>\n",
    "        </body>\n",
    "        </html>\n",
    "        \"\"\"\n",
    "        \n",
    "        with open(output_path, 'w') as f:\n",
    "            f.write(html_content)\n",
    "# chart_spec\n",
    "change_chart_spec(new_chart_spec, './results/real_data/plots/tool_performance_by_mismatches.html')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "combined_chart"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "spacer_counts_with_tools = create_spacer_counts_with_tools(tools_results, tools_list, mismatches=3,exact_or_max=\"max\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we chart the distribution of the spacer lengths, number of mismatches, and number of occurrences."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# alt.data_transformers.enable(\"vegafusion\")\n",
    "def plot_spacer_distributions(tools_results, spacer_counts_with_tools, output_prefix='results/real_data/plots'):\n",
    "    \"\"\"Create a three-panel figure showing spacer length, mismatch, and occurrence distributions using Altair.\n",
    "    Uses VegaFusion for efficient handling of large datasets.\n",
    "    \"\"\"\n",
    "    # Pre-aggregate in Polars to reduce data size\n",
    "    spacer_df = tools_results.select([\n",
    "        'spacer_id', 'contig_id', 'strand', 'start', 'end', 'spacer_length', 'mismatches'\n",
    "    ]).unique()\n",
    "\n",
    "    occurrence_df = spacer_counts_with_tools['n_occurrences'].value_counts()\n",
    "    mismatches_df=spacer_df['mismatches'].value_counts()\n",
    "    length_df=spacer_df['spacer_length'].value_counts()\n",
    "\n",
    "   \n",
    "    # Base chart for occurrence distribution\n",
    "    base = alt.Chart(occurrence_df).encode(\n",
    "        tooltip=['n_occurrences:Q', 'count:Q']\n",
    "    )\n",
    "\n",
    "    chart3 = base.mark_bar(opacity=0.5).encode(\n",
    "        alt.X('n_occurrences:Q',\n",
    "                scale=alt.Scale(type='log'),\n",
    "                title='Number of Occurrences'),\n",
    "        alt.Y('count:Q', title='Count (Linear Scale)')\n",
    "    ).properties(\n",
    "        title='B.',\n",
    "    )\n",
    "    \n",
    "    chart1 = alt.Chart(length_df).mark_bar().encode(\n",
    "        x=alt.X('spacer_length:Q', title='Spacer Length (bp)', axis=alt.Axis(grid=True,tickCount=100)),\n",
    "        y=alt.Y(\"count:Q\", axis=alt.Axis(title=\"Number of Spacers\", grid=True,ticks=False,gridDash=[2,2]),\n",
    "        # scale=alt.Scale(domain=[16, 108])\n",
    "        )\n",
    "\n",
    "        \n",
    "    ).properties(\n",
    "        title='A.',\n",
    "    )\n",
    "\n",
    "    chart2 = alt.Chart(mismatches_df).mark_bar(width=13).encode(\n",
    "        x=alt.X('mismatches:Q', title='Number of Mismatches', axis=alt.Axis(grid=True,tickCount=6)),\n",
    "        y=alt.Y(\"count:Q\", axis=alt.Axis(title=\"Number of Spacers\",  grid=True,ticks=False,gridDash=[2,2])),\n",
    "        # color=alt.Color('mismatches:Q', legend=T)\n",
    "    ).properties(\n",
    "        title='C.',\n",
    "    )\n",
    "    \n",
    "    combined_chart = chart1 | chart3 | chart2\n",
    "    combined_chart  = combined_chart.configure_title(anchor='start')\n",
    "    return combined_chart\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "combined_chart =  plot_spacer_distributions(spacer_counts_with_tools=spacer_counts_with_tools,tools_results=tools_results)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "combined_chart"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "combined_chart.save('./results/real_data/plots/spacer_distributions.html')\n",
    "combined_chart.save('./results/real_data/plots/spacer_distributions.svg',format='svg')\n",
    "combined_chart.save('./results/real_data/plots/spacer_distributions.pdf',format='pdf')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "combined_chart"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For safe keeping, also print some summary statistics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Print some summary statistics\n",
    "# plot_spacer_distributions(spacer_counts_with_tools=spacer_counts_with_tools,tools_results=tools_results)\n",
    "print(\"\\nSummary Statistics:\")\n",
    "print(\"\\nSpacer Lengths:\")\n",
    "print(f\"Mean: {tools_results['spacer_length'].mean():.2f}\")\n",
    "print(f\"Median: {tools_results['spacer_length'].median():.2f}\")\n",
    "print(f\"Std: {tools_results['spacer_length'].std():.2f}\")\n",
    "print(f\"Min: {tools_results['spacer_length'].min()}\")\n",
    "print(f\"Max: {tools_results['spacer_length'].max()}\")\n",
    "\n",
    "print(\"\\nMismatches:\")\n",
    "print(f\"Mean: {tools_results['mismatches'].mean():.2f}\")\n",
    "print(f\"Median: {tools_results['mismatches'].median():.2f}\")\n",
    "print(f\"Std: {tools_results['mismatches'].std():.2f}\")\n",
    "print(f\"Min: {tools_results['mismatches'].min()}\")\n",
    "print(f\"Max: {tools_results['mismatches'].max()}\")\n",
    "\n",
    "print(\"\\nOccurrences:\")\n",
    "print(f\"Mean: {spacer_counts_with_tools['n_occurrences'].mean():.2f}\")\n",
    "print(f\"Median: {spacer_counts_with_tools['n_occurrences'].median():.2f}\")\n",
    "print(f\"Std: {spacer_counts_with_tools['n_occurrences'].std():.2f}\")\n",
    "print(f\"Min: {spacer_counts_with_tools['n_occurrences'].min()}\")\n",
    "print(f\"Max: {spacer_counts_with_tools['n_occurrences'].max()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finnaly, we'll targz the plots folder in the results folder for easy download/sharing "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%bash\n",
    "tar -czvf results/real_data/plots.tar.gz results/real_data/plots "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "default",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.23"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
