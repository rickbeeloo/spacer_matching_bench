{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Performance Analysis: IMG/VR4 Stratified Subsamples\n",
    "\n",
    "This notebook analyzes tool performance across stratified subsamples of the IMG/VR4 dataset. The subsamples were created using taxonomy-aware, GC-content, and length-stratified sampling to preserve diversity while reducing computational requirements.\n",
    "\n",
    "**Subsamples analyzed**: fractions 0.001, 0.005, 0.01 (0.05 and 0.1 were also generated but not all tools completed successfully on these larger samples).\n",
    "**Approach**: Load tool results from each subsample, analyze performance metrics, and compare across scales.\n",
    "\n",
    "Note: Unlike simulated data, we don't have ground truth here, so we focus on:\n",
    "- Number of unique spacer-contig matches found\n",
    "The main goal here:  \n",
    "Are the tools subsampled results indicative on a larger (non sample size / larger sample size) comparison (i.e. can we trust interpration made using the largest subsample)\n",
    "\n",
    "\n",
    "Note2: The actual performence comparisons for the 1% sample are in Performence_imgvr4.ipynb\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup and Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Max mismatches: 3\n",
      "  Tool styles configured for 12 tools\n"
     ]
    }
   ],
   "source": [
    "# %load_ext autoreload\n",
    "# %autoreload 2\n",
    "import os\n",
    "import glob\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\", category=FutureWarning)\n",
    "\n",
    "os.chdir('/clusterfs/jgi/scratch/science/metagen/neri/code/blits/spacer_bench/')\n",
    "\n",
    "import polars as pl\n",
    "import matplotlib.pyplot as plt\n",
    "# import seaborn as sns\n",
    "# import numpy as np\n",
    "\n",
    "pl.Config(tbl_rows=50, tbl_cols=15)\n",
    "\n",
    "from bench.utils.functions import read_fasta,read_results, populate_pldf_withseqs_needletail, test_alignment_polars\n",
    "from bench.utils.tool_commands import load_tool_configs\n",
    "# Analysis parameters\n",
    "MAX_MISMATCHES = 3\n",
    "base_dir = \"/clusterfs/jgi/scratch/science/metagen/neri/code/blits/spacer_bench/results/real_data/subsamples\"\n",
    "spacers_file = \"/clusterfs/jgi/scratch/science/metagen/neri/code/blits/spacer_bench/imgvr4_data/spacers/All_CRISPR_spacers_nr_clean.fna\"\n",
    "fractions = [0.001, 0.005, 0.01, 0.05, 0.1]\n",
    "\n",
    "# Tool color and marker configuration\n",
    "import json\n",
    "\n",
    "# Create tool-to-style mapping\n",
    "TOOL_COLORS_FILE = \"notebooks/antonio_14_colors.json\"\n",
    "with open(TOOL_COLORS_FILE, 'r') as f:\n",
    "    color_config = json.load(f)\n",
    "TOOL_ORDER = [\n",
    "    'blastn', 'bowtie1', 'bowtie2', 'indelfree_bruteforce', 'indelfree_indexed',\n",
    "    'lexicmap', 'minimap2', 'mmseqs2', 'mummer4', 'sassy', 'strobealign', 'x_mapper'\n",
    "]\n",
    "MARKERS = ['o', 's', '^', 'v', 'D', 'P', '*', 'X', 'h', 'p', '<', '>']\n",
    "TOOL_STYLES = {}\n",
    "for i, tool in enumerate(TOOL_ORDER):\n",
    "    TOOL_STYLES[tool] = {\n",
    "        'color': color_config['hex_colors'][i % len(color_config['hex_colors'])],\n",
    "        'marker': MARKERS[i % len(MARKERS)]\n",
    "    }\n",
    "\n",
    "print(f\"  Max mismatches: {MAX_MISMATCHES}\")\n",
    "print(f\"  Tool styles configured for {len(TOOL_STYLES)} tools\")\n",
    "with open('notebooks/tool_styles.json', 'w') as f:\n",
    "    json.dump(TOOL_STYLES, f, indent=4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Scan Subsample Directories and Identify Completed Tools\n",
    "\n",
    "First, we scan each subsample fraction directory and check the SLURM logs to identify which tools completed successfully vs timed out."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fraction 0.001:\n",
      "  Completed: 11 tools - blastn, bowtie1, bowtie2, indelfree_bruteforce, indelfree_indexed, minimap2, mmseqs, mummer4, sassy, strobealign, x_mapper\n",
      "\n",
      "Fraction 0.005:\n",
      "  Completed: 12 tools - blastn, bowtie1, bowtie2, indelfree_bruteforce, indelfree_indexed, lexicmap, minimap2, mmseqs, mummer4, sassy, strobealign, x_mapper\n",
      "\n",
      "Fraction 0.01:\n",
      "  Completed: 12 tools - blastn, bowtie1, bowtie2, indelfree_bruteforce, indelfree_indexed, lexicmap, minimap2, mmseqs, mummer4, sassy, strobealign, x_mapper\n",
      "\n",
      "Fraction 0.05:\n",
      "  Completed: 11 tools - blastn, bowtie1, bowtie2, indelfree_indexed, lexicmap, minimap2, mmseqs, mummer4, sassy, strobealign, x_mapper\n",
      "  Timed out: indelfree_bruteforce\n",
      "\n",
      "Fraction 0.1:\n",
      "  Completed: 10 tools - blastn, bowtie1, bowtie2, indelfree_indexed, lexicmap, minimap2, mmseqs, mummer4, strobealign, x_mapper\n",
      "  Timed out: indelfree_bruteforce, sassy\n",
      "\n"
     ]
    }
   ],
   "source": [
    "def check_tool_completion(fraction):\n",
    "    \"\"\"Check which tools completed for a given fraction by examining SLURM logs\"\"\"\n",
    "    frac_dir = f\"{base_dir}/fraction_{fraction}\"\n",
    "    log_dir = f\"{frac_dir}/slurm_logs\"\n",
    "    \n",
    "    if not os.path.exists(log_dir):\n",
    "        return {\"completed\": [], \"timed_out\": []} #, \"failed\": []}\n",
    "    \n",
    "    completed = set()\n",
    "    timed_out = set()\n",
    "    # failed = set()\n",
    "    \n",
    "    # Check all .out and .err files\n",
    "    for out_file in glob.glob(f\"{log_dir}/*.out\"):\n",
    "        tool_name = os.path.basename(out_file).replace(\"_long-\", \"-\").split('-')[0]\n",
    "        err_file = out_file.replace('.out', '.err')\n",
    "\n",
    "        \n",
    "        if not os.path.exists(err_file):\n",
    "            continue\n",
    "            \n",
    "        # Read error log to check for timeout\n",
    "        with open(err_file, 'r') as f:\n",
    "            err_content = f.read()\n",
    "            if 'TIME LIMIT' in err_content or 'DUE TO TIME LIMIT' in err_content:\n",
    "                timed_out.add(tool_name)\n",
    "            # elif 'CANCELLED' in err_content or 'FAILED' in err_content:\n",
    "            #     failed.add(tool_name)\n",
    "            else:\n",
    "                # Check if output file exists\n",
    "                output_file = f\"{frac_dir}/raw_outputs/{tool_name}_output.{'sam' if tool_name not in ['blastn', 'lexicmap', 'mmseqs'] else 'tsv'}\"\n",
    "                if tool_name == \"sassy\":\n",
    "                    output_file = f\"{frac_dir}/raw_outputs/sassy.tsv\"\n",
    "                if os.path.exists(output_file) and os.path.getsize(output_file) > 0:\n",
    "                    completed.add(tool_name)\n",
    "    \n",
    "    return {\n",
    "        \"completed\": sorted(list(completed)),\n",
    "        \"timed_out\": sorted(list(timed_out - completed)),  # Remove if completed on resubmit\n",
    "        # \"failed\": sorted(list(failed - completed))\n",
    "    }\n",
    "\n",
    "# Check completion status for all fractions\n",
    "completion_status = {}\n",
    "for frac in fractions:\n",
    "    status = check_tool_completion(frac)\n",
    "    completion_status[frac] = status\n",
    "    print(f\"Fraction {frac}:\")\n",
    "    print(f\"  Completed: {len(status['completed'])} tools - {', '.join(status['completed'])}\")\n",
    "    if status['timed_out']:\n",
    "        print(f\"  Timed out: {', '.join(status['timed_out'])}\")\n",
    "    # if status['failed']:\n",
    "    #     print(f\"  Failed: {', '.join(status['failed'])}\")\n",
    "    print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load Tool Results from Each Subsample\n",
    "\n",
    "For each fraction, load the tool results using the `read_results` function with proper filtering."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded 3882812 spacers\n",
      "Length range: 25 - 40 bp\n"
     ]
    }
   ],
   "source": [
    "# Read spacer lengths for filtering\n",
    "spacers = read_fasta(spacers_file)\n",
    "spacer_lendf = pl.DataFrame({\n",
    "    \"spacer_id\": list(spacers.keys()), \n",
    "    \"length\": [len(seq) for seq in spacers.values()]\n",
    "})\n",
    "\n",
    "print(f\"Loaded {len(spacers)} spacers\")\n",
    "print(f\"Length range: {spacer_lendf['length'].min()} - {spacer_lendf['length'].max()} bp\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== Loading fraction 0.001 ===\n",
      "  Loading 10 tools: blastn, bowtie1, bowtie2, indelfree_bruteforce, indelfree_indexed, minimap2, mummer4, sassy, strobealign, x_mapper\n",
      "\n",
      "[ThreadControl] Setting environment variables for 8 threads\n",
      "  OMP_NUM_THREADS=None\n",
      "  OPENBLAS_NUM_THREADS=None\n",
      "  DuckDB query parallelism: 8 threads\n",
      "  DuckDB memory limit: 50GB\n",
      "\n",
      "Reading results for blastn...\n",
      "File size: 3.10 MB\n",
      "Saved blastn results to /tmp/spacer_bench_duckdb_y3h9u8bw/blastn.parquet\n",
      "\n",
      "Reading results for bowtie1...\n",
      "File size: 496.00 MB\n",
      "first line of sam file @HD\tVN:1.0\tSO:unsorted\n",
      "second line of sam file @SQ\tSN:IMGVR_UViG_2974659795_000002|2974659795|2974659795\tLN:6543\n",
      "Found 808 SQ lines, 1 PG lines\n",
      "SAM file looks good, no changes needed\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Parsing SAM file: 3897862it [00:02, 1339754.64it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved bowtie1 results to /tmp/spacer_bench_duckdb_y3h9u8bw/bowtie1.parquet\n",
      "\n",
      "Reading results for bowtie2...\n",
      "File size: 499.48 MB\n",
      "first line of sam file @HD\tVN:1.5\tSO:unsorted\tGO:query\n",
      "second line of sam file @SQ\tSN:IMGVR_UViG_2974659795_000002|2974659795|2974659795\tLN:6543\n",
      "Found 808 SQ lines, 1 PG lines\n",
      "SAM file looks good, no changes needed\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Parsing SAM file: 3894848it [00:02, 1322711.75it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved bowtie2 results to /tmp/spacer_bench_duckdb_y3h9u8bw/bowtie2.parquet\n",
      "\n",
      "Reading results for indelfree_bruteforce...\n",
      "File size: 99.26 MB\n",
      "first line of sam file @HD\tVN:1.6\tSO:unsorted\n",
      "second line of sam file @SQ\tSN:IMGVR_UViG_2974659795_000002|2974659795|2974659795\tLN:6543\n",
      "Found 808 SQ lines, 1 PG lines\n",
      "SAM file looks good, no changes needed\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Parsing SAM file: 649717it [00:02, 306195.33it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved indelfree_bruteforce results to /tmp/spacer_bench_duckdb_y3h9u8bw/indelfree_bruteforce.parquet\n",
      "\n",
      "Reading results for indelfree_indexed...\n",
      "File size: 18.63 MB\n",
      "first line of sam file @HD\tVN:1.6\tSO:unsorted\n",
      "second line of sam file @SQ\tSN:IMGVR_UViG_2974659795_000002|2974659795|2974659795\tLN:6543\n",
      "Found 808 SQ lines, 1 PG lines\n",
      "SAM file looks good, no changes needed\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Parsing SAM file: 114895it [00:00, 182219.20it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved indelfree_indexed results to /tmp/spacer_bench_duckdb_y3h9u8bw/indelfree_indexed.parquet\n",
      "\n",
      "Reading results for minimap2...\n",
      "File size: 0.06 MB\n",
      "first line of sam file @HD\tVN:1.6\tSO:unsorted\tGO:query\n",
      "second line of sam file @SQ\tSN:IMGVR_UViG_2974659795_000002|2974659795|2974659795\tLN:6543\n",
      "Found 808 SQ lines, 1 PG lines\n",
      "SAM file looks good, no changes needed\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Parsing SAM file: 13it [00:00, 24616.68it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved minimap2 results to /tmp/spacer_bench_duckdb_y3h9u8bw/minimap2.parquet\n",
      "\n",
      "Reading results for mummer4...\n",
      "File size: 4.18 MB\n",
      "first line of sam file @HD\tVN:1.6\tSO:unsorted\n",
      "second line of sam file @SQ\tSN:IMGVR_UViG_2974659795_000002|2974659795|2974659795\tLN:6543\n",
      "Found 808 SQ lines, 1 PG lines\n",
      "SAM file needs fixing\n",
      "Rewriting HD line for mummer file\n",
      "SAM file has been fixed\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Parsing SAM file: 24945it [00:00, 260350.15it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved mummer4 results to /tmp/spacer_bench_duckdb_y3h9u8bw/mummer4.parquet\n",
      "\n",
      "Reading results for sassy...\n",
      "File size: 148590.10 MB\n",
      "Checking existing cache at: /clusterfs/jgi/scratch/science/metagen/neri/code/blits/spacer_bench/results/real_data/subsamples/fraction_0.001/raw_outputs/sassy_parsed_20260114.parquet\n",
      ">> Cache file exists but appears corrupt/incomplete (parquet: File out of specification: The file must end with PAR1). Reprocessing...\n",
      "\n",
      "[ThreadControl] Setting environment variables for 8 threads (Sassy)\n",
      "Starting DuckDB Pipeline (Writing to: /clusterfs/jgi/scratch/science/metagen/neri/code/blits/spacer_bench/results/real_data/subsamples/fraction_0.001/raw_outputs/sassy_parsed_20260114.parquet)...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "fbf860973fe940b1ba5808e31ca10c55",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, layout=Layout(width='auto'), style=ProgressStyle(bar_color='black'))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Returning lazy frame (no materialization).\n",
      "Saved sassy results to /tmp/spacer_bench_duckdb_y3h9u8bw/sassy.parquet\n",
      "\n",
      "Reading results for strobealign...\n",
      "File size: 551.69 MB\n",
      "first line of sam file @HD\tVN:1.6\tSO:unsorted\n",
      "second line of sam file @SQ\tSN:IMGVR_UViG_2974659795_000002|2974659795|2974659795\tLN:6543\n",
      "Found 808 SQ lines, 1 PG lines\n",
      "SAM file looks good, no changes needed\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Parsing SAM file: 4920741it [00:08, 610962.75it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved strobealign results to /tmp/spacer_bench_duckdb_y3h9u8bw/strobealign.parquet\n",
      "\n",
      "Reading results for x_mapper...\n",
      "File size: 3.06 MB\n",
      "first line of sam file @CO\tSequence Alignment Map\n",
      "second line of sam file @CO\tFormat version, group order\n",
      "Found 808 SQ lines, 1 PG lines\n",
      "SAM file looks good, no changes needed\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Parsing SAM file: 19061it [00:00, 218479.12it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved x_mapper results to /tmp/spacer_bench_duckdb_y3h9u8bw/x_mapper.parquet\n",
      "\n",
      "Combining results from 10 tools using DuckDB...\n",
      "Writing combined results to results/real_data/subsamples_analysis/alignments_fraction_0.001.parquet using DuckDB (no materialization)...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "60c991d67fdc4c1d929441e85c5ad6e5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, layout=Layout(width='auto'), style=ProgressStyle(bar_color='black'))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Results saved to results/real_data/subsamples_analysis/alignments_fraction_0.001.parquet\n",
      "Cleaned up temporary directory: /tmp/spacer_bench_duckdb_y3h9u8bw\n",
      "\n",
      "=== Loading fraction 0.005 ===\n",
      "  Loading 11 tools: blastn, bowtie1, bowtie2, indelfree_bruteforce, indelfree_indexed, lexicmap, minimap2, mummer4, sassy, strobealign, x_mapper\n",
      "\n",
      "[ThreadControl] Setting environment variables for 8 threads\n",
      "  OMP_NUM_THREADS=None\n",
      "  OPENBLAS_NUM_THREADS=None\n",
      "  DuckDB query parallelism: 8 threads\n",
      "  DuckDB memory limit: 50GB\n",
      "\n",
      "Reading results for blastn...\n",
      "File size: 5.80 MB\n",
      "Saved blastn results to /tmp/spacer_bench_duckdb_ukrqrzta/blastn.parquet\n",
      "\n",
      "Reading results for bowtie1...\n",
      "File size: 498.79 MB\n",
      "first line of sam file @HD\tVN:1.0\tSO:unsorted\n",
      "second line of sam file @SQ\tSN:IMGVR_UViG_3300006944_000023|3300006944|Ga0099823_1018790\tLN:6647\n",
      "Found 1907 SQ lines, 1 PG lines\n",
      "SAM file looks good, no changes needed\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Parsing SAM file: 3901347it [00:03, 1299338.62it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved bowtie1 results to /tmp/spacer_bench_duckdb_ukrqrzta/bowtie1.parquet\n",
      "\n",
      "Reading results for bowtie2...\n",
      "File size: 501.34 MB\n",
      "first line of sam file @HD\tVN:1.5\tSO:unsorted\tGO:query\n",
      "second line of sam file @SQ\tSN:IMGVR_UViG_3300006944_000023|3300006944|Ga0099823_1018790\tLN:6647\n",
      "Found 1907 SQ lines, 1 PG lines\n",
      "SAM file looks good, no changes needed\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Parsing SAM file: 3894198it [00:03, 1245606.14it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved bowtie2 results to /tmp/spacer_bench_duckdb_ukrqrzta/bowtie2.parquet\n",
      "\n",
      "Reading results for indelfree_bruteforce...\n",
      "File size: 278.74 MB\n",
      "first line of sam file @HD\tVN:1.6\tSO:unsorted\n",
      "second line of sam file @SQ\tSN:IMGVR_UViG_3300006944_000023|3300006944|Ga0099823_1018790\tLN:6647\n",
      "Found 1907 SQ lines, 1 PG lines\n",
      "SAM file looks good, no changes needed\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Parsing SAM file: 1823109it [00:05, 311549.76it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved indelfree_bruteforce results to /tmp/spacer_bench_duckdb_ukrqrzta/indelfree_bruteforce.parquet\n",
      "\n",
      "Reading results for indelfree_indexed...\n",
      "File size: 44.94 MB\n",
      "first line of sam file @HD\tVN:1.6\tSO:unsorted\n",
      "second line of sam file @SQ\tSN:IMGVR_UViG_3300006944_000023|3300006944|Ga0099823_1018790\tLN:6647\n",
      "Found 1907 SQ lines, 1 PG lines\n",
      "SAM file looks good, no changes needed\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Parsing SAM file: 275357it [00:01, 174490.97it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved indelfree_indexed results to /tmp/spacer_bench_duckdb_ukrqrzta/indelfree_indexed.parquet\n",
      "\n",
      "Reading results for lexicmap...\n",
      "File size: 0.00 MB\n",
      "Saved lexicmap results to /tmp/spacer_bench_duckdb_ukrqrzta/lexicmap.parquet\n",
      "\n",
      "Reading results for minimap2...\n",
      "File size: 0.13 MB\n",
      "first line of sam file @HD\tVN:1.6\tSO:unsorted\tGO:query\n",
      "second line of sam file @SQ\tSN:IMGVR_UViG_3300006944_000023|3300006944|Ga0099823_1018790\tLN:6647\n",
      "Found 1907 SQ lines, 1 PG lines\n",
      "SAM file looks good, no changes needed\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Parsing SAM file: 9it [00:00, 9284.00it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved minimap2 results to /tmp/spacer_bench_duckdb_ukrqrzta/minimap2.parquet\n",
      "\n",
      "Reading results for mummer4...\n",
      "File size: 10.50 MB\n",
      "first line of sam file @HD\tVN:1.6\tSO:unsorted\n",
      "second line of sam file @SQ\tSN:IMGVR_UViG_3300006944_000023|3300006944|Ga0099823_1018790\tLN:6647\n",
      "Found 1907 SQ lines, 1 PG lines\n",
      "SAM file needs fixing\n",
      "Rewriting HD line for mummer file\n",
      "SAM file has been fixed\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Parsing SAM file: 62131it [00:00, 249813.12it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved mummer4 results to /tmp/spacer_bench_duckdb_ukrqrzta/mummer4.parquet\n",
      "\n",
      "Reading results for sassy...\n",
      "File size: 438791.28 MB\n",
      "Checking existing cache at: /clusterfs/jgi/scratch/science/metagen/neri/code/blits/spacer_bench/results/real_data/subsamples/fraction_0.005/raw_outputs/sassy_parsed_20260114.parquet\n",
      ">> Cache file exists but appears corrupt/incomplete (parquet: File out of specification: The file must end with PAR1). Reprocessing...\n",
      "\n",
      "[ThreadControl] Setting environment variables for 8 threads (Sassy)\n",
      "Starting DuckDB Pipeline (Writing to: /clusterfs/jgi/scratch/science/metagen/neri/code/blits/spacer_bench/results/real_data/subsamples/fraction_0.005/raw_outputs/sassy_parsed_20260114.parquet)...\n",
      "Failed to process Sassy file: Query interrupted\n",
      "Saved sassy results to /tmp/spacer_bench_duckdb_ukrqrzta/sassy.parquet\n",
      "\n",
      "Reading results for strobealign...\n",
      "File size: 918.29 MB\n"
     ]
    },
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mThe Kernel crashed while executing code in the current cell or a previous cell. \n",
      "\u001b[1;31mPlease review the code in the cell(s) to identify a possible cause of the failure. \n",
      "\u001b[1;31mClick <a href='https://aka.ms/vscodeJupyterKernelCrash'>here</a> for more info. \n",
      "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "# all_results = {}\n",
    "# fractions = [0.001, 0.005, 0.01, 0.05, 0.1]\n",
    "fractions = [0.001, 0.005, 0.01] #, 0.05, 0.1] # only fractions for which all tools finished\n",
    "\n",
    "for frac in fractions:\n",
    "    print(f\"\\n=== Loading fraction {frac} ===\")\n",
    "    frac_dir = f\"{base_dir}/fraction_{frac}\"\n",
    "    contigs_file = f\"{frac_dir}/subsampled_data/subsampled_contigs.fa\"\n",
    "    \n",
    "    # Only load completed tools\n",
    "    completed_tools = completion_status[frac]['completed']\n",
    "    if not completed_tools:\n",
    "        print(f\"  No completed tools for fraction {frac}, skipping\")\n",
    "        continue\n",
    "    \n",
    "    # Load tool configurations\n",
    "    tools = load_tool_configs(\n",
    "        results_dir=frac_dir,\n",
    "        threads=8,\n",
    "        contigs_file=contigs_file,\n",
    "        spacers_file=spacers_file\n",
    "    )\n",
    "    \n",
    "    # Filter to only completed tools\n",
    "    tools_to_load = {k: v for k, v in tools.items() if k in completed_tools}\n",
    "    print(f\"  Loading {len(tools_to_load)} tools: {', '.join(tools_to_load.keys())}\")\n",
    "    \n",
    "    # Read results with max_mismatches filter\n",
    "    try:\n",
    "        results_df = read_results(\n",
    "            tools_to_load,\n",
    "            max_mismatches=MAX_MISMATCHES+2, #tool reported, not validated for the scalling tests\n",
    "            spacer_lendf=spacer_lendf,\n",
    "            ref_file=contigs_file,\n",
    "            threads=8,\n",
    "            memory_limit=\"50GB\",\n",
    "            output_parquet=f'results/real_data/subsamples_analysis/alignments_fraction_{frac}.parquet'\n",
    "        )\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"  Error loading results: {e}\")\n",
    "        continue\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_results = pl.scan_parquet('results/real_data/subsamples_analysis/alignments_fraction_*parquet',)\n",
    "print(f\"  Loaded {results_df.height:,} alignments from {results_df['tool'].n_unique()} tools\")\n",
    "print(f\"  Unique spacers: {results_df['spacer_id'].n_unique():,}, contigs: {results_df['contig_id'].n_unique():,}\")\n",
    "print(f\"\\n✓ Loaded results from {len(all_results)} fractions\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Per-Fraction Analysis (No Aggregation)\n",
    "\n",
    "Analyze each fraction separately since they're stratified samples from the same dataset and are not independent."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Analyze each fraction separately (they are NOT independent samples)\n",
    "# For each fraction, we'll compute summary stats\n",
    "\n",
    "per_fraction_stats = {}\n",
    "\n",
    "for frac, results_df in all_results.items():\n",
    "    print(f\"\\n=== Fraction {frac} Statistics ===\")\n",
    "    \n",
    "    # Basic stats\n",
    "    print(f\"Total alignments: {results_df.height:,}\")\n",
    "    print(f\"Unique spacers: {results_df['spacer_id'].n_unique():,}\")\n",
    "    print(f\"Unique contigs: {results_df['contig_id'].n_unique():,}\")\n",
    "    print(f\"Tools: {results_df['tool'].n_unique()}\")\n",
    "    \n",
    "    # Per-tool summary\n",
    "    tool_summary = results_df.group_by('tool').agg([\n",
    "        pl.col('spacer_id').n_unique().alias('n_unique_spacers'),\n",
    "        pl.col('contig_id').n_unique().alias('n_unique_contigs'),\n",
    "        pl.len().alias('n_total_alignments'),\n",
    "        pl.col('mismatches').mean().alias('mean_mismatches'),\n",
    "        pl.col('mismatches').median().alias('median_mismatches'),\n",
    "        (pl.col('mismatches') == 0).sum().alias('n_perfect_matches'),\n",
    "    ]).sort('n_unique_spacers', descending=True)\n",
    "    \n",
    "    per_fraction_stats[frac] = tool_summary\n",
    "    print(\"\\nTop 5 tools by unique spacers:\")\n",
    "    print(tool_summary.head(5))\n",
    "\n",
    "# Display all stats\n",
    "print(\"\\n\\n=== Summary Table: All Fractions ===\")\n",
    "for frac in sorted(per_fraction_stats.keys()):\n",
    "    print(f\"\\n--- Fraction {frac} ---\")\n",
    "    print(per_fraction_stats[frac])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## DuckDB-based Multi-Fraction Analysis (Memory Efficient)\n",
    "\n",
    "This section demonstrates loading multiple fractions using DuckDB for memory-efficient analysis without materializing all data into memory simultaneously.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from bench.utils.functions import load_multiple_fractions_duckdb, query_fractions_duckdb, load_fraction_results_lazy\n",
    "\n",
    "# Build mapping of fractions to parquet files\n",
    "fraction_parquet_files = {}\n",
    "for frac in fractions:\n",
    "    pf = f'results/real_data/subsamples_analysis/alignments_fraction_{frac}.parquet'\n",
    "    if os.path.exists(pf):\n",
    "        fraction_parquet_files[frac] = pf\n",
    "        print(f\"Fraction {frac}: {os.path.getsize(pf) / 1024 / 1024:.1f} MB\")\n",
    "    else:\n",
    "        print(f\"Fraction {frac}: NOT FOUND\")\n",
    "\n",
    "print(f\"\\nTotal fractions available: {len(fraction_parquet_files)}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Option 1: Use DuckDB to query across all fractions efficiently\n",
    "if len(fraction_parquet_files) > 0:\n",
    "    print(\"\\n=== Using DuckDB for multi-fraction analysis ===\")\n",
    "    con = load_multiple_fractions_duckdb(fraction_parquet_files, threads=8, memory_limit=\"50GB\")\n",
    "    \n",
    "    # Example query: Count alignments per tool per fraction\n",
    "    query = \"\"\"\n",
    "    SELECT \n",
    "        fraction, \n",
    "        tool, \n",
    "        COUNT(*) as alignment_count,\n",
    "        COUNT(DISTINCT spacer_id) as unique_spacers,\n",
    "        COUNT(DISTINCT contig_id) as unique_contigs,\n",
    "        AVG(mismatches) as mean_mismatches,\n",
    "        MIN(mismatches) as min_mismatches,\n",
    "        MAX(mismatches) as max_mismatches\n",
    "    FROM fractions\n",
    "    GROUP BY fraction, tool\n",
    "    ORDER BY fraction DESC, alignment_count DESC\n",
    "    \"\"\"\n",
    "    \n",
    "    results_summary = query_fractions_duckdb(con, query)\n",
    "    print(\"\\nPer-tool summary across all fractions:\")\n",
    "    display(results_summary)\n",
    "else:\n",
    "    print(\"No fraction parquet files found, skipping DuckDB analysis\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Option 2: Load fractions as lazy Polars frames for further processing\n",
    "print(\"\\n=== Using Polars lazy evaluation for multi-fraction analysis ===\")\n",
    "\n",
    "if len(fraction_parquet_files) > 0:\n",
    "    combined_lazy = load_fraction_results_lazy(fraction_parquet_files, add_fraction_col=True)\n",
    "    \n",
    "    # Example: Aggregate stats without materializing all data\n",
    "    stats_lazy = combined_lazy.group_by(['fraction', 'tool']).agg([\n",
    "        pl.col('spacer_id').n_unique().alias('n_unique_spacers'),\n",
    "        pl.col('contig_id').n_unique().alias('n_unique_contigs'),\n",
    "        pl.len().alias('n_total_alignments'),\n",
    "        pl.col('mismatches').mean().alias('mean_mismatches'),\n",
    "        (pl.col('mismatches') == 0).sum().alias('n_perfect_matches'),\n",
    "    ]).sort(['fraction', 'n_unique_spacers'], descending=[False, True])\n",
    "    \n",
    "    # Collect only the aggregated result (much smaller than original data)\n",
    "    print(\"Collecting aggregated statistics...\")\n",
    "    stats_df = stats_lazy.collect(engine='streaming')\n",
    "    print(f\"Aggregated stats: {stats_df.height} rows\")\n",
    "    display(stats_df)\n",
    "else:\n",
    "    print(\"No fraction parquet files found\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Recalculate Alignments with Parasail for Deterministic Mismatch Counts\n",
    "\n",
    "The tool-reported mismatches can vary. We'll recalculate them deterministically using parasail.\n",
    "This follows the same workflow as the original full-dataset notebook."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %%time\n",
    "# For each fraction, recalculate alignments\n",
    "recalculated_results = {}\n",
    "\n",
    "for frac, results_df in all_results.items():\n",
    "    print(f\"\\n=== Recalculating alignments for fraction {frac} ===\")\n",
    "    frac_dir = f\"{base_dir}/fraction_{frac}\"\n",
    "    contigs_file = f\"{frac_dir}/subsampled_data/subsampled_contigs.fa\"\n",
    "    \n",
    "    # Get unique regions (spacer-contig pairs with coordinates)\n",
    "    unique_regions = results_df.select([\n",
    "        \"spacer_id\", \"contig_id\", \"strand\", \"start\", \"end\"\n",
    "    ]).unique()\n",
    "    \n",
    "    print(f\"Unique regions to verify: {unique_regions.height:,}\")\n",
    "    \n",
    "    # Populate with spacer sequences\n",
    "    print(\"  Loading spacer sequences...\")\n",
    "    unique_regions = populate_pldf_withseqs_needletail(\n",
    "        seqfile=spacers_file,\n",
    "        pldf=unique_regions,\n",
    "        chunk_size=2000000,\n",
    "        reverse_by_strand_col=False,\n",
    "        trim_to_region=False,\n",
    "        idcol=\"spacer_id\",\n",
    "        seqcol=\"spacer_seq\"\n",
    "    )\n",
    "    \n",
    "    # Populate with contig sequences (trimmed to region)\n",
    "    print(\"  Loading contig sequences...\")\n",
    "    unique_regions = populate_pldf_withseqs_needletail(\n",
    "        seqfile=contigs_file,\n",
    "        trim_to_region=True,\n",
    "        reverse_by_strand_col=True,\n",
    "        chunk_size=200000,\n",
    "        pldf=unique_regions,\n",
    "        idcol=\"contig_id\",\n",
    "        start_col=\"start\",\n",
    "        end_col=\"end\",\n",
    "        strand_col=\"strand\",\n",
    "        seqcol=\"contig_seq\"\n",
    "    )\n",
    "    \n",
    "    # Recalculate mismatches using parasail\n",
    "    print(\"  Recalculating mismatches with parasail...\")\n",
    "    recalced = test_alignment_polars(\n",
    "        results=unique_regions,\n",
    "        return_deviations=False,\n",
    "        ignore_region_strands=True\n",
    "    )\n",
    "    \n",
    "    # Rename columns for clarity\n",
    "    recalced = recalced.rename({\"alignment_test\": \"recalced_mismatches\"})\n",
    "    \n",
    "    # Join back with original results\n",
    "    results_with_recalc = results_df.join(\n",
    "        recalced[[\"spacer_id\", \"contig_id\", \"strand\", \"start\", \"end\", \n",
    "                  \"spacer_seq\", \"contig_seq\", \"recalced_mismatches\"]],\n",
    "        on=[\"spacer_id\", \"contig_id\", \"strand\", \"start\", \"end\"],\n",
    "        how=\"left\"\n",
    "    )\n",
    "    \n",
    "    # Rename original mismatches for clarity\n",
    "    results_with_recalc = results_with_recalc.rename({\n",
    "        \"mismatches\": \"tool_reported_mismatches\"\n",
    "    }).rename({\n",
    "        \"recalced_mismatches\": \"mismatches\"\n",
    "    })\n",
    "    \n",
    "    # Check for deviations\n",
    "    results_with_recalc = results_with_recalc.with_columns(\n",
    "        (pl.col(\"mismatches\") - pl.col(\"tool_reported_mismatches\")).alias(\"deviation\")\n",
    "    )\n",
    "    \n",
    "    n_deviations = results_with_recalc.filter(pl.col(\"deviation\") != 0).height\n",
    "    print(f\"  Alignments with deviations: {n_deviations:,} ({100*n_deviations/results_with_recalc.height:.2f}%)\")\n",
    "    \n",
    "    # Filter to max_mismatches after recalculation\n",
    "    results_with_recalc = results_with_recalc.filter(pl.col(\"mismatches\") <= MAX_MISMATCHES)\n",
    "    print(f\"  Alignments after filtering (≤{MAX_MISMATCHES} mismatches): {results_with_recalc.height:,}\")\n",
    "    \n",
    "    recalculated_results[frac] = results_with_recalc\n",
    "\n",
    "print(f\"\\n✓ Recalculated alignments for {len(recalculated_results)} fractions\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Updated Per-Fraction Statistics (with Recalculated Mismatches)\n",
    "\n",
    "Now compute statistics using the recalculated mismatches from parasail."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Recompute statistics with recalculated mismatches\n",
    "updated_per_fraction_stats = {}\n",
    "\n",
    "for frac, results_df in recalculated_results.items():\n",
    "    print(f\"\\n=== Fraction {frac} (Recalculated Mismatches) ===\")\n",
    "    \n",
    "    # Per-tool summary\n",
    "    tool_summary = results_df.group_by('tool').agg([\n",
    "        pl.col('spacer_id').n_unique().alias('n_unique_spacers'),\n",
    "        pl.col('contig_id').n_unique().alias('n_unique_contigs'),\n",
    "        pl.len().alias('n_total_alignments'),\n",
    "        pl.col('mismatches').mean().alias('mean_mismatches'),\n",
    "        pl.col('mismatches').median().alias('median_mismatches'),\n",
    "        (pl.col('mismatches') == 0).sum().alias('n_perfect_matches'),\n",
    "        pl.col('deviation').mean().alias('mean_deviation_from_tool'),\n",
    "        (pl.col('deviation') != 0).sum().alias('n_with_deviation'),\n",
    "    ]).sort('n_unique_spacers', descending=True)\n",
    "    \n",
    "    updated_per_fraction_stats[frac] = tool_summary\n",
    "    display(tool_summary)\n",
    "\n",
    "# Save updated stats\n",
    "print(\"\\n✓ Updated statistics computed\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Visualizations: Comparing Tools Across Fractions\n",
    "\n",
    "Plot tool performance metrics across subsample sizes using the recalculated mismatches."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.makedirs('results/real_data/subsamples_analysis', exist_ok=True)\n",
    "# Combine stats from all fractions for plotting\n",
    "plot_data_list = []\n",
    "for frac in sorted(updated_per_fraction_stats.keys()):\n",
    "    stats_df = updated_per_fraction_stats[frac].with_columns(pl.lit(frac).alias('fraction'))\n",
    "    plot_data_list.append(stats_df)\n",
    "\n",
    "combined_stats = pl.concat(plot_data_list)\n",
    "\n",
    "# Plot 1: Number of unique spacers found per tool across fractions\n",
    "fig, ax = plt.subplots(figsize=(14, 7))\n",
    "\n",
    "# Plot tools in consistent order with assigned colors/markers\n",
    "for tool in TOOL_ORDER:\n",
    "    tool_data = combined_stats.filter(pl.col('tool') == tool)\n",
    "    if tool_data.height == 0:\n",
    "        continue\n",
    "    tool_data = tool_data.sort('fraction')\n",
    "    style = TOOL_STYLES.get(tool, {'color': 'gray', 'marker': 'o'})\n",
    "    \n",
    "    ax.plot(tool_data['fraction'].to_list(), \n",
    "            tool_data['n_unique_spacers'].to_list(), \n",
    "            marker=style['marker'], \n",
    "            color=style['color'],\n",
    "            label=tool, \n",
    "            linewidth=2, \n",
    "            markersize=8, \n",
    "            alpha=0.85)\n",
    "\n",
    "ax.set_xlabel('Subsample Fraction', fontsize=13)\n",
    "\n",
    "ax.set_ylabel('Number of Unique Spacers Found', fontsize=13)\n",
    "print(f\"✓ Saved unique spacers plot (≤{MAX_MISMATCHES} mismatches)\")\n",
    "\n",
    "ax.set_title(f'Tool Performance: Unique Spacers vs Subsample Size (Recalculated ≤{MAX_MISMATCHES} mismatches)')\n",
    "plt.show(fontsize=14, fontweight='bold', pad=15)\n",
    "plt.savefig('results/real_data/subsamples_analysis/unique_spacers_vs_fraction.png', dpi=300, bbox_inches='tight')\n",
    "ax.set_xscale('log')\n",
    "plt.savefig('results/real_data/subsamples_analysis/unique_spacers_vs_fraction.pdf', bbox_inches='tight')\n",
    "\n",
    "ax.set_yscale('log')\n",
    "plt.tight_layout()\n",
    "\n",
    "ax.legend(bbox_to_anchor=(1.05, 1), loc='upper left', fontsize=10, framealpha=0.9)\n",
    "ax.grid(True, alpha=0.3, which='both')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot 2: Number of unique contigs matched per tool across fractions\n",
    "fig, ax = plt.subplots(figsize=(14, 7))\n",
    "\n",
    "# Plot tools in consistent order with assigned colors/markers\n",
    "for tool in TOOL_ORDER:\n",
    "    tool_data = combined_stats.filter(pl.col('tool') == tool)\n",
    "    if tool_data.height == 0:\n",
    "        continue\n",
    "    tool_data = tool_data.sort('fraction')\n",
    "    style = TOOL_STYLES.get(tool, {'color': 'gray', 'marker': 'o'})\n",
    "    \n",
    "    ax.plot(tool_data['fraction'].to_list(), \n",
    "            tool_data['n_unique_contigs'].to_list(), \n",
    "            marker=style['marker'],\n",
    "            color=style['color'],\n",
    "            label=tool, \n",
    "            linewidth=2, \n",
    "            markersize=8,\n",
    "            alpha=0.85)\n",
    "\n",
    "ax.set_xlabel('Subsample Fraction', fontsize=12)\n",
    "ax.set_ylabel('Number of Unique Contigs Matched', fontsize=12)\n",
    "ax.set_title(f'Tool Performance: Unique Contigs Across Subsample Sizes (≤{MAX_MISMATCHES} mismatches)', \n",
    "             fontsize=14, fontweight='bold')\n",
    "ax.set_xscale('log')\n",
    "\n",
    "ax.set_yscale('log')\n",
    "print(f\"✓ Saved contigs plot (≤{MAX_MISMATCHES} mismatches)\")\n",
    "\n",
    "ax.legend(bbox_to_anchor=(1.05, 1), loc='upper left', fontsize=10)\n",
    "plt.show()\n",
    "\n",
    "ax.grid(True, alpha=0.3)\n",
    "plt.savefig('results/real_data/subsamples_analysis/contigs_by_fraction.png', dpi=150, bbox_inches='tight')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig('results/real_data/subsamples_analysis/contigs_by_fraction.pdf', dpi=300, bbox_inches='tight')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Conclusion\n",
    "In the these selected subsampling sizes, the results of each tool appear consist (similar ratios compared to the same tool but the other size fractions).\n",
    "This suggests we should be able to extrapolate/assume that should we have the CPU time to run all tools on the entire dataset (no subsampling) the results would be qualitative similar (from tool vs tool comparison perspective)."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "default",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.23"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
