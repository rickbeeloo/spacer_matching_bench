---
title: "Computational Tool Choice Impacts CRISPR Spacer-Protospacer Detection"
author:
  - name: Uri Neri*^1^
    corresponding: true
  - name: Antonio Pedro Camargo^1^
  - name: Brian Bushnell^1^
  - name: Rick Beeloo^2^
  - name: Simon Roux^1^
format:
  pdf:
    documentclass: article
    geometry:
      - margin=1in
    fig-format: pdf
    embed-resources: true
    keep-tex: true
    fig-pos: 'H'
    number-sections: true
    cite-method: biblatex
    bibliography: references.bib
    pdf-engine: xelatex
  html:
    toc: true
    toc-depth: 3
    number-sections: true
  docx: default
execute:
  echo: false
  warning: false
---
1: DOE Joint Genome Institute, Berkeley, CA, USA  
2: Utrecht University, Padualaan 8, Utrecht, NL 3584 CH  
* Uri Neri (uneri@lbl.gov)


## Abstract {#sec-abstract}

CRISPR (Clustered Regularly Interspaced Short Palindromic Repeats) systems are a fundamental defense mechanism in prokaryotes, where short sequences called spacers are stored in the host genome to recognize and target exogenous genetic elements. Viromics, the study of viral communities in environmental samples, relies heavily on identifying these spacer-target interactions to understand host-virus relationships. However, the choice of sequence search tool to identify putative spacer targets is often overlooked, leading to an unknown impact of downstream inferences in virus-host analysis. Here, we utilize simulated and real datasets to compare popular sequence alignment and search tools, revealing critical differences in their ability to detect multiple matches and handle varying degrees of sequence identity between spacers and potential targets. Finally, we provide general guidelines that may inform future research regarding matching, which is a common practice in studying the complex nature of host-MGE interactions.


## Introduction  {#sec-introduction}

CRISPR (clustered regularly interspaced short palindromic repeats) systems play a vital role in prokaryotic defense against mobile genetic elements, including viruses, plasmids, and other autonomous genetic elements [@Mojica_2005; @CRISPR_review]. These systems are organized as arrays in the bacteria or archaea genome, where short sequences called spacers are interspersed between repeated sequences. The spacer sequences within these arrays guide the targeting of invasive genetic elements, allowing for specific defense against these threats [@CRISPR_classification]. The corresponding locus on the virus genome where the spacer complements is termed "protospacer". The analysis of spacer-protospacer pairs is essential in understanding the complex interactions between hosts and MGEs [@Edwards2015_phage_host].

The identification of genuine host-MGE interactions through spacer-protospacer matching presents unique challenges due to the dynamic nature of these relationships and the complexity of sequence evolution. While matches between spacers and protospacers are often interpreted as evidence of interaction, various biological and technical factors can complicate this interpretation [@Edwards2015_phage_host; @soto_perez_crispr_2019].

Several key scenarios can lead to false positive assignments in spacer-protospacer matching. Low complexity sequences can create spurious matches between simple repeat regions (albeit these can be mitigated through complexity filtering via Shannon entropy or DUST). Another type of potential false positives are highly conserved sequences shared by unrelated MGEs, potentially resulting from horizontal gene transfer between MGEs. The horizontal transfer of CRISPR arrays themselves on mobile elements further requires careful examination of array genomic context (regions outside the CRISPR loci) and phylogenetic analysis. Self-targeting events, where matches occur against the host genome rather than MGEs, necessitate comparison against host genome databases and analysis of targeting context. Finally, historical acquisition events may not reflect current interactions, requiring consideration of phylogenetic dating, evolution rates and the effects of the protospacers being under selective pressure to mutate (which may reduce the MGE susceptibility to deterioration by the CRISPR system). This is further complicated by the fact that increasing the allowed distance between sequences directly increases the likelihood of considering sequences similar or related to each other.

False negatives present another challenge in spacer-protospacer matching, particularly when dealing with large databases of potential targets. Many alignment and search tools default to reporting only the best matches or the first matches that pass a given threshold for a given query or HSP. This may result in potentially missing additional legitimate matches. Unfortunately, different tools also handle ambiguous or secondary alignments differently: they may be reported, reported up to a number or based on relative alignment quality, or omitted. Similarly, cases where a query sequence has multiple matches in different reference sequences are not handled uniformly across tools. This limitation becomes increasingly problematic as databases grow larger and more diverse, a single spacer might match (implying a targeting) multiple related MGEs.

Yet despite these variations, the choice of spacer-to-protospacer search or alignment tool is often not deeply considered. Presently, the common option for this task, popularized by Edwards et al and Biswas et al [@Edwards2015_phage_host; @Biswas2013], uses BLASTn [@Altschul1990_blast] with parameters adjusted for short input sequences. However for most bioinformatic tools, the exact workflow design and parameter choice can impact the outcome, including in sequence analysis. The importance of proper tool usage and parameter interpretation is highlighted by historical examples in bioinformatics. A striking example is the work of Shah et al, @Shah2018, in which they report how certain misunderstandings of BLAST's `-max_target_seqs` parameter may lead to incorrect assumptions about result completeness, potentially impacting published analyses. Albeit this was later clarified by Madden et al., [@Madden2018] (of the blast development team) as an unfortunate combination of a software bug (that were since patched) affecting rare cases, and misconceptions regarding the process BLAST+ uses for tie-breaking (alignments of equal plausibility), and finally a consideration regarding composition base scoring. Apart from the patched bug, the main outcome of this correspondence led to more explicit details in blast documentation (specifically the appendix "Outline of the BLAST process"). Still, this highlights that misconceptions about the expected exhaustiveness of tools' result-reporting can also lead to incorrect assumptions about the outcome of an analysis. In practice, most bioinformatic tools use various heuristics and optimizations, typically designed with specific use cases in mind. For example, most short-read mappers assume the reference to be the output of a singular assembly - which would imply the reference does not contain extremely redundant copies of the same loci, or a limited number of very similar sequences (e.g. strain variants, alternative splice variants), and this assumption impacts the way read mapping is computed and results are reported.

The choice of tool and its parameters can significantly impact the detection of these multiple matches, with some tools prioritizing speed over completeness by limiting the number of reported matches, or by other internal heuristics such as seed sequence selection from high occurring sequences being penalized. This trade-off between sensitivity and computational efficiency is especially important to consider as most available tools were designed for different tasks than spacer-protospacer matching (e.g. expression analysis, homology detection, and variant calling), and under different assumptions (such as reference and query sequence size and database size or the nature of the reference source: from a single isolate or metagenomic sample rather than from aggregation of sequences from different sources).

**Sequence Alignment Methods and Their Design Constraints:**

Modern sequence alignment/search tools were developed to address diverse computational challenges in genomics, such as protein domain function inference, taxonomic profiling, abundance calculation, quantitative/compartive expression analyses and so on.  
These different use caseare often optimized (or have optimized variants) for specific use cases with distinct assumptions and performance trade-offs.  
For example, tools designed for homology detection and  (e.g. PSI-BLAST, MMseqs2 profile) may prioritise (or be used when) high sensitivity even at low sequence identity levels.  
    specificity through multi-stage filtering and statistical scoring (e-values, bit scores), aiming to distinguish true homologs from spurious matches across evolutionary timescales. Read mappers for differential expression analysis (Bowtie2, STAR) assume single-source references and optimize for unique best-hit reporting, using mapping quality scores to reflect alignment confidence. Variant callers and genome assemblers require exhaustive local exploration but can tolerate higher computational costs for small, targeted regions. These tools employ various heuristics—seed-based indexing (FM-index in Bowtie, minimizers in Minimap2, strobemers in StrobeAlign), k-mer filtering (MMseqs2's double k-mer matches), chaining algorithms (collinear chaining in Minimap2), and early termination strategies—to achieve practical performance on large datasets. Common reporting metrics include alignment scores (reflecting match/mismatch penalties), e-values (expected number of matches by chance given database size), mapping quality (phred-scaled probability of correct placement), and distance thresholds (edit distance allowing indels, hamming distance for substitutions only, or exact matching). Critically, these design choices reflect the intended application: BLAST's e-value framework assumes searching for distant homologs across diverse sequences, while read mappers assume low divergence from a single reference assembly. Understanding these algorithmic assumptions is essential when repurposing tools for spacer-protospacer matching, where the biological constraints (short sequences, moderate divergence, potentially high target abundance) differ substantially from the original design specifications.

**Algorithmic Approaches and Distance Metrics:**
*** NOTE THIS NEEDS TO BE EXTENDED / MORE DETAILED (which heuristics, what algos, what intended uses...)

A fundamental consideration in tool selection is the underlying algorithmic approach and distance metric used. Tools can be broadly categorized as:

1. **Exhaustive vs Heuristic:** Exhaustive methods guarantee finding all matches within a specified distance threshold but are computationally intensive. Heuristic methods use various optimizations (seed-based indexing, chaining algorithms, k-mer filtering) to dramatically improve speed but may miss some matches. For large-scale analyses, heuristic methods are often necessary, but understanding their limitations is critical.

2. **Distance Metric Type:** Tools differ fundamentally in how they measure sequence similarity:
   - **Hamming distance** counts only substitutions (e.g., bowtie1, indelfree.sh). Requires query and subject string to be of same length (albeit some tools using hamming distance may support partial alignments made by clipping/truncating terminal bases)
   - **Edit/affine distance** allows insertions and deletions in addition to substitutions (e.g., bowtie2, minimap2, BLAST, sassy, mmseqs)
   - **Exact matching** requires perfect sequence identity (e.g., spacer-containment, grep, )

Importantly, when comparing tools using different distance metrics with the same numeric threshold (e.g., "≤3 mismatches"), edit/affine-based algorithms will naturally report more matches than hamming-based ones. This reflects different computational problems being solved rather than differences in tool quality. From a biological perspective, the choice of distance metric should reflect the expected mutation patterns in the system being studied. 

**Distance Metric Choice and Experimental Evidence:**

For CRISPR spacer-protospacer matching, several lines of evidence suggest hamming distance (substitutions only) is more appropriate than edit distance (allowing indels). Experimental studies of phage escape from CRISPR immunity consistently report that escape mutations are predominantly single nucleotide substitutions, particularly in the PAM-proximal "seed" region where mismatches have the strongest effect on targeting [@Deveau2008; @Semenova2011; @Fineran2014]. While some studies report indels as escape mutations, systematic quantitative comparisons of mutation type frequencies (substitutions vs indels vs rearrangements) across diverse phage-host systems remain lacking in the literature. This creates a potential "observation bias" where substitutions may be easier to detect and characterize than indels, especially in coding-dense phage genomes where frameshifts are often lethal. 

Additionally, bacterial mutation rates show indels occurring ~4× less frequently than substitutions, and phage genomes are typically coding-dense (>90% coding sequences), making frameshift-inducing indels particularly deleterious. These biological constraints suggest that while indels can occur, they are likely rare compared to substitutions in successful escape mutations that allow phage propagation.

Our empirical analysis using semi-synthetic data (~3.7 million real spacers searched against 400k simulated contigs matching IMG/VR4 sequence characteristics) demonstrates that non-planned matches (validated alignments occurring outside planned insertion regions) increase dramatically with allowed distance: 1 exact match (hamming distance 0), 47 alignments at hamming distance ≤1, 2,217 unique alignments at hamming distance ≤2, and 54,388 unique alignments at hamming distance ≤3. These rates correspond to approximately 0.36 non-planned matches per million spacer-bp searched against one million contig-bp at hamming distance ≤3. When allowing indels (edit distance), non-planned match rates increase substantially (see Methods: Hamming vs Edit Distance Comparison and Supplementary Note 3 for detailed methodology and quantified rates).

Another potential consideration is the computational resources requirements. Memory, storage, and availability of CPU cores are factors differing between tools. Parameter choice may also impact these factors considerably, with certain tools offering tunable parameters to trade-off between sensitivity and computational efficiency. In recent years, spacer database size has been rapidly increasing - from 366,799 unique spacers in 2017 [@Shmakov_2017] to 1,173,006 unique spacers reported in 2021 [@Dion_2021] to 3,835,942 unique spacers in 2023 [@camargo_img_vr4_2023]. Similarly, public virus and MGE databases are growing rapidly, with large contributions from metagenomic samples resulting in routine fold increases in the number of predicted viral contigs [@camargo_img_vr4_2023]. Most tools require more resources as the size of the database grows, and as this trend continues, certain workflows and tools may become prohibitively expensive to run in a reasonable time frame.


## Methods {#sec-methods}

### Tool Selection {#sec-tool-selection}

We evaluated several widely-used sequence alignment and search tools, spanning different algorithmic approaches and computational strategies. The tools were selected based on their availability, historical use in sequence analysis, and diversity of algorithmic approaches. The selection includes both exhaustive methods (Sassy, indelfree.sh in bruteforce mode) that guarantee finding all matches within specified distance thresholds, and heuristic methods that use various optimizations for improved speed.

It is important to note that most of these tools were not specifically designed for CRISPR spacer-protospacer matching, but rather for more general sequence search tasks (MMseqs2, BLASTn-short, LexicMap), alignment/mapping of short reads to reference genomes (Bowtie1, Bowtie2, Minimap2, MUMmer4, StrobeAlign, X-mapper), or versatile pattern matching (Sassy, indelfree.sh). Our focus is specifically on spacer-to-protospacer sequence matching as a bioinformatics task, and we did not evaluate integrated host-prediction tools like SpacePHARER [@Zhang_2021] or iPHoP [@Roux2023_iphop], which perform additional analyses such as phylogenetic evaluation or LCA determination from multiple spacer-protospacer matches.

All tools were configured to maximize sensitivity and avoid artificial limitations on multiple match detection. Some tools required specific parameter adjustments to enable detection of short sequences (e.g., BLASTn-short task, Bowtie1/2 short read modes) or to report all matches rather than only top hits. The exhaustive tools (Sassy, indelfree.sh bruteforce mode) were included specifically to validate the completeness of heuristic tool results on smaller datasets where computational costs remain feasible.


| Aligner | Indexing | Main Algorithm | Heuristic/Exhaustive | Reporting/Limiting Threshold Used in Benchmark| Year | Original Purpose | Notes |
|:--------|:---------:|:---------------|:---------------------|:-----------------------------|:----:|:-----------------|:------|
| [Bowtie1](https://github.com/BenLangmead/bowtie) | Yes | FM-Index (BWT) | Yes (backtracking) | Hamming distance | 2009 | Short read mapping | Optimized for 25-50 bp reads (max 1kbp); ungapped alignment only; backtracking heuristic limits to 3 mismatches |
| [Bowtie2](https://github.com/BenLangmead/bowtie2) | Yes | FM-Index (BWT) | Yes (multiseed + extend) | Affine/Edit distance | 2012 | Read mapping | Uses FM-index for seeding with SIMD-accelerated DP extension; supports gapped, local, and end-to-end alignment |
| [Minimap2](https://github.com/lh3/minimap2) | Optional | Minimizer + chaining | Yes (minimizer seeding) | Edit distance | 2018 | Long read mapping | Lexicographically smallest k-mer per window; collinear chaining with gap penalties; versatile across read types |
| [indelfree.sh](https://github.com/bbushnell/BBTools/) | No | Multi-kmer matching | Bruteforce mode is exhaustive, while ion "Indexed" mode this can be limited by selected kmer length, query length, and number of substitutions | Hamming distance | Publically introduced to bbtools September 2025 | Read mapping | BBTools suite is Java based, and will use available memory - so the peak memory reported herein (sourced from SLURM logs) does not equate with "minimal required memory" |
| [StrobeAlign](https://github.com/ksahlin/StrobeAlign) | Yes | Randstrobes | Yes (syncmer thinning) | Edit distance | 2022 | Read mapping | Uses hash-based linked strobes (randstrobes) with multi-context seeds (MCS) for hierarchical search |
| [BLAST+](https://blast.ncbi.nlm.nih.gov/doc/blast-help/downloadblastdata.html) | Optional | Hit-and-extend | Yes (contiguous word) | E-value, bit score | 2009 | Sequence search | BLASTN-short mode uses 11-mer seeds; reports matches based on e-value (expected hits by chance given DB size) |
| [MMseqs2](https://github.com/soedinglab/MMseqs2) | Optional | K-mer prefiltering | Yes (3-stage cascade) | E-value, bit score | 2017 | Sequence search | Double k-mer matching → vectorized ungapped → gapped SW; optimized for many-against-many searches |
| [Sassy](https://github.com/RagnarGrootKoerkamp/sassy) | No | Uses a bit-parallel algorithm based on Myers' bitpacking to perform exhaustive approximate string matching (ASM) | Exhaustive | Edit distance | 2024/5 | Versatile pattern matching, suggested for use in CRISPR (gene-editing) off target detection and raw-read alignments| Guarantees perfect recall; explores full edit distance landscape; supports arbitrary distances; high computational cost, requires SIMD instructions (AVX2 and NEON), which most modern CPUs support |
| [X-mapper](https://github.com/mathjeff/Mapper) | Yes | Gapped x-mer pyramid | Yes (dynamic seeds) | Edit distance | 2024 | Read mapping | **Preprint** - Dynamic-length gapped x-mers with "pyramid walking" to optimize seed specificity |
| [mummer4](https://github.com/mummer4/mummer) | Optional | 48-bit suffix array | Yes (MUM-based) | Edit distance | 2018 | Genome alignment | Identifies Maximal Unique Matches (MUMs) using enhanced suffix array; handles genomes up to 141 Tbp |
| [spacer-containment](https://github.com/apcamargo/spacer-containment) | No | String containment | Exhaustive (exact) | Exact match | - | String matching | Basic exact substring matching without mismatch or indel tolerance |
| [LexicMap](https://github.com/shenwei356/LexicMap) | Yes | LexicHash + WFA | Yes (hierarchical index) | Edit distance | 2024 | Gene/genome search | **Preprint** - Window-guaranteed seeding with 15-bp prefix/suffix matching; uses Wavefront Alignment algorithm |


: Evaluated Tools and Their Characteristics. **Indexing:** "Yes" indicates a precomputed index is required (in our benchmark, if an index has to be pre-created the time to construct it is measured along the search/alignment step); "Optional" means a persistent index can be generated for reuse but the tool can also build it on-the-fly for single runs; "No" means no indexing is required or supported. **Heuristic/Exhaustive:** Exhaustive methods guarantee finding all matches within the specified distance threshold; heuristic methods use optimizations (seed-based indexing, chaining, k-mer filtering) to improve speed but may miss some matches. **Reporting/Limiting Threshold:** The primary metric used to report or filter alignments. Hamming distance counts only substitutions; Edit distance allows insertions and deletions; Affine distance uses gap penalties; E-value represents expected matches by chance given database size; Exact match requires perfect identity. Note that edit/affine-based algorithms will naturally report more matches than hamming-based ones when both use the same numeric threshold—this reflects different computational problems being solved rather than tool quality differences. **Tool Configuration:** All tools were configured to maximize sensitivity and avoid artificial limitations on multiple match detection. Exact commands, parameters, and tool versions are provided in Supplementary Table S1. {#tbl-tools}


### Data Generation and acquisition {#sec-data}

#### Dataset Overview {#sec-datasets}

We evaluated tool performance using three complementary datasets with varying levels of ground truth information:

| Dataset Type | Spacers | Spacer Stats | Contigs | Contig Stats | Notes |
|:------------|:--------|:-------------|:--------|:-------------|:------|
| **Fully Synthetic** | 100,000 simulated | 132 Mbp total; 25-100 bp range; median 34 bp; GC 49% | 20,000 simulated | 2.0 Gbp total; 1,501-200,000 bp range; median 100,831 bp; GC 46% | Complete ground truth; planned insertions allow differentiation of true vs non-planned matches |
| **Semi-Synthetic** | 3,835,942 real (iPHoP) | 132 Mbp total; 25-100 bp range; median 34 bp; GC 46.73% | 400,000 simulated | 40.3 Gbp total; 1,501-200,000 bp range; median 100,831 bp; GC 46% | Real spacers + synthetic contigs matching IMG/VR4; enables non-planned match rate estimation in realistic sequence composition |
| **IMG/VR4 Real Data** | 3,835,942 real (iPHoP) | 132 Mbp total; 25-100 bp range; median 34 bp; GC 46.73% | 421,431 HQ contigs | 18.9 Gbp total; 1,001-2,473,870 bp range; median 7,664 bp; GC 44.45% | Real-world performance; HQ subset from 5.1M filtered contigs; subsampled at 0.0005× (279 contigs, 7 Mbp), 0.001× (421 contigs, 10 Mbp), 0.005× (2.1k contigs, 57 Mbp), 0.01× (4.2k contigs, 124 Mbp), 0.05× (21k contigs, 715 Mbp), 0.1× (42k contigs, 1.5 Gbp), and 1× (421k contigs, 18.9 Gbp) using stratified sampling to maintain taxonomic distributions |

: Dataset characteristics and ground truth availability. Synthetic contigs were generated to match IMG/VR4 sequence characteristics (GC content, length distributions). Subsampling of IMG/VR4 HQ contigs enabled inclusion of exhaustive tools on smaller fractions while evaluating all tools on larger fractions and the full HQ dataset. {#tbl-datasets}

#### Synthetic dataset generation {#sec-synthetic-data}

To examine each tool's performance across diverse spacer-to-target matching scenarios, we developed a Python-based simulation framework with fine-grained control over sequence characteristics. The simulator allows specification of target contig length distributions (uniform or normal), GC content variation, spacer length, number of planned occurrences in targets, range of mismatches (substitutions), and reverse complement frequency. Critically, this approach records the ground truth of all planned insertions, enabling differentiation between true positives (planned matches) and non-planned matches (validated alignments occurring in unplanned regions but meeting distance thresholds).

**Enhanced Realism:** Following reviewer suggestions, we configured the synthetic data generation to match key characteristics of the IMG/VR4 dataset rather than using purely random sequences:

- **GC content:** Matched to iPHoP spacer data (~49% for spacers, ~46% for contigs)
- **Length distributions:** Contig lengths follow realistic distributions based on IMG/VR4 viral contig characteristics (range: 1,501-200,000 bp, median: ~100,831 bp)  
- **Sequence complexity:** We systematically verified that k-mer distributions, Shannon entropy, and other sequence characteristics are comparable between synthetic and real sequences using multiple analyses (see Supplementary Figure S6 and notebooks: spacer_inspection.ipynb, distance_metric_analysis.ipynb)

**Non-planned Match Rate Estimation:** Importantly, the synthetic dataset allows us to estimate non-planned match rates as a function of distance threshold and search space size. Matches occurring in regions where we did not plan insertions but that meet the distance threshold represent chance similarities arising from sequence composition and length. Using our semi-synthetic dataset (~3.7 million real spacers searched against 400k simulated contigs), we identified 54,388 validated non-planned matches at hamming distance ≤3, corresponding to approximately 0.36 non-planned matches per million spacer-bp searched against one million contig-bp (see Supplementary Note 1 for detailed calculations).

See Supplementary Figure 1 for a visual representation of the benchmarking framework and data generation workflow.

#### Real datasets {#sec-real-data}

To evaluate tool performance in real-world scenarios, we used predicted viral contigs and CRISPR spacers from recent comprehensive databases. 

**Viral contigs:** We used the IMG/VR4 v1.1 high-confidence viral contigs [@camargo_img_vr4_2023], one of the most comprehensive databases of uncultured phage and viral genomes. The viral contigs were predicted using multiple tools (primarily geNomad [@camargo_genomad_2024]) and supplemented with sequences from NCBI's RefSeq and GenBank. 

To focus on prokaryotic phages and exclude eukaryotic viruses (which typically lack CRISPR systems in their hosts), we applied taxonomic filtering based on ICTV classifications. Specifically, we removed contigs classified into eukaryotic virus families, orders, and classes, including but not limited to major groups such as Adenoviridae, Herpesviridae, Poxviridae, Coronaviridae (families); Herpesvirales, Picornavirales, Bunyavirales (orders); and Megaviricetes, Alsuviricetes, Pokkesviricetes (classes). We additionally filtered out contigs ≤1000 bp to ensure sufficient sequence length for reliable spacer-protospacer matching. After these filtering steps (starting from 5,457,198 high-confidence contigs), the final dataset contains 5,115,894 prokaryotic viral contigs with a total size of ~79 Gbp (range: 1,001 - 2,473,870 bp, median: 7,664 bp, GC%: 44.45%).

For benchmarking, we selected a high-quality (HQ) subset of 421,431 contigs (~18.9 Gbp) using stratified sampling to maintain taxonomic class label distributions while focusing on the most reliable viral sequences. This HQ subset served as the basis for all performance analyses and subsampling experiments.

**CRISPR spacers:** Following reviewer recommendations, we used the curated spacer dataset from iPHoP (June 2025 release) [@Roux2023_iphop], which combines CRISPR spacers from both reference genomes and metagenomes. This dataset represents an updated and more comprehensive collection compared to the original IMG/VR4 spacer predictions, containing 3,835,942 unique spacers (total size: ~132 Mbp, length range: 25-100 bp, median: 34 bp, GC%: 46.73%) compiled from CRISPR arrays identified primarily via piler-cr [@edgar_piler_cr_2007] and CRT [@bland_crt_2007] across IMG/M genomes and metagenomes. This update addresses reviewer concerns about using the most current and comprehensively curated spacer data available.

**Stratified Subsampling Strategy:** For benchmarking purposes, we selected a high-quality (HQ) subset of 421,431 contigs (~18.9 Gbp) from the filtered dataset, maintaining representative taxonomic class distributions. Due to computational constraints from exhaustive tools (Sassy, indelfree.sh bruteforce) and to evaluate tool behavior at different database scales, we created stratified subsamples at seven fractions: 0.0005 (279 contigs, 7.04 Mbp), 0.001 (421 contigs, 9.75 Mbp), 0.005 (2,107 contigs, 57.06 Mbp), 0.01 (4,214 contigs, 123.67 Mbp), 0.05 (21,071 contigs, 715.07 Mbp), 0.1 (42,143 contigs, 1.50 Gbp), and 1.0 (full HQ set: 421,431 contigs, 18.87 Gbp). Each subsample maintains the taxonomic class label distributions from the HQ dataset through stratified sampling. This approach allows us to:

1. Include exhaustive tools in small-scale analyses (0.0005-0.01 fractions) to validate heuristic tool completeness
2. Assess whether tool performance patterns remain consistent across different database sizes
3. Address the practical reality that databases continue growing rapidly (spacers: 366,799 in 2017 → 3,835,942 in 2023)

For the smallest subsamples (0.0005-0.01 fractions, ranging from 279 to 4,214 contigs) where exhaustive tools completed within reasonable computational budgets, we verified that heuristic tools captured the vast majority of matches found by exhaustive approaches. This validation supports our conclusions for larger datasets where exhaustive methods are computationally prohibitive.

**Historical comparison:** The original IMG/VR4 release included spacer-protospacer matching results using blastn with different parameters (specifically default `-max_target_seqs 500`), enabling direct comparison with historical analyses and highlighting the impact of parameter choices on detection outcomes (our analysis shows substantial underestimation with default parameters, particularly for high-abundance spacers).

For more information about the datasets, please refer to the IMG/VR4 [@camargo_img_vr4_2023] and iPHoP [@Roux2023_iphop] publications.


### Coordinate Tolerance and Unique Region Counting {#sec-coordinate-tolerance}

When aggregating results across tools, we implement coordinate tolerance matching to handle slight boundary differences in reported alignments. Tools may report alignments with minor variations in start/end coordinates (typically 1-5 bp) due to different handling of terminal mismatches or gaps. We use a default 5bp tolerance when merging alignments to count unique spacer-contig regions.

For example, if one tool reports an alignment at positions 5000-5036 and another reports 5001-5036 for the same spacer-contig pair, these are counted as a single unique region rather than two separate matches. This approach:
- Reduces double-counting of essentially identical matches
- Accounts for valid algorithmic differences in gap vs substitution placement at alignment boundaries
- Enables fair comparison of tool coverage (total unique regions detected)

All reported alignments are verified by extracting the reference contig region and realigning to the spacer sequence (see Alignment Verification section below). Alignments are classified into three categories:
- **Positive in plan**: Matches ground truth planned insertions (synthetic data only)
- **Positive not in plan**: Valid alignments NOT in ground truth (non-planned matches or real biological matches)
- **Invalid alignment**: Failed verification (exceeds distance threshold)

This coordinate tolerance and verification framework is implemented in the `compare-results` command of our benchmarking tool.

### Alignment Verification and Distance Metric Calculation {#sec-alignment-recalc}

For comparing alignments across tools, we use hamming distance (counting only substitutions) as our primary distance metric, with biological and computational justification provided below. Our benchmarking CLI tool supports setting thresholds for three distance metrics: hamming distance, edit distance, and gap-affine distance. However, for the analyses presented here, we focus primarily on hamming distance due to biological considerations.

To enable comparison with tools using different alignment strategies, we also calculate edit distance (allowing insertions and deletions) for reported matches and conduct systematic comparisons of hamming vs edit distance effects.

**Biological Justification for Hamming Distance:**

**Important Note - Scope of This Analysis:** This benchmark addresses spacer-protospacer matching for inferring historical phage-host interactions, which is fundamentally different from predicting CRISPR off-target effects in gene editing. When considering mismatches between spacers and viral sequences, we explicitly aim to identify evolutionary relationships since protospacer acquisition, where sequence divergence indicates selective pressure to mutate and escape host defenses. For gene editing, even partial base-pairing (including alignments with indels) can cause unwanted off-target cleavage. In contrast, for phage-host evolutionary analysis, known escape mutations reflect selection that occurred before escaping detection by the host defense system. Penalizing indels aligns with the assumption that over short evolutionary timescales, most indels cause lethal frameshifts in coding-dense phage genomes. Additionally, phage targeting by CRISPR systems in natural environments likely operates far from excess or saturation conditions, requiring high sequence complementarity for functional base-pairing. Therefore, our recommendation to prioritize hamming distance applies specifically to the question "was this spacer acquired from this sequence and has it evolved under selective pressure?" rather than "could this guide cause off-target effects in gene editing?"

Several lines of evidence support the use of hamming distance over edit distance for CRISPR spacer-protospacer matching in the context of phage-host interactions. First, indels are approximately 4× rarer than substitutions in bacteria, and while quantitative measurements in phages are limited, this trend is expected to be similar or more pronounced in phage genomes. Second, most experimental phage-host studies report that escape mutations are predominantly single substitutions, particularly in the PAM-proximal "seed" region. While indels have been reported, quantitative comparisons across mutation types are lacking in the literature. Third, phage genomes are coding-dense, with most of the genome covered by coding sequences. A single indel in a coding sequence causes a frameshift mutation, which is often lethal, whereas substitutions may only affect a single amino acid residue. Frame-preserving indels (multiples of 3 bp) are extremely rare but when observed may be particularly strong indicators of selection.

Our empirical analysis using semi-synthetic data demonstrates that allowing indels (edit distance) leads to substantially higher non-planned match frequencies compared to hamming distance. At hamming distance ≤3, we observed ~0.36 non-planned matches per million spacer-bp × million contig-bp, while edit distance produces dramatically higher rates. We note that for most (but not all) cases, this increase in background matches may not justify the minimal additional biological information gained. Additionally, with sufficient sequencing depth, sequencing-induced indels should be rare in assembled contigs for Illumina-based datasets. For low-accuracy technologies (e.g., Oxford Nanopore R9 chemistry) or low-depth raw reads, some tolerance for indels may be necessary, though alignment quality cannot exceed underlying sequencing accuracy. While large genomic rearrangements exist (e.g., gene order changes affecting spacer targets at gene boundaries), these won't be captured by edit distance searches and are not well quantified in literature.

We acknowledge that most existing literature focuses on substitutions, potentially creating an "assumption bias" where indels are underexplored. Future experimental work could systematically compare escape mutation types.

**Distance Calculation:**

To ensure consistent distance calculation across tools, we realigned all reported matches using parasail [@Daily2016_parasail], specifically the `nw_stats_scan` (Needleman-Wunsch global alignment). For hamming distance calculations, we use gap opening and extension penalties of 10 with scoring matrix `nuc44`, which strongly penalizes gaps. For edit distance calculations (used only for comparative analysis), we use standard parameters allowing indels. This approach allows us to:

- Calculate hamming distances independently of tool-specific scoring schemes
- Verify reported matches and their orientations  
- Generate standardized alignment visualizations for manual comparison
- Compare hamming vs edit distance effects empirically

### Hamming vs Edit Distance Comparison Methodology {#sec-hamming-edit-comparison}

To systematically evaluate the impact of distance metric choice on match specificity, we conducted empirical comparisons using both fully synthetic and semi-synthetic datasets.

**Classification Framework:**

For the semi-synthetic dataset (~3.7M real iPHoP spacers searched against 400k synthetic contigs with IMG/VR4-matched characteristics), we classified all detected alignments into three categories:

1. **positive_in_plan:** Alignments matching planned insertion coordinates (±5bp tolerance, see §sec-coordinate-tolerance). These represent the intended ground truth matches.

2. **positive_not_in_plan:** Alignments passing distance threshold and parasail validation but occurring outside planned regions. These represent chance similarities at the specified distance - not false positives in the technical sense (they are valid alignments), but non-planned matches that increase background noise.

3. **invalid_alignment:** Alignments that fail parasail re-verification or exceed quality thresholds. These are true false positives resulting from tool-specific reporting artifacts ().

**Comparative Analysis:**

We quantified non-planned match rates (category 2) as a function of:
- **Hamming distance thresholds:** 0, ≤1, ≤2, ≤3, ≤5 substitutions
- **Edit distance thresholds:** 0, ≤1, ≤2, ≤3, ≤5 edits (substitutions + indels)

For each threshold, we calculated:
- Total non-planned matches (positive_not_in_plan count)
- Non-planned match frequency: Expected number of non-planned matches per million spacer-bp searched against million contig-bp
- Invalid alignment rate: invalid_alignment / (all_alignments) × 100 (note: this reflects tool reporting behavior rather than fundamental tool quality, as not all tools allow strict distance threshold control)

**Tool Selection for Edit Distance Analysis:**

Sassy was used as the primary tool for edit distance comparison because it provides perfect recall with arbitrary edit distance support. For hamming distance, we used Bowtie1 (perfect recall up to 3 substitutions) and indelfree.sh indexed mode (for hamming distance ≤5). Cross-validation with other exhaustive tools (indelfree.sh bruteforce mode, MUMmer4) confirmed result consistency.

**Key Findings:**

This methodology revealed that non-planned match rates increase dramatically with allowed distance, and even more so when allowing indels:
- Hamming distance ≤3: 54,388 non-planned matches (~0.36 per M spacer-bp × M contig-bp)
- Hamming distance ≤2: 2,217 non-planned matches
- Hamming distance ≤1: 47 non-planned matches
- Hamming distance = 0: 1 exact non-planned match
- Edit distance: Substantially higher non-planned match frequencies when indels are allowed

Complete results, including per-distance breakdowns and validation analyses, are provided in Supplementary Note 3 and the distance_metric_analysis notebook in the project repository.


### Performance definition and calculation {#sec-performance-calc}

We defined true and false positives/negatives differently for synthetic and real datasets. For synthetic datasets, sequences were generated following pre-planned insertion patterns with known coordinates, strands, and number of mismatches. While analyzing 7,638,511 pre-planned spacer occurrences, we discovered that some sequences could align at unplanned locations while still meeting our mismatch threshold (≤3). To ensure accuracy, we recalculated all alignments and filtered out sequences with extracted contig regions longer than 130bp (120bp being the maximum spacer length). After this validation, we identified 999,916 additional validated non-planned alignments (13 with 0 mismatches, 68,612 with 1 mismatch, 224,044 with 2 mismatches, and 707,247 with 3 mismatches), demonstrating that with short sequences, the number of non-planned matches increases substantially with the allowed mismatch threshold. Given their validated alignment scores, we included both pre-planned and these additional non-planned alignments in our positive set. For real data (IMG VR4), the positive set comprised all alignments from all tools that met the specified threshold criteria. In both cases, we calculated standard performance metric:

Recall = true positives / (true positives + false negatives)

where true positives are matches found by a tool that exist in the positive set, false positives are matches reported by a tool that do not exist in the positive set, and false negatives are matches in the positive set that were not found by the tool. We note that in this context, "true negative" (i.e., a false alignment a tool did not report) does not represent a meaningful metric.


### Benchmarking framework {#sec-benchmark}

### Computational Resource and Runtime Tracking {#sec-resource-tracking}

While the primary focus of this study was to evaluate the ability of each tool to accurately identify spacer-protospacer matches, computational resource usage may become a limiting factor as metagenomic databases continue growing rapidly. We therefore designed the benchmark to automatically log comprehensive usage metrics.

**Synthetic Data Benchmarking:** For the synthetic datasets, we used hyperfine [@Peter_hyperfine_2023] to track runtime and resource usage, with a maximum of 5 runs for each tool to assess consistency. Hyperfine captures wall clock time, CPU time, and peak memory usage for local execution.

**Real Data Cluster Benchmarking:** For the IMG/VR4 datasets, all analyses were performed on the NERSC computing cluster using SLURM (Simple Linux Utility for Resource Management) job scheduling. We captured resource usage via SLURM's built-in accounting system using the `sacct` command, accessed through a custom Python wrapper (pyseff.py) [@pyseff]. For each tool execution, we recorded:

- **Wall clock time**: Actual elapsed time from job start to completion
- **CPU time**: Total CPU time across all cores (wall time × cores × CPU efficiency)
- **Peak memory usage**: Maximum RSS (Resident Set Size) during execution
- **CPU utilization**: Percentage of allocated CPU time actually used
- **I/O operations**: Read/write operations and data volumes
- **Job state**: Completion status, exit codes, and any timeout/memory limit violations

All SLURM job logs were retained for reproducibility and are available in the Zenodo repository.

**Important Considerations:**

1. **Memory allocation vs usage**: Some tools (particularly BBMap suite products) opportunistically use all available memory rather than deterministically based on input size. We report peak memory usage rather than allocated memory.

2. **Multi-job tools**: Some tools (notably BLASTn-short) exceeded single-job resource limits and were run as multiple jobs on data subsets. For these tools, we report the sum of wall times and maximum peak memory across all jobs.

3. **Index construction**: For indexed tools, we include index construction time in the total runtime as this represents real computational cost, though for production use with repeated searches, index construction is a one-time cost.

4. **Scaling analysis**: We analyzed CPU time scaling as a function of search space size (spacer-bp × contig-bp) to identify tools with linear vs superlinear computational complexity (see Results, Computational Resource Requirements section).

See Supplementary Figure 1 for an overview of the benchmarking workflow and resource monitoring pipeline.


### Versioning and Reproducibility {#sec-reproducibility}

All tools were installed and managed using conda [@conda] (via the mamba [@mamba] package manager) in isolated environments. Each tool was installed in a separate environment to prevent dependency conflicts and ensure reproducibility. Environment activation time was excluded from performance measurements to focus on actual tool runtime.

The exact versions and configurations of all tools were recorded in environment files, allowing for exact replication of our testing environment. All benchmarks were performed on identical hardware configurations to ensure fair comparison.


### Hardware specification  {#sec-specification}
All SLURM or hyperfine based analysis were ran on:
{
    "os": "Linux",
    "kernel": "4.18.0-553.58.1.el8_10.x86_64",
    "architecture": "x86_64",
    "cpu": "AMD EPYC 7543 32-Core Processor X 2",
    "cpu_cores": "64",
    "cpu_threads": "64",
    "cpu_frequency": "3705.616 MHz",
    "cpu_cache_l3": "32768K",
    "ram": "512.0G",
    "ram_total_bytes": "549755813888",
    "filesystem": "nfs",
    "disk_model": "SAMSUNG MZ1LB1T9HALS-00007",
    "python_version": "3.10.19",
    "hyperfine_version": "1.19.0"
}

### Extensibility

The framework is designed to be expandable through the integration of new tools. Each tool/software configuration is saved as a separate JSON file, which includes the exact commands and conda/mamba environment it uses. This configuration files can use placeholder variables which the main benchmarking script replaces with user choices during execution (such as {threads}, {contigs_file}, {spacers_file}, {output_dir}, and {results_dir}). A new JSON file can be added manually or via bench.utils.tool_commands:add_tool function in a semi automated method.


## Results

### Performance as a function of mismatch threshold {#sec-mismatch-performance}


![Performance of each tool as a function of mismatch threshold. The horizontal axis shows the number of allowed mismatches, while the vertical axis represents the mean detection fraction (0-1) aggregated across all spacer-contig pairs at a given mismatch threshold. Each color and shape indicates a different tool plot (shapes connected by lines for interpolation) Panel B shows the performance of the tools on the IMG/VR4 dataset, while panel A shows the performance of the tools on the synthetic dataset.](figures/main/tool_performance_by_mismatches.svg){#fig-tool-performance}

First we investigated potential tradeoffs and effects of the total edit distance (henceforth, interchangeable with mismatches) on the observed recall metric of the tools. Generally, the detection rate of each tool decreases as mismatch thresholds increase. Additionally, no single tool was able to identify all spacer occurrences, although at 0 mismatches the recall of bowtie1, bowtie2, blastn and mummer4 is approximately 0.99 (See supplementary table 2 for the recall values for each tool at different mismatch thresholds). At increased allowed mismatches, the tools showed more divergence, yet bowtie1 remained the single tool with the most unique matches by a considerable margin (@fig-tool-performance). Overall, the performance of the tools is similar between the synthetic and real datasets, albeit the overall lower sample size of the synthetic data should be considered when interpreting the results (see [table 1](#tbl-tools)).


### Performance as a function of query (spacer) abundance in reference database {#sec-abundance-performance}

![Comparison of recall (detection rate) across different mismatch thresholds and target abundance levels for IMG/VR v4 virus and spacer dataset. Top panel displays the subset of results with up to 1 mismatch, and the bottom panel displays the results with up to 3 mismatches. The horizontal axis shows the number of target occurrences on a logarithmic scale from 1 to 10^4^, while the vertical axis represents the mean detection fraction (0-1). Each color and shape indicates a different tool plot (shapes connected by lines for interpolation). The low-abundance region (1 - 1000 occurrences) is binned into logarithmically-spaced bins, while the high-abundance region (>1000 occurrences) is divided into only 3 additional bins, as such ultra-high abundance sequences are rare. The detection fraction is the mean detection fraction across all spacer-contig pairs at a given mismatch threshold and target abundance level.](figures/main/recall_vs_occurrences_combined.svg){#fig-recall}

We then investigated if there are any potential effects for the number of times each protospacer sequence appears in the target set (i.e. the virus sequence set). For perfect matches (0 mismatches), bowtie1 demonstrates exceptional performance with recall rates consistently above 0.99 across all occurrence frequencies (Figure 2). Mummer4, bowtie2 and blastn all maintain a detection rate close to bowtie1. For low-occurrence spacers (1-10 occurrences), strobealign achieves detection rates of 95.44% but shows a systematic decline to approximately 20% for spacers occurring >100 times, and further drops below 5% in the high occurrence range (>1000). 

When allowing one mismatch, the overall detection capabilities decrease across all tools, although Bowtie1 maintains its high performance. At up to three mismatches, the overall recall rates for all other tools further decrease, while Bowtie1 maintains detection rates above 97% throughout the occurrence spectrum.

The data shows a consistent pattern where detection rates generally decline for spacers with very high occurrence frequencies (>1000), though this effect becomes less pronounced as more mismatches are permitted. Quantitatively, this decline is most evident in tools like strobealign and bbmap-skimmer, while bowtie1 maintains its high performance even with highly repetitive sequences. Detailed statistics and recall curves for exact mismatch values (rather than at a maximal value) can be found in the supplementary.


### Overall number of identified spacer-contig {#sec-pairwise}

![Tool vs Tool (pairwise) comparisons - set intersections and differences matrixes. The value of a cell(i,j) is number of spacer-contig pairs identified by the tool listed in row i, which were not identified by the tool listed in the j column. Panel A shows the results for the synthetic dataset, while panel B shows the results for the IMG/VR4 dataset. ](figures/main/tool_comaprison_matrix.svg){#fig-pairwise}  

The pairwise comparison of the tool results suggests (@fig-pairwise), reinforces the observation regarding bowtie1's unique ability to recover a maximum number of spacer matches. Generally, it appears that, when compared to any single other tool, the total number of contig-spacer pairs bowtie1 misses is relatively smaller than the number of pairs the compared tool identified which were not identified by bowtie1.


### Computational Resource Requirements and Scalability {#sec-resource-usage}

Computational resource requirements vary dramatically across tools, reflecting fundamental differences in algorithmic approaches and trade-offs between sensitivity and efficiency (@fig-resource-usage). Understanding these resource requirements is critical for practical tool selection, particularly as CRISPR spacer and viral databases continue to grow rapidly.

![Computational resource usage for selected tools on the synthetic dataset. Panel A shows peak memory usage (GB) vs wall clock time (minutes) on log-log scale, with point size indicating total CPU time. Panel B shows CPU time scaling with search space size (spacer-bp × contig-bp), demonstrating different computational complexity classes. Exhaustive tools (Sassy, indelfree bruteforce) are shown in red hues, while heuristic tools are in blue hues.](figures/main/resource_usage.svg){#fig-resource-usage}

**Exhaustive vs Heuristic Tool Performance:**

Unsuprisingly, we observed massive computational gap between exhaustive and heuristic approaches:

- **Sassy (exhaustive, edit distance ≤5):** For the 5% IMG/VR4 HQ subsample (21,071 contigs, 715 Mbp), sassy required ~1M CPU seconds. Extrapolating to the full HQ benchmark dataset (421k contigs, 18.9 Gbp) would require >20M CPU seconds (~230 CPU-years) - computationally prohibitive for routine analysis.

- **Indelfree.sh bruteforce mode (exhaustive, hamming distance):** while not as optimised or efficient as sassy, was only able to  requiring ~100k CPU seconds for small subsamples. The indexed mode provides substantially better performance while maintaining hamming distance constraints.

- **Heuristic tools (Bowtie1, Bowtie2, MMseqs2, BLASTn):** Orders of magnitude faster than exhaustive approaches. Bowtie1 completed the full IMG/VR4 HQ benchmark dataset (3.8M spacers × 421k contigs, 18.9 Gbp) in approximately 12 hours wall time with 16 cores, demonstrating practical scalability to large metagenomic datasets.

**Memory Requirements:**

Peak memory usage ranges from <10GB (Bowtie1, StrobeAlign) to >200GB (MMseqs2, Sassy on larger datasets). Some tools (particularly BBMap suite products like indelfree.sh) use available memory opportunistically rather than deterministically, making resource allocation challenging in shared computing environments.

**Scaling Behavior:**

Different tools exhibit different computational complexity patterns (@fig-resource-usage Panel B):

- **Linear scaling:** Tools like Bowtie1 show near-linear scaling with search space size, making them suitable for ever-growing databases
- **Superlinear scaling:** Some tools show worse-than-linear scaling, which may become prohibitive as databases continue doubling in size every few years
- **Index construction overhead:** For indexed tools, the one-time cost of index construction should be considered separately from per-query search time

**Practical Implications:**

For large-scale metagenomic analyses involving millions of spacers and billions of bases of viral sequence:

1. **Heuristic tools are essential:** Exhaustive approaches, while providing perfect recall and valuable for validation on small datasets, are computationally prohibitive for routine large-scale analysis.

2. **Bowtie1 offers optimal balance:** Combining high recall (>95% for ≤3 mismatches), low non-planned match rates at hamming distance ≤3, and excellent computational efficiency, making it the practical choice for most applications.

3. **Exhaustive tools for validation:** Tools like Sassy and indelfree.sh bruteforce mode are valuable for establishing baseline performance on small subsamples and validating heuristic tool results, but not for production workflows.

4. **Storage considerations:** With 4.7TB output for just the 5% subsample (21k contigs, 715 Mbp) at edit distance ≤5, storage becomes a limiting factor for exhaustive approaches even when CPU time is available.

These computational considerations reinforce our recommendation of Bowtie1 for most applications, with exhaustive tools reserved for specialized validation studies or small-scale analyses where their computational costs are acceptable.


## Discussion

### Tool Performance and Recommendations

Our analysis, combining synthetic datasets with known ground truth and real-world metagenomic data, reveals critical insights for CRISPR spacer-protospacer matching tool selection.

**Primary Finding - Hamming Distance vs Edit Distance Analysis:**

Our systematic comparison of hamming distance (substitutions only) versus edit distance (allowing indels) reveals profound implications for match specificity and biological relevance. Using both synthetic datasets with complete ground truth and semi-synthetic datasets combining real spacers with synthetic contigs, we quantified how distance metric choice affects non-planned match rates.

**Key Findings from Distance Metric Comparison:**

Using the semi-synthetic dataset (~3.7M real spacers vs 400k synthetic contigs), we observed:

- **Hamming distance ≤3:** 54,388 validated non-planned matches (~0.36 per million spacer-bp × million contig-bp)
- **Hamming distance ≤2:** 2,217 validated non-planned matches 
- **Hamming distance ≤1:** 47 validated non-planned matches
- **Hamming distance = 0:** 1 exact match

When allowing indels (edit distance), non-planned match rates increase substantially, though the exact magnitude depends on allowed gap parameters. Using Sassy (the only tool with perfect recall supporting arbitrary edit distances) on the fully synthetic dataset, we confirmed that edit distance >3 leads to dramatically higher rates of non-planned matches compared to hamming distance ≤3.

See Supplementary Note 3 and the distance_metric_analysis notebook for detailed methodology and complete results of this comparison.

**Biological Justification for Preferring Hamming Distance:**

Our preference for hamming distance is supported by multiple lines of evidence: (1) indels are ~4× rarer than substitutions in bacteria, (2) most experimental escape mutations are single substitutions, particularly in PAM-proximal regions, (3) phage genomes are coding-dense making frameshift-inducing indels often lethal, and (4) with sufficient sequencing depth, sequencing-induced indels should be rare in assembled contigs from Illumina data.

**Tool Selection Flowchart and Recommendations:**

Based on our comprehensive benchmarking, we provide the following evidence-based recommendations organized by use case. @fig-flowchart provides a decision flowchart to guide tool selection based on dataset characteristics and analysis goals.

```{mermaid}
%%| label: fig-flowchart
%%| fig-cap: "Tool selection decision flowchart for CRISPR spacer-protospacer matching. Decision nodes (diamonds) represent classification criteria based on dataset scale, computational resources, distance metric requirements, and biological constraints. Edge labels specify the conditions and reasoning for each path. Terminal nodes (rectangles) indicate recommended tools with performance characteristics and application domains. Bowtie1 (green) represents the primary recommendation for large-scale analyses with hamming distance ≤3. Indelfree.sh indexed (blue) provides extended hamming distance capability (>3 substitutions, up to ≤5). Sassy (orange) enables exhaustive edit distance search for small datasets or low-accuracy long-read data."

flowchart TD
    Start([Tool Selection for<br/>Spacer-Protospacer Matching]) --> Q1{Dataset Scale<br/>and Computational<br/>Resources}
    
    Q1 -->|Small experimental dataset<br/><1M spacers, <10 Gbp contigs<br/>Computational cost acceptable| Q2{Distance Metric<br/>Requirements}
    Q1 -->|Large-scale metagenomic<br/>>1M spacers or >10 Gbp contigs<br/>Efficiency critical| Q3{Distance Threshold<br/>and Biological<br/>Constraints}
    
    Q2 -->|Edit distance needed:<br/>Indel tolerance required for<br/>low-accuracy long reads: ONT R9, PacBio CLR<br/>or mutation type characterization| Sassy[<b>Sassy - Exhaustive Edit Distance</b><br/>✓ Perfect recall, arbitrary thresholds<br/>✓ Supports indels and substitutions<br/>⚠ ~1M CPU-seconds per 5% subsample (21k contigs)<br/>Applications: Experimental phage-host studies,<br/>low-accuracy long-read assemblies,<br/>off-target analysis, methodological validation]
    Q2 -->|Hamming distance sufficient:<br/>Substitutions only<br/>High-quality assembled data: Illumina<br/>Historical infection inference| Q3
    
    Q3 -->|≤3 substitutions:<br/>Biologically relevant threshold<br/>Indels ~4× rarer than substitutions<br/>Most escape mutations ≤3 nt| Bowtie1[<b>Bowtie1 - Hamming ≤3</b><br/>✓ >99% recall at intended threshold<br/>✓ Scales to millions of spacers<br/>✓ Low non-planned match rate<br/>✓ Ungapped alignment only<br/><b>PRIMARY RECOMMENDATION</b><br/>Applications: Large-scale host prediction,<br/>metagenomic spacer-protospacer matching,<br/>high-throughput CRISPR target identification]
    Q3 -->|>3 substitutions:<br/>Extended divergence detection<br/>Accepts higher non-planned matches<br/>Conserved indel-free constraint| Indelfree[<b>Indelfree.sh Indexed - Hamming ≤5</b><br/>✓ Near-perfect recall for hamming ≤5<br/>✓ Maintains substitution-only constraint<br/>⚠ 10-100× slower than Bowtie1<br/>⚠ Higher non-planned match frequency<br/>Applications: Divergent phage detection,<br/>extended mismatch tolerance,<br/>sensitivity-prioritized analyses]
    
    style Bowtie1 fill:#90EE90,stroke:#228B22,stroke-width:3px
    style Sassy fill:#FFE4B5,stroke:#FF8C00,stroke-width:2px
    style Indelfree fill:#ADD8E6,stroke:#4169E1,stroke-width:2px
    style Start fill:#F0F0F0,stroke:#333,stroke-width:2px
```

**Detailed Recommendations by Use Case:**

1. **Primary recommendation for most applications - Bowtie1 (hamming ≤3):**
   - **Use for:** Large-scale metagenomic analyses, routine host-virus prediction, high-throughput spacer-protospacer matching
   - **Performance:** >95% recall for 0-3 mismatch spacers, <1% false positive rate
   - **Advantages:** Excellent computational efficiency, scales well to millions of spacers and billions of bases, maintains high performance even for high-abundance targets
   - **Limitations:** Maximum 3 substitutions, does not support indels
   - **Biological justification:** Most experimental escape mutations are ≤3 substitutions; higher thresholds increase false positives without substantial biological gain

2. **Extended hamming distance - Indelfree.sh indexed mode (hamming ≤5):**
   - **Use for:** Scenarios requiring detection up to 5 substitutions while maintaining hamming distance
   - **Performance:** Good recall for 4-5 mismatch spacers without the false positive explosion of edit distance
   - **Advantages:** Extends beyond bowtie1's 3-mismatch limitation while avoiding indel-associated false positives
   - **Limitations:** More computationally intensive than bowtie1, still limited to substitutions only
   - **When to use:** When analyzing divergent sequences or when increased sensitivity is needed beyond 3 mismatches

3. **Sassy (edit distance) - Specific applications only:**
   - **Use for:** 
     - Small datasets where computational cost is acceptable
     - Experimental setups where mutation type (substitution vs indel) is of research interest
     - Comparative studies of escape mutation types in controlled systems
     - Methodological validation and establishing baseline performance
     - Low-accuracy sequencing (e.g., Oxford Nanopore R9) where indel tolerance may be necessary
   - **Performance:** Perfect recall (100%), supports arbitrary edit distances
   - **Limitations:** Massive computational requirements (~1M CPU seconds for 5% subsample), enormous output size (4.7TB for 5% subsample with ≤5 edits), prohibitive for large-scale analysis
   - **Critical note:** We recommend sassy only for specialized applications where its unique capabilities (perfect recall + arbitrary edit distance) are essential and computational resources are available

4. **BLASTn-short - Parameter-dependent performance:**
   - **Use for:** Legacy compatibility, when familiarity with BLAST ecosystem is important
   - **Critical considerations:** Performance heavily dependent on `-max_target_seqs` parameter; default value significantly impacts high-abundance spacer detection
   - **Our analysis:** Used most sensitive parameters (lowest word size, high e-value, `-max_target_seqs 100000`)
   - **Advantages:** Familiar to many researchers, well-documented
   - **Limitations:** Lower recall than bowtie1, especially for high-abundance targets; parameter sensitivity requires careful configuration

**Algorithmic Insights:**

A key finding is that tools differ not because they are "bad" but because their algorithms solve different computational problems. Edit/affine-based algorithms (bowtie2, minimap2, BLAST, sassy) naturally report more matches than hamming-based ones (bowtie1, indelfree.sh) when using the same numeric threshold - this reflects fundamental algorithmic differences rather than tool quality. For CRISPR spacer-protospacer matching, where biological evidence strongly supports substitution-dominant mutation patterns, hamming-based approaches are more appropriate.

**Abundance Effects:**

Our findings reveal that within the mismatch thresholds tested (≤3), no single tool identified all spacer occurrences. The main performance differentiator is how tools handle high-abundance spacers (>1000 occurrences). Bowtie1 maintains >97% recall even for highly repetitive sequences, while other tools show systematic decline (e.g., strobealign drops to <5% for >1000 occurrences). This abundance sensitivity likely reflects tool-specific heuristics designed for different use cases (e.g., read mapping assumes reference sequences are not highly redundant).

**Practical Implications:**

Of specific concern is the relatively high number of alignments missed by blastn-short at default parameters, which is currently common in published analyses. Missing genuine spacer-protospacer pairs can significantly impact downstream conclusions about MGE host range, virus-host networks, and CRISPR system evolution. Our analysis suggests that many published studies using blastn-short may have substantially underestimated the number of spacer-protospacer matches, particularly for high-abundance targets.

**Context-Dependent Considerations:**

Experimental and analytical context must be considered when selecting tools:

- **Large-scale meta-analyses:** Favor bowtie1 for high recall and computational efficiency. In these studies where spacers and targets may not co-occur (from different samples/environments), high recall is critical while acknowledging that similarity implies ancestral encounters rather than current infectivity.

- **Experimental isolate studies:** When studying known phage-host pairs from isolates or temporally resolved samples, tools allowing higher mismatch tolerance may be appropriate, though biological justification for edit distance remains weak.

- **CRISPR array context:** Spacers from complete arrays provide additional information (order, genomic location, host genome origin). Recent studies (Mitrofanov et al., Vink et al.) reveal system-specific spacer loss patterns and mismatch tolerance, which may inform post-search verification.

- **Low-complexity filtering:** Regardless of tool choice, applying DUST masking or similar complexity filtering (e.g., ldust from minimap2, BBDuk from bbmap) prior to searching is prudent to reduce spurious matches from repetitive regions.

Another consideration should be the source of the spacer data: spacers sequences extracted from raw NGS data and spacers extracted from assembled CRISPR arrays (either from assembled or long read sequencing). Specifically, spacers from complete arrays present additional information, namely the location and order of the spacers within the array, and the observation they originate from the same host genome. Notably, a recent in-depth study by Mitrofanov et al @Mitrofanov2025, investigating the mutational landscape of repeats across many isolate prokaryote genomes, have identified patterns of spacer loss based on system sub/type and location. A similar meta-analysis of spacer mutations by Vink et al @Vink2021, have revealed that different CRISPR subtypes exhibit varying tolerance for mismatches within the spacer sequences, with most matched spacers containing three or fewer mismatched nucleotides. This aligns with our current general recommendation of using Bowtie1. Additionally, Vink et al observed that Type I-E and Type II systems preferentially target template strands while Type I-A, I-B, and Type III systems prefer coding strands, emphasizing system-specific characteristics which may also inform post-search verification methods (albeit this may require additional information, such as the sequences orientation or coding potential, and the CRISPR subtype of the spacer).

### Biological Interpretation and Potential False Positives

While our technical comparison focuses on tool performance, the biological interpretation of identified matches also requires careful consideration. The arms race between prokaryotes and MGEs creates a complex landscape where simple sequence matching may not directly translate to genuine host-parasite relationships. We identified several scenarios that could lead to false positive assignments:

1. **Low Complexity Sequence Matches**: Independently of the tool choice, low-complexity (regions with highly skewed GC content, or composed of many repeated sequences) can be susceptible to spurious matches. Low complexity regions may be present in both the virus target set, or in the spacer set, where some non-CRISPR repeated sequences may have been misclassified as such. While certain tools employ filters and heuristics to mitigate the effect of low complexity regions, a prudent procedure should include a step separated from the search, to specifically identify, filter or mask the spacer and virus sets. Dustmasker [@Morgulis_2006], or a similar tool (e.g. ldust from minimap, or BBDuk from bbmap) could be used prior to the search.

2. **Common Sequence Motifs**: Some matches may correspond to highly conserved sequences shared across various biological systems. For instance, horizontal gene transfer events can lead to the spread of similar sequences across diverse MGEs, potentially creating spurious matches that don't reflect direct host-MGE interactions. An example of potential HGT mediated matches was described by Kosmopoulos et al. 2023, where a transposon-mediated transferred of a phage lysin gene (to the host genome) created a true sequence similarity (which was verified by the authors using a combination of sequencing technologies) [@Kosmopoulos_2023]. Even if anecdotally observed, this suggests that an unknown number of observed "good" alignments may be due to HGT, which in the absence of additional information, could not be ruled out as a false positive.

3. **Self-Targeting Events**: Some matches may represent CRISPR targeting of host genes [@Wimmer_2020]. Previous studies estimated a varying amount of these actually target sequences with putative exogenous origin such as prophages, ranging from ~50% [@Stern_2010], to ~80% [@Shmakov_2017]. In Shmakov et al., the authors estimate non-defence targeting is likely a rare event. So far, most observations of non-defence (or counter defence) molecular functions of CRISPRs did not directly involve the spacer sequences, but rather the Cas genes or related effectors. Some observed functions include genome remodelling and evolution, or temporal regulation of gene expression. For example, in *Francisella novicida*, Sampson et al. 2013 demonstrated that certain lipoprotein production is mediated by a CRISPR system [@Sampson_2013].

4. **non-chromosomal replicon encoded CRISPRs**: Similarly to the potential of CRISPR systems to act in non-immune functions, in certain scenarios spacers may be acquired from non-MGE replicons, or be carried (even if partially) by mobile elements. While some types are known to be chromosomal, others are known to be carried entirely by plasmids (i.e. both the Cas proteins and the array loci are on the plasmid), for example in various halophilic archaea [@Maier_2018]. A recent study by Zhang et al [@Zhang_2025] observed similar phenomena in the human gut microbiome, specifically in _Bifidobacterium longum_. Another non-MGE targeting phenomena in archaea was described by Turgeman-Grott et al. [@Turgeman_Grott_2018], where inter-species spacers (targeting genes from related species) were demonstrated to be common in archaea, at least in the context of cellular mating. Another confounding factor is the potential of certain MGEs to target host genes, potentially for regulatory functions, or as counter-defense mechanisms[@Shmakov_2023]. Shmakov et al. 2023 have identified widespread CRISPR-derived phage-encoded mini-arrays, which can hijack and interfere with their host native system.


### Study Limitations

**Synthetic Dataset Characteristics:**

The synthetic data was generated with sequence characteristics matched to real biological sequences, including GC content matched to iPHoP spacer data (~49%), simulated contig GC% and length distributions based on IMG/VR4 characteristics (~46% GC), and verification that k-mer distributions and sequence complexity match real data (see supplementary notebook: spacer_inspection.ipynb). However, real biological sequences have additional complexities not fully captured, including locus-specific composition biases, regional variation in nucleotide frequencies, and evolutionary constraints that shape sequence structure. These differences should be considered when interpreting results, though our enhanced synthetic dataset provides substantially improved realism compared to purely random sequences.

**Distance Metric Focus:**

Our analysis primarily focuses on hamming distance (substitutions only) which we argue aligns with the underlying biological question. While we demonstrate that edit distance dramatically increases false positives (>10% for edit >3 vs <1% for hamming ≤3), we acknowledge that rare cases exist where frame-preserving indels (multiples of 3 bp) could indicate genuine interactions under strong selection. We used sassy to comprehensively test edit distances up to 5, confirming our recommendation against routine use of edit distance for most applications due to prohibitive computational costs and false positive rates. For specialized applications requiring indel detection (e.g., low-accuracy long-read sequencing, experimental mutation-type studies), sassy provides perfect recall but at massive computational cost (~1M CPU seconds for 5% subsample at 21k contigs, 4.7TB output with ≤5 edits). We note that while long-read technologies are attractive for CRISPR arrays (which are repetitive and prone to short-read misassembly), alignment quality cannot exceed underlying sequencing accuracy.

**Sequencing Technology Considerations:**

Our recommendations are primarily based on Illumina-derived assembled sequences where sequencing-induced indels are rare with sufficient depth. For other technologies (Oxford Nanopore R9, PacBio CLR) or low-depth datasets using raw reads, different considerations may apply. However, we argue that even in these cases, biological indels remain rare compared to substitutions, and the false positive explosion with edit distance must be carefully weighed against potential benefits.

**Parameter Space and Tool Versions:**

Tools were not exhaustively tested across all possible parameter combinations, and further optimization may be possible for specific use cases. Several tools presented version-specific challenges: MMseqs2's latest release version had frequent crashes requiring use of the latest GitHub commit, LexicMap had no formal v1 release at time of testing (though newer versions are now available), and various index construction parameters (offset rate, seed selection) may affect performance in ways not fully explored. We focused on parameters maximizing sensitivity and avoiding artificial limitations on multiple match detection, representing realistic use cases rather than exhaustive parameter sweeps.

**Computational Resource Constraints:**

Exhaustive tools (Sassy, indelfree.sh bruteforce mode) were not run on the full HQ benchmark dataset due to prohibitive computational costs. Our extrapolations are based on smaller subsamples (ranging from 0.0005× to 0.1×, corresponding to 279 to 42,143 contigs), and actual performance on the full dataset might differ from these estimates. However, the consistent scaling patterns observed across subsample sizes suggest that our computational cost projections are reasonable approximations.

**Sample Size and Scale:**

The synthetic dataset is smaller than the real dataset, though this enabled controlled testing of rare scenarios like ultra-high spacer occurrence rates. We employed stratified subsampling of real data (IMG/VR4 HQ contigs at 0.0005, 0.001, 0.005, 0.01, 0.05, 0.1, and 1.0 fractions) to assess tool behavior across different database scales, finding consistent performance patterns suggesting our recommendations are robust across database sizes ranging from 279 contigs (7 Mbp) to 421,431 contigs (18.9 Gbp).

### Future Directions

Several research directions emerge from this work that could refine tool selection and improve spacer-protospacer matching accuracy. First, systematic experimental validation comparing substitution versus indel frequencies in phage escape mutations across diverse CRISPR-Cas systems would provide direct evidence for or against our hamming distance preference. While existing studies predominantly report substitutions [@Deveau2008; @Semenova2011; @Fineran2014], comprehensive quantitative comparisons across mutation types remain lacking.

Second, investigation of frame-preserving indels (3bp multiples) as indicators of strong selection in coding regions could reveal rare but biologically significant patterns. Such indels would maintain reading frame integrity while potentially disrupting CRISPR targeting, representing adaptations under particularly strong selective pressure.

Third, incorporation of CRISPR subtype information could refine matching strategies. Recent work by Mitrofanov et al. [@Mitrofanov2025] on spacer loss patterns and Vink et al. [@Vink2021] on mismatch tolerance differences across CRISPR subtypes suggests that system-specific characteristics could inform both search parameters and post-search verification. Type I-E and Type II systems preferentially target template strands while Type I-A, I-B, and Type III systems prefer coding strands, information that could enhance match validation when combined with strand orientation and coding potential analysis.

Fourth, development of purpose-built tools for spacer-protospacer matching that combine bowtie1-level computational efficiency with extended hamming distance support (≥4 substitutions) would fill a practical gap. Current options either sacrifice sensitivity (bowtie1 maximum 3 mismatches) or computational efficiency (indelfree.sh indexed mode).

Finally, integrated workflows incorporating complexity filtering, post-search verification using tools like parasail [@Daily2016_parasail], phylogenetic context analysis, and genomic location assessment would provide more robust inference frameworks. Such workflows could systematically address confounding factors including low-complexity sequences, horizontal gene transfer events [@Kosmopoulos_2023], self-targeting [@Wimmer_2020; @Stern_2010; @Shmakov_2017], non-defense functions [@Sampson_2013], plasmid-encoded CRISPR systems [@Maier_2018; @Zhang_2025], inter-species targeting [@Turgeman_Grott_2018], and phage-encoded mini-arrays [@Shmakov_2023].

## Conclusion

Our comprehensive comparison of spacer-protospacer search tools, combining synthetic datasets with known ground truth and real-world metagenomic data, reveals critical insights for tool selection and establishes evidence-based recommendations.

**Key Findings:**

1. **Distance metric choice is critical:** Hamming distance (≤3 substitutions) maintains <1% false positive rate while edit distance (>3 edits) leads to >10% false positives. Biological evidence strongly supports hamming distance: indels are ~4× rarer than substitutions, most escape mutations are single substitutions, and phage coding-dense genomes make frameshift-inducing indels often lethal.

2. **Tool performance varies by algorithmic approach:** Tools differ not in quality but in the computational problems they solve. Edit/affine-based algorithms naturally report more matches than hamming-based ones - understanding these algorithmic differences is essential for appropriate tool selection.

3. **Abundance sensitivity distinguishes tools:** The primary performance differentiator is handling high-abundance spacers (>1000 occurrences). Bowtie1 maintains >97% recall for highly repetitive sequences, while other tools show systematic decline due to heuristics designed for different use cases.

4. **Current practices may miss many matches:** BLASTn-short at default parameters misses substantially more matches than bowtie1, particularly for high-abundance targets. Many published studies likely underestimate spacer-protospacer matches.

**Evidence-Based Recommendations:**

- **Most applications:** Bowtie1 with hamming ≤3 (high recall, low false positives, excellent computational efficiency)
- **Extended mismatches (4-5):** Indelfree.sh indexed mode (avoids edit distance false positive explosion)
- **Specialized only:** Sassy for small datasets when mutation type matters or perfect recall is essential (computationally prohibitive for large-scale analysis)

**Broader Implications:**

The interpretation of spacer-protospacer matches requires careful consideration of biological context beyond tool performance: low-complexity sequences, horizontal gene transfer, self-targeting, and mobile element-encoded CRISPRs can all complicate straightforward interpretation. Proper workflow should include complexity filtering, post-search verification of alignment quality, and contextual analysis of genomic location and phylogeny.

Our findings emphasize that tool selection should be guided by understanding algorithmic assumptions and their alignment with biological expectations. As CRISPR spacer and viral databases continue growing rapidly (from 366,799 spacers in 2017 to 3,835,942 in 2023), choosing appropriate tools becomes increasingly critical for accurate host-MGE interaction inference.

We provide not just tool performance metrics but a framework for understanding why tools differ and how to select appropriate methods for specific research contexts. This work enables more accurate inference of virus-host relationships and CRISPR system evolution across diverse microbial ecosystems.
 
## Code and data availability

All code generated for this study can be found in the git repository: [code.jgi.doe.gov/spacersdb/spacer_matching_bench](http://code.jgi.doe.gov/spacersdb/spacer_matching_bench). All raw outputs (tool results on real and synthetic datasets, the simulated sequence files, the SLURM logs, and the hyperfine runtime measurements) are available on Zenodo [@zenodo_doi].

## Acknowledgements
Work conducted by the U.S. DOE Joint Genome Institute (https://ror.org/04xm1d337) (SR, UN, APC and BB), a DOE Office of Science User Facility, is supported by the Office of Science of the U.S. DOE operated under Contract DE-AC02-05CH11231.  

We would like to thank the following people for their helpful feedback and suggestions:
Uri Gophna, Georg Rath, and Ragnar Groot Koerkamp for valuable discussions on distance metrics and tool performance.