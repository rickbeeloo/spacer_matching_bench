---
title: "Tool Choice Impacts CRISPR Spacer-Protospacer Detection"
author:
  - name: Uri Neri*^1^
    corresponding: true
  - name: Antonio Pedro Camargo^1^
  - name: Brian Bushnell^1^
  - name: Rick Beeloo^2^
  - name: Simon Roux^1^
format:
  pdf:
    documentclass: article
    geometry:
      - margin=1in
    fig-format: pdf
    embed-resources: true
    keep-tex: true
    fig-pos: 'H'
    number-sections: true
    cite-method: biblatex
    bibliography: references.bib
    pdf-engine: xelatex
  html:
    toc: true
    toc-depth: 3
    number-sections: true
  docx: default
execute:
  echo: false
  warning: false
---
1: DOE Joint Genome Institute, Berkeley, CA, USA  
2: Utrecht University, Padualaan 8, Utrecht, NL 3584 CH  
* Uri Neri (uneri@lbl.gov)


## Abstract {#sec-abstract}

CRISPR (Clustered Regularly Interspaced Short Palindromic Repeats) systems are a fundamental defense mechanism in prokaryotes, where short sequences called spacers are stored in the host genome to recognize and target exogenous genetic elements. Viromics, the study of viral communities in environmental samples, relies heavily on identifying these spacer-target interactions to understand host-virus relationships. However, the choice of sequence search tool to identify putative spacer targets is often overlooked, leading to an unknown impact of downstream inferences in virus-host analysis. Here, we utilize simulated and real datasets to compare popular sequence alignment and search tools, revealing critical differences in their ability to detect multiple matches and handle varying degrees of sequence identity between spacers and potential targets. Finally, we provide general guidelines that may inform future research regarding matching, which is a common practice in studying the complex nature of host-MGE interactions.


## Introduction  {#sec-introduction}

CRISPR (clustered regularly interspaced short palindromic repeats) systems play a vital role in prokaryotic defense against mobile genetic elements, including viruses, plasmids, and other autonomous genetic elements [@Mojica_2005; @CRISPR_review]. These systems are organized as arrays in the bacteria or archaea genome, where short sequences called spacers are interspersed between repeated sequences. The spacer sequences within these arrays guide the targeting of invasive genetic elements, allowing for specific defense against these threats [@CRISPR_classification]. The corresponding locus on the virus genome where the spacer complements is termed "protospacer". The analysis of spacer-protospacer pairs is essential in understanding the complex interactions between hosts and MGEs [@Edwards2015_phage_host].

The identification of genuine host-MGE interactions through spacer-protospacer matching presents unique challenges due to the dynamic nature of these relationships and the complexity of sequence evolution. While matches between spacers and protospacers are often interpreted as evidence of interaction, various biological and technical factors can complicate this interpretation [@Edwards2015_phage_host; @soto_perez_crispr_2019].

Several key scenarios can lead to false positive assignments in spacer-protospacer matching. Low complexity sequences can create spurious matches between simple repeat regions (albeit these can be mitigated through complexity filtering via Shannon entropy or DUST). Another type of potential false positives are highly conserved sequences shared by unrelated MGEs, potentially resulting from horizontal gene transfer between MGEs. The horizontal transfer of CRISPR arrays themselves on mobile elements further requires careful examination of array genomic context (regions outside the CRISPR loci) and phylogenetic analysis. Self-targeting events, where matches occur against the host genome rather than MGEs, necessitate comparison against host genome databases and analysis of targeting context. Finally, historical acquisition events may not reflect current interactions, requiring consideration of phylogenetic dating, evolution rates and the effects of the protospacers being under selective pressure to mutate (which may reduce the MGE susceptibility to deterioration by the CRISPR system). This is further complicated by the fact that increasing the allowed distance between sequences directly increases the likelihood of considering sequences similar or related to each other.

False negatives present another challenge in spacer-protospacer matching, particularly when dealing with large databases of potential targets. Many alignment and search tools default to reporting only the best matches or the first matches that pass a given threshold for a given query or HSP. This may result in potentially missing additional legitimate matches. Unfortunately, different tools also handle ambiguous or secondary alignments differently: they may be reported, reported up to a number or based on relative alignment quality, or omitted. Similarly, cases where a query sequence has multiple matches in different reference sequences are not handled uniformly across tools. This limitation becomes increasingly problematic as databases grow larger and more diverse, a single spacer might match (implying a targeting) multiple related MGEs.

Yet despite these variations, the choice of spacer-to-protospacer search or alignment tool is often not deeply considered. Presently, the common option for this task, popularized by Edwards et al and Biswas et al [@Edwards2015_phage_host; @Biswas2013], uses BLASTn [@Altschul1990_blast] with parameters adjusted for short input sequences. However for most bioinformatic tools, the exact workflow design and parameter choice can impact the outcome, including in sequence analysis. The importance of proper tool usage and parameter interpretation is highlighted by historical examples in bioinformatics. A striking example is the work of Shah et al, @Shah2018, in which they report how certain misunderstandings of BLAST's `-max_target_seqs` parameter may lead to incorrect assumptions about result completeness, potentially impacting published analyses. Albeit this was later clarified by Madden et al., [@Madden2018] (of the blast development team) as an unfortunate combination of a software bug (that were since patched) affecting rare cases, and misconceptions regarding the process BLAST+ uses for tie-breaking (alignments of equal plausibility), and finally a consideration regarding composition base scoring. Apart from the patched bug, the main outcome of this correspondence led to more explicit details in blast documentation (specifically the appendix "Outline of the BLAST process"). Still, this highlights that misconceptions about the expected exhaustiveness of tools' result-reporting can also lead to incorrect assumptions about the outcome of an analysis. In practice, most bioinformatic tools use various heuristics and optimizations, typically designed with specific use cases in mind. For example, most short-read mappers assume the reference to be the output of a singular assembly - which would imply the reference does not contain extremely redundant copies of the same loci, or a limited number of very similar sequences (e.g. strain variants, alternative splice variants), and this assumption impacts the way read mapping is computed and results are reported.

The choice of tool and its parameters can significantly impact the detection of these multiple matches, with some tools prioritizing speed over completeness by limiting the number of reported matches, or by other internal heuristics such as seed sequence selection from high occurring sequences being penalized. This trade-off between sensitivity and computational efficiency is especially important to consider as most available tools were designed for different tasks than spacer-protospacer matching (e.g. expression analysis, homology detection, and variant calling), and under different assumptions (such as reference and query sequence size and database size or the nature of the reference source: from a single isolate or metagenomic sample rather than from aggregation of sequences from different sources).

**Sequence Alignment Methods and Their Design Constraints:**

Modern sequence alignment/search tools were developed to address diverse computational challenges in genomics, such as protein domain function inference, taxonomic profiling, abundance calculation, quantitative/compartive expression analyses and so on.  
These different use caseare often optimized (or have optimized variants) for specific use cases with distinct assumptions and performance trade-offs.  
For example, tools designed for homology detection and  (e.g. PSI-BLAST, MMseqs2 profile) may prioritise (or be used when) high sensitivity even at low sequence identity levels.  
    specificity through multi-stage filtering and statistical scoring (e-values, bit scores), aiming to distinguish true homologs from spurious matches across evolutionary timescales. Read mappers for differential expression analysis (Bowtie2, STAR) assume single-source references and optimize for unique best-hit reporting, using mapping quality scores to reflect alignment confidence. Variant callers and genome assemblers require exhaustive local exploration but can tolerate higher computational costs for small, targeted regions. These tools employ various heuristics—seed-based indexing (FM-index in Bowtie, minimizers in Minimap2, strobemers in StrobeAlign), k-mer filtering (MMseqs2's double k-mer matches), chaining algorithms (collinear chaining in Minimap2), and early termination strategies—to achieve practical performance on large datasets. Common reporting metrics include alignment scores (reflecting match/mismatch penalties), e-values (expected number of matches by chance given database size), mapping quality (phred-scaled probability of correct placement), and distance thresholds (edit distance allowing indels, hamming distance for substitutions only, or exact matching). Critically, these design choices reflect the intended application: BLAST's e-value framework assumes searching for distant homologs across diverse sequences, while read mappers assume low divergence from a single reference assembly. Understanding these algorithmic assumptions is essential when repurposing tools for spacer-protospacer matching, where the biological constraints (short sequences, moderate divergence, potentially high target abundance) differ substantially from the original design specifications.

**Algorithmic Approaches and Distance Metrics:**
*** NOTE THIS NEEDS TO BE EXTENDED / MORE DETAILED (which heuristics, what algos, what intended uses...)

A fundamental consideration in tool selection is the underlying algorithmic approach and distance metric used. Tools can be broadly categorized as:

1. **Exhaustive vs Heuristic:** Exhaustive methods guarantee finding all matches within a specified distance threshold but are computationally intensive. Heuristic methods use various optimizations (seed-based indexing, chaining algorithms, k-mer filtering) to dramatically improve speed but may miss some matches. For large-scale analyses, heuristic methods are often necessary, but understanding their limitations is critical.

2. **Distance Metric Type:** Tools differ fundamentally in how they measure sequence similarity:
   - **Hamming distance** counts only substitutions (e.g., bowtie1, indelfree.sh). Requires query and subject string to be of same length (albeit some tools using hamming distance may support partial alignments made by clipping/truncating terminal bases)
   - **Edit/affine distance** allows insertions and deletions in addition to substitutions (e.g., bowtie2, minimap2, BLAST, sassy, mmseqs)
   - **Exact matching** requires perfect sequence identity (e.g., spacer-containment, grep, )

Importantly, when comparing tools using different distance metrics with the same numeric threshold (e.g., "≤3 mismatches"), edit/affine-based algorithms will naturally report more matches than hamming-based ones. This reflects different computational problems being solved rather than differences in tool quality. From a biological perspective, the choice of distance metric should reflect the expected mutation patterns in the system being studied. For CRISPR spacer-protospacer matching, several lines of evidence suggest hamming distance is more appropriate: indels are rarer than substitutions (~4× in bacteria), most experimental escape mutations are single substitutions, and phage genomes are coding-dense making frameshift-inducing indels often lethal. Furthermore, our empirical analysis demonstrates that allowing indels dramatically increases false positive rates (>10% for edit distance >3) compared to hamming distance (≤1% for hamming ≤3), without providing substantial additional biological insight for most applications.

Another potential consideration is the computational resources requirements. Memory, storage, and availability of CPU cores are factors differing between tools. Parameter choice may also impact these factors considerably, with certain tools offering tunable parameters to trade-off between sensitivity and computational efficiency. In recent years, spacer database size has been rapidly increasing - from 366,799 unique spacers in 2017 [@Shmakov_2017] to 1,173,006 unique spacers reported in 2021 [@Dion_2021] to 3,835,942 unique spacers in 2023 [@camargo_img_vr4_2023]. Similarly, public virus and MGE databases are growing rapidly, with large contributions from metagenomic samples resulting in routine fold increases in the number of predicted viral contigs [@camargo_img_vr4_2023]. Most tools require more resources as the size of the database grows, and as this trend continues, certain workflows and tools may become prohibitively expensive to run in a reasonable time frame.


## Methods {#sec-methods}

### Tool Selection {#sec-tool-selection}
*** TODO - THIS NEEDS FACT CHECKIN AND MORE DETAILS ***
We evaluated several widely-used sequence alignment and search tools, alongside an implementation of basic string containment. The tools were selected based on their availability and historical use, and were chosen to span a range of different algorithmic approaches. It is important to note that these tools were not specifically designed for spacer-protospacer matching, but rather for more general sequence search tasks (mmseqs2, blastn-short and lexicmap), or alignment/mapping of short reads to reference genomes (bowtie1, bowtie2, minimap2, nucmer, bbmap-skimmer, strobealign). Additionally, our focus here is specifically on spacer-protospacer matching, and so we did not evaluate general host-prediction tools (even those that may perform spacer-protospacer matching), like SpacePHARER [@Zhang_2021] or iPHoP [@Roux2023_iphop], which may perform additional analyses based on prior additional information, such as evaluating the LCA from multiple spacer-protospacer matches.


| Aligner | Indexing | Main Algorithm | Heuristic/Exhaustive | Reporting/Limiting Threshold Used in Benchmark| Year | Original Purpose | Notes |
|:--------|:---------:|:---------------|:---------------------|:-----------------------------|:----:|:-----------------|:------|
| [Bowtie1](https://github.com/BenLangmead/bowtie) | Yes | FM-Index (BWT) | Yes (backtracking) | Hamming distance | 2009 | Short read mapping | Optimized for 25-50 bp reads (max 1kbp); ungapped alignment only; backtracking heuristic limits to 3 mismatches |
| [Bowtie2](https://github.com/BenLangmead/bowtie2) | Yes | FM-Index (BWT) | Yes (multiseed + extend) | Affine/Edit distance | 2012 | Read mapping | Uses FM-index for seeding with SIMD-accelerated DP extension; supports gapped, local, and end-to-end alignment |
| [Minimap2](https://github.com/lh3/minimap2) | Optional | Minimizer + chaining | Yes (minimizer seeding) | Edit distance | 2018 | Long read mapping | Lexicographically smallest k-mer per window; collinear chaining with gap penalties; versatile across read types |
| [indelfree.sh](https://github.com/bbushnell/BBTools/) | No | Multi-kmer matching | Bruteforce mode is exhaustive, while ion "Indexed" mode this can be limited by selected kmer length, query length, and number of substitutions | Hamming distance | Publically introduced to bbtools September 2025 | Read mapping | BBTools suite is Java based, and will use available memory - so the peak memory reported herein (sourced from SLURM logs) does not equate with "minimal required memory" |
| [StrobeAlign](https://github.com/ksahlin/StrobeAlign) | Yes | Randstrobes | Yes (syncmer thinning) | Edit distance | 2022 | Read mapping | Uses hash-based linked strobes (randstrobes) with multi-context seeds (MCS) for hierarchical search |
| [BLAST+](https://blast.ncbi.nlm.nih.gov/doc/blast-help/downloadblastdata.html) | Optional | Hit-and-extend | Yes (contiguous word) | E-value, bit score | 2009 | Sequence search | BLASTN-short mode uses 11-mer seeds; reports matches based on e-value (expected hits by chance given DB size) |
| [MMseqs2](https://github.com/soedinglab/MMseqs2) | Optional | K-mer prefiltering | Yes (3-stage cascade) | E-value, bit score | 2017 | Sequence search | Double k-mer matching → vectorized ungapped → gapped SW; optimized for many-against-many searches |
| [Sassy](https://github.com/RagnarGrootKoerkamp/sassy) | No | Uses a bit-parallel algorithm based on Myers' bitpacking to perform exhaustive approximate string matching (ASM) | Exhaustive | Edit distance | 2024/5 | Versatile pattern matching, suggested for use in CRISPR (gene-editing) off target detection and raw-read alignments| Guarantees perfect recall; explores full edit distance landscape; supports arbitrary distances; high computational cost, requires SIMD instructions (AVX2 and NEON), which most modern CPUs support |
| [X-mapper](https://github.com/mathjeff/Mapper) | Yes | Gapped x-mer pyramid | Yes (dynamic seeds) | Edit distance | 2024 | Read mapping | **Preprint** - Dynamic-length gapped x-mers with "pyramid walking" to optimize seed specificity |
| [mummer4](https://github.com/mummer4/mummer) | Optional | 48-bit suffix array | Yes (MUM-based) | Edit distance | 2018 | Genome alignment | Identifies Maximal Unique Matches (MUMs) using enhanced suffix array; handles genomes up to 141 Tbp |
| [spacer-containment](https://github.com/apcamargo/spacer-containment) | No | String containment | Exhaustive (exact) | Exact match | - | String matching | Basic exact substring matching without mismatch or indel tolerance |
| [LexicMap](https://github.com/shenwei356/LexicMap) | Yes | LexicHash + WFA | Yes (hierarchical index) | Edit distance | 2024 | Gene/genome search | **Preprint** - Window-guaranteed seeding with 15-bp prefix/suffix matching; uses Wavefront Alignment algorithm |


: Evaluated Tools and Their Characteristics. **Indexing:** "Yes" indicates a precomputed index is required (in our benchmark, if an index has to be pre-created the time to construct it is measured along the search/alignment step); "Optional" means a persistent index can be generated for reuse but the tool can also build it on-the-fly for single runs; "No" means no indexing is required or supported. **Heuristic/Exhaustive:** Exhaustive methods guarantee finding all matches within the specified distance threshold; heuristic methods use optimizations (seed-based indexing, chaining, k-mer filtering) to improve speed but may miss some matches. **Reporting/Limiting Threshold:** The primary metric used to report or filter alignments. Hamming distance counts only substitutions; Edit distance allows insertions and deletions; Affine distance uses gap penalties; E-value represents expected matches by chance given database size; Exact match requires perfect identity. Note that edit/affine-based algorithms will naturally report more matches than hamming-based ones when both use the same numeric threshold—this reflects different computational problems being solved rather than tool quality differences. **Tool Configuration:** All tools were configured to maximize sensitivity and avoid artificial limitations on multiple match detection. Exact commands, parameters, and tool versions are provided in Supplementary Table S1. {#tbl-tools}


### Data Generation and acquisition
*** TODO: Update numbers based on the notebooks and the filtered iphop + img/vr sets (and semi-synthetic sets).
| Dataset | Fully Synthetic - random sequences | Semi-Synthetic - real spacers, synthetic contigs | IMG/VR4 (v1.1) - high confidence viral contigs |
|:--------|:-----------------------------|:------------------------------------------------|:----------------------------------------------|
| Spacer Sequence Count | 9,020 | 3,835,942 | 3,835,942 |
| Total Spacer size (bp) | 618,663 | 132,094,902 | 132,094,902 |
| Spacer Length Range | 18 - 120 | 25 - 100 | 25 - 100 |
| Spacer GC% | 50 | 46.73 | 46.73 |
| Spacer Median length | 68.6 | 34.4 | 34.4 |
| Target Length Range | 1,501 - 200,000 | 1,501 - 200,000 | 165 - 2,473,870 |
| Target GC% | 50 | 46 | 44.45 |
| Target Median Length | 100,831.50 | 100,831.50 | 7664 |
| Target Sequence Count | 320,000 | 400,000 | 5,465,076 |
| Target size (bp) | 32,266,068,302 | 40,332,585,378 | 79,049,118,084 |

: Dataset characteristics. Target is interchangeable with "contig". The semi-synthetic dataset combines real spacers from iPHoP with synthetic contigs that match IMG/VR4 contig characteristics, enabling estimation of false positive rates in a more realistic setting. {#tbl-datasets}


### Synthetic dataset

To examine each tool's performance across diverse spacer-to-target matching scenarios (i.e., different numbers of matches, varying sequence similarity, etc.), we developed a Python-based simulation framework with fine-grained control over sequence characteristics. The simulator allows specification of target contig length distributions (uniform or normal), GC content variation, spacer length, number of planned occurrences in targets, range of mismatches (substitutions), and reverse complement frequency. Critically, this approach records the ground truth of all planned insertions, enabling differentiation between true positives (planned matches) and false positives (spurious matches in unplanned regions).

**Enhanced Realism:** To better approximate real-world datasets, we configured the synthetic data generation to match key characteristics of the IMG/VR4 dataset:

- **GC content:** Matched to iPHoP spacer data (~49% for spacers, ~46% for contigs)
- **Length distributions:** Contig lengths follow realistic distributions based on IMG/VR4 viral contig characteristics (range: 1,501-200,000 bp, median: ~100,831 bp)  
- **Sequence complexity:** We verified that k-mer distributions and other sequence characteristics are comparable between synthetic and real spacer sequences (see supplementary notebook: spacer_inspection.ipynb)

**False Positive Estimation:** Importantly, the synthetic dataset allows us to estimate false positive rates as a function of distance threshold and search space size. Matches occurring in regions where we did not plan insertions but that meet the distance threshold are considered spurious matches arising by chance. The fraction of these spurious matches provides a proxy for expected false positive rate, conceptually similar to an e-value: "given this sequence length and target database size, what is the expected number of matches with ≤k mismatches by chance?"

See supplementary figure 1 for a visual representation of the data generation workflow.


### Real datasets

To evaluate tool performance in real-world scenarios, we used predicted viral contigs and CRISPR spacers from recent comprehensive databases. 

**Viral contigs:** We used the IMG/VR4 v1.1 high-confidence viral contigs [@camargo_img_vr4_2023], one of the most comprehensive databases of uncultured phage and viral genomes. The viral contigs were predicted using multiple tools (primarily genomad [@camargo_genomad_2024]) and supplemented with sequences from NCBI's RefSeq and GenBank.

**CRISPR spacers:** We used the curated spacer dataset from iPHoP (June 2025 release) [@Roux2023_iphop], which combines CRISPR spacers from both reference genomes and metagenomes. This dataset represents an updated and more comprehensive collection compared to the original IMG/VR4 spacer predictions, containing 3,835,942 unique spacers compiled from CRISPR arrays identified primarily via piler-cr [@edgar_piler_cr_2007] and CRT [@bland_crt_2007] across IMG/M genomes and metagenomes.

**Subsampling strategy:** Due to computational constraints and to evaluate tool behavior at different scales, we employed a stratified subsampling approach for the IMG/VR4 contigs. We created subsamples at varying fractions (0.001, 0.005, 0.01, 0.05, and 0.1) that maintain the distributions of taxonomic class labels from the full dataset. This approach allows us to assess whether tool performance patterns remain consistent across different database sizes, which is particularly relevant as public databases continue to grow rapidly.

**Historical comparison:** The original IMG/VR4 release included spacer-protospacer matching results using blastn with different parameters (specifically `--max-target-seqs`), enabling direct comparison with historical analyses and highlighting the impact of parameter choices on detection outcomes.

For more information about the datasets, please refer to the IMG/VR4 [@camargo_img_vr4_2023] and iPHoP [@Roux2023_iphop] publications.


### Alignment and Recalculation of Mismatches {#sec-alignment-recalc}

For comparing alignments across tools, we use hamming distance (counting only substitutions) as our primary distance metric, with biological and computational justification provided below. However, to enable comparison with tools using different alignment strategies, we also calculate edit distance (allowing insertions and deletions) for reported matches.

**Biological Justification for Hamming Distance:**

**Important Note - Scope of This Analysis:** This benchmark addresses spacer-protospacer matching for inferring historical phage-host interactions, which is fundamentally different from predicting CRISPR off-target effects in gene editing. When considering mismatches between spacers and viral sequences, we explicitly aim to identify evolutionary relationships since protospacer acquisition, where sequence divergence indicates selective pressure to mutate and escape host defenses. For gene editing, even partial base-pairing (including alignments with indels) can cause unwanted off-target cleavage. In contrast, for phage-host evolutionary analysis, known escape mutations reflect selection that occurred before escaping detection by the host defense system. Penalizing indels aligns with the assumption that over short evolutionary timescales, most indels cause lethal frameshifts in coding-dense phage genomes. Additionally, phage targeting by CRISPR systems in natural environments likely operates far from excess or saturation conditions, requiring high sequence complementarity for functional base-pairing. Therefore, our recommendation to prioritize hamming distance applies specifically to the question "was this spacer acquired from this sequence and has it evolved under selective pressure?" rather than "could this guide cause off-target effects in gene editing?"

Several lines of evidence support the use of hamming distance over edit distance for CRISPR spacer-protospacer matching in the context of phage-host interactions:

1. **Mutation rates:** Indels are approximately 4× rarer than substitutions in bacteria. While quantitative measurements in phages are limited, this trend is expected to be similar or more pronounced in phage genomes.

2. **Experimental escape mutations:** Most experimental phage-host studies report that escape mutations are predominantly single substitutions, particularly in the PAM-proximal "seed" region. While indels have been reported, quantitative comparisons across mutation types are lacking in literature.

3. **Phage genome structure:** Phage genomes are coding-dense, with most of the genome covered by coding sequences. A single indel in a coding sequence causes a frameshift mutation, which is often lethal. Substitutions may only affect a single amino acid residue. Frame-preserving indels (multiples of 3 bp) are extremely rare but when observed may be particularly strong indicators of selection.

4. **False positive rates:** Our empirical analysis using synthetic data demonstrates that edit distance >3 leads to >10% false positives, while hamming distance ≤3 maintains <1% false positive rate. This dramatic increase in false positives is not justified by the minimal additional biological information gained.

5. **Sequencing vs biological indels:** With sufficient sequencing depth, sequencing-induced indels should be rare in assembled contigs for Illumina-based datasets. For low-accuracy technologies (e.g., Oxford Nanopore R9 chemistry) or low-depth raw reads, some tolerance for indels may be necessary, though alignment quality cannot exceed underlying sequencing accuracy.

6. **Large-scale rearrangements:** While large genomic rearrangements exist (e.g., gene order changes affecting spacer targets at gene boundaries), these won't be captured by edit distance searches and are not well quantified in literature.

We acknowledge that most existing literature focuses on substitutions, potentially creating an "assumption bias" where indels are underexplored. Future experimental work could systematically compare escape mutation types.

**Distance Calculation:**

To ensure consistent distance calculation across tools, we realigned all reported matches using parasail [@Daily2016_parasail], specifically the `nw_stats_scan` (Needleman-Wunsch global alignment). For hamming distance calculations, we use gap opening and extension penalties of 10 with scoring matrix `nuc44`, which strongly penalizes gaps. For edit distance calculations (used only for comparative analysis), we use standard parameters allowing indels. This approach allows us to:

- Calculate hamming distances independently of tool-specific scoring schemes
- Verify reported matches and their orientations  
- Generate standardized alignment visualizations for manual comparison
- Compare hamming vs edit distance effects empirically


### Performance definition and calculation {#sec-performance-calc}

We defined true and false positives/negatives differently for synthetic and real datasets. For synthetic datasets, sequences were generated following pre-planned insertion patterns with known coordinates, strands, and number of mismatches. While analyzing 7,638,511 pre-planned spacer occurrences, we discovered that some sequences could align at unplanned locations while still meeting our mismatch threshold (≤3). To ensure accuracy, we recalculated all alignments and filtered out sequences with extracted contig regions longer than 130bp (120bp being the maximum spacer length). After this validation, we identified 999,916 additional valid alignments (13 with 0 mismatches, 68,612 with 1 mismatch, 224,044 with 2 mismatches, and 707,247 with 3 mismatches), demonstrating that with short sequences, the number of potential spurious alignments increases substantially with the allowed mismatch threshold. Given their validated alignment scores, we included both pre-planned and these additional alignments in our positive set. For real data (IMG VR4), the positive set comprised all alignments from all tools that met the specified threshold criteria. In both cases, we calculated standard performance metric:

Recall = true positives / (true positives + false negatives)

where true positives are matches found by a tool that exist in the positive set, false positives are matches reported by a tool that do not exist in the positive set, and false negatives are matches in the positive set that were not found by the tool. We note that in this context, "true negative" (i.e., a false alignment a tool did not report) does not represent a meaningful metric.


### Benchmarking framework {#sec-benchmark}

### Computational Resource and runtime tracking {#sec-resource-tracking}

While we have made the runtime and resource (memory and storage usage) data available, the primary focus of this study was to evaluate the ability of each tool to accurately identify spacer-protospacer matches, rather than their computational performance. However, as resource usage may become a limiting factor in the future (with the advent and accumulation of metagenomic sequence data), we designed the benchmark to automatically log several usage metrics.

For the synthetic data, we used hyperfine [@Peter_hyperfine_2023] to track runtime and resource usage, with a maximum of 5 runs for each tool. For the real data, we used SLURM's built-in accounting system for cluster-based runs (`sacct`) via a custom script (pyseff.py)[@pyseff]. For each tool, several metrics were recorded, including wall clock time, Peak memory usage, CPU utilization, I/O operations (see supplementary figure 1 for an overview of the synthetic data workflow). Memory efficiency was calculated as the ratio of the peak memory usage divided by the total allocated memory, and it is important to note that certain tools would identify available memory and utilize all of it, while other tools would utilize it in a deterministic manner given input size, parameters, threads, batch size, etc. This can also affect the overall runtime for different tools, making it difficult to directly compare the scaling efficiency of different tools. Additionally, the runtime reported is not the actual runtime of the tool, but rather the runtime of the SLURM job. While most tools were run as a single job, some tools (notably blastn-short) failed to complete given our maximum available resources (either due to time or memory constraints), and were instead run as multiple jobs, on different subsets of the data. As such, the runtime reported for these tools is the sum of the runtime of all their jobs.


### Versioning and Reproducibility {#sec-reproducibility}

All tools were installed and managed using conda [@conda] (via the mamba [@mamba] package manager) in isolated environments. Each tool was installed in a separate environment to prevent dependency conflicts and ensure reproducibility. Environment activation time was excluded from performance measurements to focus on actual tool runtime.

The exact versions and configurations of all tools were recorded in environment files, allowing for exact replication of our testing environment. All benchmarks were performed on identical hardware configurations to ensure fair comparison.


### Extensibility

The framework is designed to be expandable through the integration of new tools. Each tool/software configuration is saved as a separate JSON file, which includes the exact commands and conda/mamba environment it uses. This configuration files can use placeholder variables which the main benchmarking script replaces with user choices during execution (such as {threads}, {contigs_file}, {spacers_file}, {output_dir}, and {results_dir}). A new JSON file can be added manually or via bench.utils.tool_commands:add_tool function in a semi automated method.


## Results

### Performance as a function of mismatch threshold {#sec-mismatch-performance}


![Performance of each tool as a function of mismatch threshold. The horizontal axis shows the number of allowed mismatches, while the vertical axis represents the mean detection fraction (0-1) aggregated across all spacer-contig pairs at a given mismatch threshold. Each color and shape indicates a different tool plot (shapes connected by lines for interpolation) Panel B shows the performance of the tools on the IMG/VR4 dataset, while panel A shows the performance of the tools on the synthetic dataset.](figures/main/tool_performance_by_mismatches.svg){#fig-tool-performance}

First we investigated potential tradeoffs and effects of the total edit distance (henceforth, interchangeable with mismatches) on the observed recall metric of the tools. Generally, the detection rate of each tool decreases as mismatch thresholds increase. Additionally, no single tool was able to identify all spacer occurrences, although at 0 mismatches the recall of bowtie1, bowtie2, blastn and mummer4 is approximately 0.99 (See supplementary table 2 for the recall values for each tool at different mismatch thresholds). At increased allowed mismatches, the tools showed more divergence, yet bowtie1 remained the single tool with the most unique matches by a considerable margin (@fig-tool-performance). Overall, the performance of the tools is similar between the synthetic and real datasets, albeit the overall lower sample size of the synthetic data should be considered when interpreting the results (see [table 1](#tbl-tools)).


### Performance as a function of query (spacer) abundance in reference database {#sec-abundance-performance}

![Comparison of recall (detection rate) across different mismatch thresholds and target abundance levels for IMG/VR v4 virus and spacer dataset. Top panel displays the subset of results with up to 1 mismatch, and the bottom panel displays the results with up to 3 mismatches. The horizontal axis shows the number of target occurrences on a logarithmic scale from 1 to 10^4^, while the vertical axis represents the mean detection fraction (0-1). Each color and shape indicates a different tool plot (shapes connected by lines for interpolation). The low-abundance region (1 - 1000 occurrences) is binned into logarithmically-spaced bins, while the high-abundance region (>1000 occurrences) is divided into only 3 additional bins, as such ultra-high abundance sequences are rare. The detection fraction is the mean detection fraction across all spacer-contig pairs at a given mismatch threshold and target abundance level.](figures/main/recall_vs_occurrences_combined.svg){#fig-recall}

We then investigated if there are any potential effects for the number of times each protospacer sequence appears in the target set (i.e. the virus sequence set). For perfect matches (0 mismatches), bowtie1 demonstrates exceptional performance with recall rates consistently above 0.99 across all occurrence frequencies (Figure 2). Mummer4, bowtie2 and blastn all maintain a detection rate close to bowtie1. For low-occurrence spacers (1-10 occurrences), strobealign achieves detection rates of 95.44% but shows a systematic decline to approximately 20% for spacers occurring >100 times, and further drops below 5% in the high occurrence range (>1000). 

When allowing one mismatch, the overall detection capabilities decrease across all tools, although Bowtie1 maintains its high performance. At up to three mismatches, the overall recall rates for all other tools further decrease, while Bowtie1 maintains detection rates above 97% throughout the occurrence spectrum.

The data shows a consistent pattern where detection rates generally decline for spacers with very high occurrence frequencies (>1000), though this effect becomes less pronounced as more mismatches are permitted. Quantitatively, this decline is most evident in tools like strobealign and bbmap-skimmer, while bowtie1 maintains its high performance even with highly repetitive sequences. Detailed statistics and recall curves for exact mismatch values (rather than at a maximal value) can be found in the supplementary.


### Overall number of identified spacer-contig {#sec-pairwise}

![Tool vs Tool (pairwise) comparisons - set intersections and differences matrixes. The value of a cell(i,j) is number of spacer-contig pairs identified by the tool listed in row i, which were not identified by the tool listed in the j column. Panel A shows the results for the synthetic dataset, while panel B shows the results for the IMG/VR4 dataset. ](figures/main/tool_comaprison_matrix.svg){#fig-pairwise}  

The pairwise comparison of the tool results suggests (@fig-pairwise), reinforces the observation regarding bowtie1's unique ability to recover a maximum number of spacer matches. Generally, it appears that, when compared to any single other tool, the total number of contig-spacer pairs bowtie1 misses is relatively smaller than the number of pairs the compared tool identified which were not identified by bowtie1.


## Discussion

### Tool Performance and Recommendations

Our analysis, combining synthetic datasets with known ground truth and real-world metagenomic data, reveals critical insights for CRISPR spacer-protospacer matching tool selection.

**Primary Finding - Hamming Distance vs Edit Distance:**

Our synthetic dataset analysis demonstrates that the choice between hamming distance (substitutions only) and edit distance (allowing indels) has profound implications for false positive rates. Using sassy, the only tool with perfect recall supporting arbitrary edit distances, we found that:

- **Hamming distance ≤3:** <1% false positive rate, maintaining high biological relevance
- **Hamming distance >3:** >1% false positive rate
- **Edit distance >3:** >10% false positive rate, dramatically higher than hamming distance

This dramatic increase in false positives when allowing indels is not justified by minimal additional biological information gained. Our biological justification for preferring hamming distance includes: (1) indels are ~4× rarer than substitutions in bacteria, (2) most experimental escape mutations are single substitutions, particularly in PAM-proximal regions, (3) phage genomes are coding-dense making frameshift-inducing indels often lethal, and (4) with sufficient sequencing depth, sequencing-induced indels should be rare in assembled contigs from Illumina data.

**Tool Recommendations by Use Case:**

Based on our comprehensive benchmarking, we provide the following evidence-based recommendations:

1. **Primary recommendation for most applications - Bowtie1 (hamming ≤3):**
   - **Use for:** Large-scale metagenomic analyses, routine host-virus prediction, high-throughput spacer-protospacer matching
   - **Performance:** >95% recall for 0-3 mismatch spacers, <1% false positive rate
   - **Advantages:** Excellent computational efficiency, scales well to millions of spacers and billions of bases, maintains high performance even for high-abundance targets
   - **Limitations:** Maximum 3 substitutions, does not support indels
   - **Biological justification:** Most experimental escape mutations are ≤3 substitutions; higher thresholds increase false positives without substantial biological gain

2. **Extended hamming distance - Indelfree.sh indexed mode (hamming ≤5):**
   - **Use for:** Scenarios requiring detection up to 5 substitutions while maintaining hamming distance
   - **Performance:** Good recall for 4-5 mismatch spacers without the false positive explosion of edit distance
   - **Advantages:** Extends beyond bowtie1's 3-mismatch limitation while avoiding indel-associated false positives
   - **Limitations:** More computationally intensive than bowtie1, still limited to substitutions only
   - **When to use:** When analyzing divergent sequences or when increased sensitivity is needed beyond 3 mismatches

3. **Sassy (edit distance) - Specific applications only:**
   - **Use for:** 
     - Small datasets where computational cost is acceptable
     - Experimental setups where mutation type (substitution vs indel) is of research interest
     - Comparative studies of escape mutation types in controlled systems
     - Methodological validation and establishing baseline performance
     - Low-accuracy sequencing (e.g., Oxford Nanopore R9) where indel tolerance may be necessary
   - **Performance:** Perfect recall (100%), supports arbitrary edit distances
   - **Limitations:** Massive computational requirements (~1M CPU seconds for IMG/VR4), enormous output size (4.7TB for 5% subsample with ≤5 edits), prohibitive for large-scale analysis
   - **Critical note:** We recommend sassy only for specialized applications where its unique capabilities (perfect recall + arbitrary edit distance) are essential and computational resources are available

4. **BLASTn-short - Parameter-dependent performance:**
   - **Use for:** Legacy compatibility, when familiarity with BLAST ecosystem is important
   - **Critical considerations:** Performance heavily dependent on `-max_target_seqs` parameter; default value significantly impacts high-abundance spacer detection
   - **Our analysis:** Used most sensitive parameters (lowest word size, high e-value, `-max_target_seqs 100000`)
   - **Advantages:** Familiar to many researchers, well-documented
   - **Limitations:** Lower recall than bowtie1, especially for high-abundance targets; parameter sensitivity requires careful configuration

**Algorithmic Insights:**

A key finding is that tools differ not because they are "bad" but because their algorithms solve different computational problems. Edit/affine-based algorithms (bowtie2, minimap2, BLAST, sassy) naturally report more matches than hamming-based ones (bowtie1, indelfree.sh) when using the same numeric threshold - this reflects fundamental algorithmic differences rather than tool quality. For CRISPR spacer-protospacer matching, where biological evidence strongly supports substitution-dominant mutation patterns, hamming-based approaches are more appropriate.

**Abundance Effects:**

Our findings reveal that within the mismatch thresholds tested (≤3), no single tool identified all spacer occurrences. The main performance differentiator is how tools handle high-abundance spacers (>1000 occurrences). Bowtie1 maintains >97% recall even for highly repetitive sequences, while other tools show systematic decline (e.g., strobealign drops to <5% for >1000 occurrences). This abundance sensitivity likely reflects tool-specific heuristics designed for different use cases (e.g., read mapping assumes reference sequences are not highly redundant).

**Practical Implications:**

Of specific concern is the relatively high number of alignments missed by blastn-short at default parameters, which is currently common in published analyses. Missing genuine spacer-protospacer pairs can significantly impact downstream conclusions about MGE host range, virus-host networks, and CRISPR system evolution. Our analysis suggests that many published studies using blastn-short may have substantially underestimated the number of spacer-protospacer matches, particularly for high-abundance targets.

**Context-Dependent Considerations:**

Experimental and analytical context must be considered when selecting tools:

- **Large-scale meta-analyses:** Favor bowtie1 for high recall and computational efficiency. In these studies where spacers and targets may not co-occur (from different samples/environments), high recall is critical while acknowledging that similarity implies ancestral encounters rather than current infectivity.

- **Experimental isolate studies:** When studying known phage-host pairs from isolates or temporally resolved samples, tools allowing higher mismatch tolerance may be appropriate, though biological justification for edit distance remains weak.

- **CRISPR array context:** Spacers from complete arrays provide additional information (order, genomic location, host genome origin). Recent studies (Mitrofanov et al., Vink et al.) reveal system-specific spacer loss patterns and mismatch tolerance, which may inform post-search verification.

- **Low-complexity filtering:** Regardless of tool choice, applying DUST masking or similar complexity filtering (e.g., ldust from minimap2, BBDuk from bbmap) prior to searching is prudent to reduce spurious matches from repetitive regions.

Another consideration should be the source of the spacer data: spacers sequences extracted from raw NGS data and spacers extracted from assembled CRISPR arrays (either from assembled or long read sequencing). Specifically, spacers from complete arrays present additional information, namely the location and order of the spacers within the array, and the observation they originate from the same host genome. Notably, a recent in-depth study by Mitrofanov et al @Mitrofanov2025, investigating the mutational landscape of repeats across many isolate prokaryote genomes, have identified patterns of spacer loss based on system sub/type and location. A similar meta-analysis of spacer mutations by Vink et al @Vink2021, have revealed that different CRISPR subtypes exhibit varying tolerance for mismatches within the spacer sequences, with most matched spacers containing three or fewer mismatched nucleotides. This aligns with our current general recommendation of using Bowtie1. Additionally, Vink et al observed that Type I-E and Type II systems preferentially target template strands while Type I-A, I-B, and Type III systems prefer coding strands, emphasizing system-specific characteristics which may also inform post-search verification methods (albeit this may require additional information, such as the sequences orientation or coding potential, and the CRISPR subtype of the spacer).

### Biological Interpretation and Potential False Positives

While our technical comparison focuses on tool performance, the biological interpretation of identified matches also requires careful consideration. The arms race between prokaryotes and MGEs creates a complex landscape where simple sequence matching may not directly translate to genuine host-parasite relationships. We identified several scenarios that could lead to false positive assignments:

1. **Low Complexity Sequence Matches**: Independently of the tool choice, low-complexity (regions with highly skewed GC content, or composed of many repeated sequences) can be susceptible to spurious matches. Low complexity regions may be present in both the virus target set, or in the spacer set, where some non-CRISPR repeated sequences may have been misclassified as such. While certain tools employ filters and heuristics to mitigate the effect of low complexity regions, a prudent procedure should include a step separated from the search, to specifically identify, filter or mask the spacer and virus sets. Dustmasker [@Morgulis_2006], or a similar tool (e.g. ldust from minimap, or BBDuk from bbmap) could be used prior to the search.

2. **Common Sequence Motifs**: Some matches may correspond to highly conserved sequences shared across various biological systems. For instance, horizontal gene transfer events can lead to the spread of similar sequences across diverse MGEs, potentially creating spurious matches that don't reflect direct host-MGE interactions. An example of potential HGT mediated matches was described by Kosmopoulos et al. 2023, where a transposon-mediated transferred of a phage lysin gene (to the host genome) created a true sequence similarity (which was verified by the authors using a combination of sequencing technologies) [@Kosmopoulos_2023]. Even if anecdotally observed, this suggests that an unknown number of observed "good" alignments may be due to HGT, which in the absence of additional information, could not be ruled out as a false positive.

3. **Self-Targeting Events**: Some matches may represent CRISPR targeting of host genes [@Wimmer_2020]. Previous studies estimated a varying amount of these actually target sequences with putative exogenous origin such as prophages, ranging from ~50% [@Stern_2010], to ~80% [@Shmakov_2017]. In Shmakov et al., the authors estimate non-defence targeting is likely a rare event. So far, most observations of non-defence (or counter defence) molecular functions of CRISPRs did not directly involve the spacer sequences, but rather the Cas genes or related effectors. Some observed functions include genome remodelling and evolution, or temporal regulation of gene expression. For example, in *Francisella novicida*, Sampson et al. 2013 demonstrated that certain lipoprotein production is mediated by a CRISPR system [@Sampson_2013].

4. **non-chromosomal replicon encoded CRISPRs**: Similarly to the potential of CRISPR systems to act in non-immune functions, in certain scenarios spacers may be acquired from non-MGE replicons, or be carried (even if partially) by mobile elements. While some types are known to be chromosomal, others are known to be carried entirely by plasmids (i.e. both the Cas proteins and the array loci are on the plasmid), for example in various halophilic archaea [@Maier_2018]. A recent study by Zhang et al [@Zhang_2025] observed similar phenomena in the human gut microbiome, specifically in _Bifidobacterium longum_. Another non-MGE targeting phenomena in archaea was described by Turgeman-Grott et al. [@Turgeman_Grott_2018], where inter-species spacers (targeting genes from related species) were demonstrated to be common in archaea, at least in the context of cellular mating. Another confounding factor is the potential of certain MGEs to target host genes, potentially for regulatory functions, or as counter-defense mechanisms[@Shmakov_2023]. Shmakov et al. 2023 have identified widespread CRISPR-derived phage-encoded mini-arrays, which can hijack and interfere with their host native system.


### Study Limitations

**Synthetic Dataset Characteristics:**

The synthetic data was generated with sequence characteristics matched to real biological sequences, including:
- GC content matched to iPHoP spacer data (~49%)
- Simulated Contig GC% and length distributions based on IMG/VR4 characteristics (~46% GC)
- Verification that k-mer distributions and sequence complexity match real data (see supplementary notebook: spacer_inspection.ipynb)

However, real biological sequences have additional complexities not fully captured, including locus-specific composition biases and regional variation in nucleotide frequencies. These differences should be considered when interpreting results, though our enhanced synthetic dataset provides substantially improved realism compared to purely random sequences.

**Distance Metric Focus:**

Our analysis primarily focuses on hamming distance (substitutions only) which we argue aligns with the underlying biological question. While we demonstrate that edit distance dramatically increases false positives (>10% for edit >3 vs <1% for hamming ≤3), we acknowledge that rare cases exist where frame-preserving indels (multiples of 3 bp) could indicate genuine interactions under strong selection.  We used sassy to comprehensively test edit distances up to 5, confirming our recommendation against routine use of edit distance for most applications due to prohibitive computational costs and false positive rates.

For specialized applications requiring indel detection (e.g., low-accuracy long-read sequencing, experimental mutation-type studies), sassy provides perfect recall but at massive computational cost (~1M CPU seconds, 4.7TB output for 5% subsample). We note that while long-read technologies are attractive for CRISPR arrays (which are repetitive and prone to short-read misassembly), alignment quality cannot exceed underlying sequencing accuracy.

**Sequencing Technology Considerations:**

Our recommendations are primarily based on Illumina-derived assembled sequences where sequencing-induced indels are rare with sufficient depth. For other technologies (Oxford Nanopore R9, PacBio CLR) or low-depth datasets using raw reads, different considerations may apply. However, we argue that even in these cases, biological indels remain rare compared to substitutions, and the false positive explosion with edit distance must be carefully weighed against potential benefits.

**Parameter Space:**

Tools were not exhaustively tested across all possible parameter combinations, and further optimization may be possible. Notably:
- MMseqs2: Latest release version had frequent crashes; we used latest GitHub commit
- LexicMap: No formal v1 release at time of testing; newer versions available
- Index construction parameters (offset rate, seed selection) may affect performance

We focused on parameters maximizing sensitivity and avoiding artificial limitations on multiple match detection, representing realistic use cases rather than exhaustive parameter sweeps.

**Sample Size and Scale:**

The synthetic dataset is smaller than the real dataset, though this enabled controlled testing of rare scenarios like ultra-high spacer occurrence rates. We employed stratified subsampling of real data (IMG/VR4 contigs at 0.001, 0.005, 0.01, 0.05, 0.1 fractions) to assess tool behavior across different scales, finding consistent performance patterns suggesting our recommendations are robust across database sizes.

## Conclusion

Our comprehensive comparison of spacer-protospacer search tools, combining synthetic datasets with known ground truth and real-world metagenomic data, reveals critical insights for tool selection and establishes evidence-based recommendations.

**Key Findings:**

1. **Distance metric choice is critical:** Hamming distance (≤3 substitutions) maintains <1% false positive rate while edit distance (>3 edits) leads to >10% false positives. Biological evidence strongly supports hamming distance: indels are ~4× rarer than substitutions, most escape mutations are single substitutions, and phage coding-dense genomes make frameshift-inducing indels often lethal.

2. **Tool performance varies by algorithmic approach:** Tools differ not in quality but in the computational problems they solve. Edit/affine-based algorithms naturally report more matches than hamming-based ones - understanding these algorithmic differences is essential for appropriate tool selection.

3. **Abundance sensitivity distinguishes tools:** The primary performance differentiator is handling high-abundance spacers (>1000 occurrences). Bowtie1 maintains >97% recall for highly repetitive sequences, while other tools show systematic decline due to heuristics designed for different use cases.

4. **Current practices may miss many matches:** BLASTn-short at default parameters misses substantially more matches than bowtie1, particularly for high-abundance targets. Many published studies likely underestimate spacer-protospacer matches.

**Evidence-Based Recommendations:**

- **Most applications:** Bowtie1 with hamming ≤3 (high recall, low false positives, excellent computational efficiency)
- **Extended mismatches (4-5):** Indelfree.sh indexed mode (avoids edit distance false positive explosion)
- **Specialized only:** Sassy for small datasets when mutation type matters or perfect recall is essential (computationally prohibitive for large-scale analysis)

**Broader Implications:**

The interpretation of spacer-protospacer matches requires careful consideration of biological context beyond tool performance: low-complexity sequences, horizontal gene transfer, self-targeting, and mobile element-encoded CRISPRs can all complicate straightforward interpretation. Proper workflow should include complexity filtering, post-search verification of alignment quality, and contextual analysis of genomic location and phylogeny.

Our findings emphasize that tool selection should be guided by understanding algorithmic assumptions and their alignment with biological expectations. As CRISPR spacer and viral databases continue growing rapidly (from 366,799 spacers in 2017 to 3,835,942 in 2023), choosing appropriate tools becomes increasingly critical for accurate host-MGE interaction inference.

We provide not just tool performance metrics but a framework for understanding why tools differ and how to select appropriate methods for specific research contexts. This work enables more accurate inference of virus-host relationships and CRISPR system evolution across diverse microbial ecosystems.
 
## Code and data availability

All code generated for this study can be found in the git repository: [code.jgi.doe.gov/spacersdb/spacer_matching_bench](http://code.jgi.doe.gov/spacersdb/spacer_matching_bench). All raw outputs (tool results on real and synthetic datasets, the simulated sequence files, the SLURM logs, and the hyperfine runtime measurements) are available on Zenodo [@zenodo_doi].

## Acknowledgements
Work conducted by the U.S. DOE Joint Genome Institute (https://ror.org/04xm1d337) (SR, UN, APC and BB), a DOE Office of Science User Facility, is supported by the Office of Science of the U.S. DOE operated under Contract DE-AC02-05CH11231.  

We would like to thank the following people for their helpful feedback and suggestions:
Uri Gophna, Georg Rath, and Ragnar Groot Koerkamp for valuable discussions on distance metrics and tool performance.