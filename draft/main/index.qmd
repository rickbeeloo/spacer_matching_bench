---
title: "Computational Tool Choice Impacts CRISPR Spacer-Protospacer Detection"
author:
  - name: Uri Neri*^1^
    corresponding: true
  - name: Antonio Pedro Camargo^1^
  - name: Brian Bushnell^1^
  - name: Rick Beeloo^2^
  - name: Simon Roux^1^
format:
  pdf:
    documentclass: article
    geometry:
      - margin=1in
    fig-format: pdf
    embed-resources: true
    keep-tex: true
    fig-pos: 'H'
    number-sections: true
    cite-method: biblatex
    bibliography: references.bib
    pdf-engine: xelatex
  html:
    toc: true
    toc-depth: 3
    number-sections: true
  docx: default
execute:
  echo: false
  warning: false
---

1: DOE Joint Genome Institute, Berkeley, CA, USA\
2: Utrecht University, Padualaan 8, Utrecht, NL 3584 CH\
\* Uri Neri (uneri\@lbl.gov)

## Abstract {#sec-abstract}

CRISPR (Clustered Regularly Interspaced Short Palindromic Repeats) systems are a fundamental defense mechanism in prokaryotes, where short sequences called spacers are stored in the host genome to recognize and target exogenous genetic elements. Viromics, the study of viral communities in environmental samples, relies heavily on identifying these spacer-target interactions to understand host-virus relationships. However, the choice of sequence search tool to identify putative spacer targets is often overlooked, leading to an unknown impact of downstream inferences in virus-host analysis. Here, we utilize simulated and real datasets to compare popular sequence alignment and search tools, revealing critical differences in their ability to detect multiple matches and handle varying degrees of sequence identity between spacers and potential targets. Finally, we provide general guidelines that may inform future research regarding matching, which is a common practice in studying the complex nature of host-MGE interactions.

## Introduction {#sec-introduction}

CRISPR (clustered regularly interspaced short palindromic repeats) systems play a vital role in prokaryotic defense against mobile genetic elements, including viruses, plasmids, and other autonomous genetic elements [@Mojica_2005; @CRISPR_review]. These systems are organized as arrays in the bacteria or archaea genome, where short sequences called spacers are interspersed between repeated sequences. The spacer sequences within these arrays guide the targeting of invasive genetic elements, allowing for specific defense against these threats [@CRISPR_classification]. The corresponding locus on the virus genome where the spacer complements is termed "protospacer". The analysis of spacer-protospacer pairs is essential in understanding the complex interactions between hosts and MGEs [@Edwards2015_phage_host]. Beyond their natural role in prokaryotic immunity, CRISPR systems have been adapted into powerful gene-editing frameworks for biotechnology and therapeutic applications (cite nnn), though the computational considerations for analyzing natural CRISPR-mediated phage-host interactions differ substantially from predicting off-target effects in gene editing contexts, as discussed below.

The identification of genuine host-MGE interactions through spacer-protospacer matching presents unique challenges due to the dynamic nature of these relationships and the complexity of sequence evolution. While matches between spacers and protospacers are often interpreted as evidence of interaction, various biological and technical factors can complicate this interpretation [@Edwards2015_phage_host; @soto_perez_crispr_2019].

Several key scenarios can lead to false positive assignments in spacer-protospacer matching. Low complexity sequences can create spurious matches between simple repeat regions (albeit these can be mitigated through complexity filtering such as tantan [@Frith_2010] or DUST [@Morgulis_2006]). Another type of potential false positives are highly conserved sequences shared by unrelated MGEs, potentially resulting from horizontal gene transfer between MGEs. The horizontal transfer of CRISPR arrays themselves on mobile elements further requires careful examination of array genomic context (regions outside the CRISPR loci) and phylogenetic analysis. Self-targeting events, where matches occur against the host genome rather than MGEs, necessitate comparison against host genome databases and analysis of targeting context [@Stern_2010]. Finally, historical acquisition events may not reflect current interactions, requiring consideration of phylogenetic dating, evolution rates and the effects of the protospacers being under selective pressure to mutate (which may reduce the MGE susceptibility to deterioration by the CRISPR system). This is further complicated by the fact that increasing the allowed distance between sequences directly increases the likelihood of identifying non-related sequences as similar (sharing high nucleic identity) to each other.

False negatives present another challenge in spacer-protospacer matching, particularly when dealing with large databases of potential targets. Many alignment and search tools default to reporting only the best (top) matches or the first matches that pass a given threshold for a given query or HSP. This may result in potentially missing additional legitimate matches. Unfortunately, different tools also handle ambiguous or secondary alignments differently: they may be reported completely, reported up to a number or based on relative alignment quality, or omitted. Similarly, cases where a query sequence has multiple equally scoring matches in different reference sequences are not handled uniformly across tools. This limitation becomes increasingly problematic as databases grow larger and more diverse, a single spacer might match (implying a targeting) multiple related MGEs.

Yet despite these variations, the choice of spacer-to-protospacer search or alignment tool is often not deeply considered. Presently, the common option for this task, popularized by Edwards et al and Biswas et al [@Edwards2015_phage_host; @Biswas2013], uses BLASTn [@Altschul1990_blast] with parameters adjusted for short input sequences. However as for most bioinformatic tools, the exact workflow design and parameter choice can impact the outcome, including in sequence analysis. The importance of proper tool usage and parameter interpretation is highlighted by historical examples in bioinformatics. A striking example is the work of Shah et al, @Shah2018, in which they report how certain misunderstandings of BLAST's `-max_target_seqs` parameter may lead to incorrect assumptions about result completeness, potentially impacting published analyses. Albeit this was later clarified by Madden et al., [@Madden2018] (of the blast development team) as an unfortunate combination of a software bug (that were since patched) affecting rare cases, and misconceptions regarding the process BLAST+ uses for tie-breaking (alignments of equal plausibility), and finally a consideration regarding composition base scoring. Apart from the patched bug, the main outcome of this correspondence led to more explicit details in blast documentation (specifically the appendix "Outline of the BLAST process"). Still, this highlights that misconceptions about the expected exhaustiveness of tools' result-reporting can also lead to incorrect assumptions about the outcome of an analysis. In practice, most bioinformatic tools use various heuristics and optimizations, typically designed with specific use cases in mind. For example, most short-read mappers assume the reference to be the output of a singular assembly - which would imply the reference does not contain extremely redundant copies of the same nucleic regions, or a limited number of very similar sequences (e.g. strain variants, alternative splice variants), and this assumption impacts the way read mapping is computed and results are reported.

The choice of tool and its parameters can significantly impact the detection of these multiple matches, with some tools prioritizing speed over completeness by limiting the number of reported matches, or by other internal heuristics such as seed sequence selection from high occurring sequences being penalized. This trade-off between sensitivity and computational efficiency is especially important to consider as most available tools were designed for different tasks than spacer-protospacer matching (e.g. expression analysis, homology detection, and variant calling), and under different assumptions (such as reference and query sequence size and database size or the nature of the reference source: from a single isolate or metagenomic sample rather than from aggregation of sequences from different sources).

**Computational Foundations of Sequence Similarity:**

From a computer science perspective, biological sequences are represented as strings of characters drawn from finite alphabets: DNA and RNA sequences use the four-letter nucleobase alphabet (A, U/T, G, C), while protein sequences use the twenty-letter amino acid alphabet. Determining sequence similarity thus becomes a string matching problem, where the goal is to find all occurrences of a query string (or similar variants) within a reference string or database, subject to specified constraints on permitted differences. The fundamental challenge lies in defining and efficiently computing a meaningful notion of "similarity" between sequences that may have diverged through evolutionary processes including substitutions, insertions, deletions, and rearrangements.

The classical computational approach to sequence alignment employs dynamic programming algorithms, most notably the Needleman-Wunsch algorithm for global alignment [@Needleman1970_global_alignment] and the Smith-Waterman algorithm for local alignment [@Smith1981_local_alignment]. These algorithms guarantee optimal alignments under a given scoring scheme but operate with $O(mn)$ time complexity, where $m$ and $n$ are the lengths of the two sequences being compared. When searching a query of length $m$ against a database of total length $N$, exhaustive application of dynamic programming requires $O(mN)$ operations. For modern metagenomic databases where $N$ can exceed $10^{11}$ bases and query sets may contain millions of spacers, this quadratic scaling becomes computationally prohibitive. For instance, searching 3.8 million spacers (total length \$\sim$132 Mbp) against the IMG/VR4 database ($\sim\$79 Gbp) would require approximately $10^{19}$ operations if using exhaustive pairwise comparisons, translating to centuries of computation time even on modern hardware.

Exhaustive methods that guarantee perfect recall (sensitivity = 1.0) within specified distance thresholds do exist for specific use cases. Tools like Sassy employ bit-parallel algorithms based on Myers' algorithm [@Myers1999_bitparallel] to achieve exhaustive approximate string matching with arbitrary edit distance thresholds, while indelfree.sh (in bruteforce mode) provides exhaustive hamming distance matching. These approaches are valuable for validation and ground truth establishment on small datasets, but their computational costs scale poorly. Our benchmarking demonstrates this scaling limitation: on the 5% IMG/VR4 subsample (715 Mbp), Sassy required \$\sim\$1M CPU seconds; extrapolating to the full dataset (18.9 Gbp) would require \$\>$20M CPU seconds ($\sim\$230 CPU-years), rendering exhaustive searches impractical for routine large-scale analyses.

**Heuristic Algorithms and Their Goal-Driven Design:**

To achieve practical performance on large datasets, virtually all widely-used sequence alignment tools employ heuristic algorithms that sacrifice guaranteed completeness for dramatic improvements in speed. These heuristics are fundamentally goal-driven: they are often designed and optimized for specific biological questions and use cases, with algorithmic choices reflecting assumptions about the expected characteristics of both queries and references. Understanding these design constraints is essential when repurposing tools for applications outside their intended scope.

Heuristic sequence search tools typically employ multi-stage filtering architectures. BLAST [@Altschul1990_blast], perhaps the most widely used sequence search tool, uses a seed-and-extend strategy: it identifies short exact matches ("seeds" or "words") between query and database sequences, then extends these seeds using gapped alignment only in promising regions. The seed length, extension threshold, and statistical framework (E-values based on extreme value distribution) are all calibrated for detecting homologs across diverse sequence databases. Modern short-read mappers use similar principles but with different optimizations: Bowtie1 employs FM-index data structures enabling efficient exact substring matching followed by backtracking to allow mismatches [@Langmead2009_bowtie]; Bowtie2 extends this with a multiseed heuristic and affine gap penalties [@Langmead2012_bowtie2]; Minimap2 uses minimizer-based sparse seeding combined with chaining algorithms to handle long reads with higher error rates [@Li2018_minimap2]; StrobeAlign employs randstrobes (hash-based linked k-mers) to improve seed specificity [@Sahlin2022_strobealign]. Tools designed for large-scale homology searches like MMseqs2 use cascaded k-mer filtering: sequences must share sufficient k-mer matches to pass initial filtering before undergoing more expensive alignment [@Steinegger2017_mmseqs2].

Critically, these heuristics introduce reporting biases and completeness limitations that vary depending on database composition and query characteristics. Many tools employ early termination strategies, reporting only the top $k$ matches or the first matches passing a threshold, which can lead to missing equally valid (within alignment thersholds set) alternative alignments. Smart seed selection may penalize high-frequency k-mers to reduce computational burden from repetitive regions, potentially causing reduced sensitivity for highly abundant targets. Some tools may assume references derive from single-source assemblies and optimize for unique best-hit assignment rather than comprehensive multi-mapping detection. These design choices, while appropriate for the tools' intended applications, currently have unknown impact in the context of spacer-protospacer matching, where queries are short (typicaly within 25-65 bp), searched across diverse reference sequences often comprised of multiple potential hosts genomes and mobile gentic elements (which may share genes), where a comprehensive detection of all valid matches should be consdiered.

**Distance Metrics and Their Biological Interpretation:**

Tools differ fundamentally in how they measure sequence similarity, employing different distance metrics that reflect distinct evolutionary models. Hamming distance counts only substitutions and requires sequences of equal length, making it appropriate for scenarios where length-changing mutations are rare or highly deleterious. Edit distance (Levenshtein distance) allows insertions and deletions in addition to substitutions, reflecting a broader evolutionary model. Affine gap distance extends edit distance by assigning different penalties to gap opening versus gap extension, better modeling the biological reality that indels often occur in clusters. Finally, some applications require exact matching with zero tolerance for differences.

Importantly, when comparing tools using different distance metrics with the same numeric threshold (e.g., "≤3 mismatches"), edit/gap-affine-based algorithms will naturally report more matches than hamming-based ones because they solve a more permissive computational problem (). This reflects different definitions of sequence similarity rather than differences in tool quality. Hence, the choice of distance metric should be driven by the biological question and system being studied.

**Distance Metric Choice and Experimental Evidence:**

For CRISPR spacer-protospacer matching in natural systems, the choice between hamming distance and edit distance has both biological and computational implications. This benchmark addresses spacer-protospacer matching in the context of inferring historical phage-host interactions in natural prokaryotic populations, which differs fundamentally from predicting CRISPR off-target effects in gene editing applications. In natural systems, we aim to identify evolutionary relationships since protospacer acquisition, where sequence divergence reflects selective pressure on MGEs to mutate and "escape" host defenses. For gene editing applications, even partial base-pairing (including alignments with indels) can cause unwanted off-target cleavage, necessitating more permissive distance metrics. Similarly, when working with low-accuracy sequencing data (e.g., Oxford Nanopore R9 chemistry (cite nnn)) or analyzing raw reads rather than assembled contigs (made from sufficent sequecing depth), some tolerance for indels may be necessary to account for sequencing errors, as the alignment quality cannot exceed the underlying data quality.

Experimental studies consistently report that phage escape mutations from CRISPR immunity are predominantly single nucleotide substitutions, particularly in the PAM-proximal "seed" region where mismatches have the strongest effect on targeting. Foundational work by Deveau et al. [@Deveau2008] demonstrated that phages escape CRISPR immunity in *Streptococcus thermophilus* through point mutations in protospacers. Semenova et al. [@Semenova2011] established that in *E. coli* type I-E CRISPR-Cas system, a seven-nucleotide seed region immediately following the PAM is critical for targeting, with mutations in this seed region abolishing immunity by reducing crRNA-guided Cascade complex binding affinity. Fineran et al. [@Fineran2014] further showed that phages readily escape through point mutations in the PAM or seed region. More recently, Schelling et al. [@Schelling2023] demonstrated that phage escape occurs mainly through mutations in PAM and seed regions, with preexisting mismatches at any target location accelerating emergence of mutant phages. Across these experimental systems, escape mutations are consistently reported as single nucleotide polymorphisms rather than indels. Bacterial (the host) mutation rates show indels occurring approximately 4× less frequently than substitutions (cite nnn), and this trend is expected to be similar or more pronounced in phage genomes. Phage genomes are typically coding-dense (\$\>\$90% coding sequences; cite nnn), making frameshift-inducing indels particularly deleterious, while substitutions may affect only a single amino acid residue. Frame-preserving indels (multiples of 3 bp) are extremely rare but, when observed, may be particularly strong indicators of selection (to our knowledge, no previous study has reported this, although we observed this phenomena anecdotaly, see supp. note. nnn). We acknowledge that most existing literature focuses on substitutions, potentially stemming from substitutions being easier to detect and characterize than indels, or a potential "assumption of expected" bias where the lack of reports about indel escape mutations may not translate to it being a less frequent phenomena. Indeed, a systematic quantitative comparisons of mutation type frequencies across diverse phage-host systems remain lacking. The only report of a verified indel escape mutation we were able to find is from a study by Paez-Espino et al, [@Paez_Espino_2015]. In that long-term coevolution experiment with *S. thermophilus* phage 2972, the authors note in the methods section "Finally, postassembly as well as comparative analyses were performed to identify SNPs, indels, and recombination events", however indels (or gaps) are not mentioned in the main text discussing escape mechanisms, and only a single indel event is listed in the supplemental "Table S5. Phage 2972 targeting" among the escape mutations identified, suggeesting even this relatively large exprimental setup is not adqequte to observe enough varied mutations required for statisical analysis.

**False Positives in Sequence Similarity Searches:**

A critical consideration in sequence similarity searches is the expected rate of spurious matches arising by chance rather than true biological relationships. Traditionally, false positives in sequence similarity searches are considered as matches that appear similar by standard alignment metrics but arise from convergent evolution, random sequence similarity, or compositional biases rather than common ancestry or functional relationships (note: in our Methods and Results sections we define false positives very differently, as we lack ground truth for evolutionary relationships; see Methods). For random DNA sequences of equal nucleotide composition, the probability of finding an exact match of length $L$ is approximately $(1/4)^L$, suggesting exact matches should be extremely rare. However, this simple model fails to capture biological reality: sequences are not random but exhibit compositional biases (GC content variation), low-complexity regions (simple repeats, homopolymers), and conserved functional elements that can create spurious similarities.

Most sequence search tools employ statistical frameworks to estimate false positive rates. BLAST calculates E-values representing the expected number of matches with a given score occurring by chance in a database of specified size, based on extreme value distribution theory [@Altschul1990_blast]. This framework assumes sequences are i.i.d. (independent and identically distributed) random samples, an assumption violated by real biological sequences. Low-complexity filtering tools like DUST [@Morgulis_2006] and tantan [@Frith_2010] attempt to mask repetitive regions that contribute disproportionately to spurious matches. However, determining what constitutes a "true" versus "false" positive becomes challenging when dealing with short sequences: a spacer matching multiple related MGE variants may represent true biological targeting of a virus population rather than spurious similarity.

The false positive rate increases systematically with allowed distance threshold and total search space size. As we demonstrate empirically using semi-synthetic data, non-planned matches (validated alignments occurring in regions where we did not insert sequences) increase dramatically with distance: from 1 exact match at hamming distance 0, to 47 matches at hamming distance ≤1, to 2,217 at hamming distance ≤2, and 54,388 at hamming distance ≤3 when searching \$\sim\$3.7 million real spacers against 400k synthetic contigs totaling \$\sim\$40 Gbp. These rates correspond to approximately 0.36 non-planned matches per million spacer-bp searched against million contig-bp at hamming distance ≤3. Allowing indels (edit distance) increases non-planned match rates substantially. This scaling has important implications: as CRISPR spacer databases have grown from 366,799 unique spacers in 2017 [@Shmakov_2017] to 3,835,942 in 2023 [@camargo_img_vr4_2023], and viral contig databases continue expanding through metagenomic sequencing, researchers must account for increasing background match rates when interpreting results.

**Computational Resource Considerations:**

Another important consideration is computational resource requirements. Memory, storage, and availability of CPU cores are factors differing between tools. Parameter choice may also impact these factors considerably, with certain tools offering tunable parameters to trade-off between sensitivity and computational efficiency. In recent years, spacer database size has been rapidly increasing - from 366,799 unique spacers in 2017 [@Shmakov_2017] to 1,173,006 unique spacers reported in 2021 [@Dion_2021] to 3,835,942 unique spacers in 2023 [@camargo_img_vr4_2023]. Similarly, public virus and MGE databases are growing rapidly, with large contributions from metagenomic samples resulting in routine fold increases in the number of predicted viral contigs [@camargo_img_vr4_2023]. Most tools require more resources as the size of the database grows, and as this trend continues, certain workflows and tools may become prohibitively expensive to run in a reasonable time frame.

## Methods {#sec-methods}

### Tool Selection {#sec-tool-selection}

We evaluated several widely-used sequence alignment and search tools, spanning different algorithmic approaches and computational strategies (table 1). The tools were selected based on their availability, historical use in sequence analysis, and diversity of algorithmic approaches. The selection includes both exhaustive methods (Sassy, indelfree.sh in bruteforce mode) that guarantee finding all matches within specified distance thresholds, and heuristic methods that use various optimizations for improved speed.

It is important to note that most of these tools were not specifically designed for CRISPR spacer-protospacer matching, but rather for more general sequence search tasks (MMseqs2, BLASTn-short), alignment/mapping of short reads to reference genomes (Bowtie1, Bowtie2, Minimap2, MUMmer4, StrobeAlign, X-mapper), or versatile pattern matching (Sassy, indelfree.sh). Our focus is specifically on spacer-to-protospacer sequence matching as a bioinformatics task, and we did not evaluate integrated host-prediction tools like SpacePHARER [@Zhang_2021] or iPHoP [@Roux2023_iphop], which perform additional analyses such as phylogenetic evaluation or LCA determination from multiple spacer-protospacer matches.

All tools were configured to maximize sensitivity and avoid artificial limitations on multiple match detection. Some tools required specific parameter adjustments to enable detection of short sequences (e.g., BLASTn-short task, Bowtie1/2 short read modes) or to report all matches rather than only top hits. The exhaustive tools (Sassy, indelfree.sh bruteforce mode) were included specifically to validate the completeness of heuristic tool results on smaller datasets where computational costs remain feasible.

| Aligner | Indexing | Main Algorithm | Heuristic/Exhaustive | Reporting/Limiting Threshold Used in Benchmark | Year | Original Purpose | Notes |
|:--------|:-------:|:--------|:--------|:--------|:-------:|:--------|:--------|
| [Bowtie1](https://github.com/BenLangmead/bowtie) | Yes | FM-Index (BWT) | Yes (backtracking) | Hamming distance | 2009 | Short read mapping | Optimized for 25-50 bp reads (max 1kbp); ungapped alignment only; backtracking heuristic limits to 3 mismatches |
| [Bowtie2](https://github.com/BenLangmead/bowtie2) | Yes | FM-Index (BWT) | Yes (multiseed + extend) | Affine/Edit distance | 2012 | Read mapping | Uses FM-index for seeding with SIMD-accelerated DP extension; supports gapped, local, and end-to-end alignment |
| [Minimap2](https://github.com/lh3/minimap2) | Optional | Minimizer + chaining | Yes (minimizer seeding) | Edit distance | 2018 | Long read mapping | Lexicographically smallest k-mer per window; collinear chaining with gap penalties; versatile across read types |
| [indelfree.sh](https://github.com/bbushnell/BBTools/) | No | Multi-kmer matching | Bruteforce mode is exhaustive, while ion "Indexed" mode this can be limited by selected kmer length, query length, and number of substitutions | Hamming distance | Publically introduced to bbtools September 2025 | Read mapping | BBTools suite is Java based, and will use available memory - so the peak memory reported herein (sourced from SLURM logs) does not equate with "minimal required memory" |
| [StrobeAlign](https://github.com/ksahlin/StrobeAlign) | Yes | Randstrobes | Yes (syncmer thinning) | Edit distance | 2022 | Read mapping | Uses hash-based linked strobes (randstrobes) with multi-context seeds (MCS) for hierarchical search |
| [BLAST+](https://blast.ncbi.nlm.nih.gov/doc/blast-help/downloadblastdata.html) | Optional | Hit-and-extend | Yes (contiguous word) | E-value, bit score | 2009 | Sequence search | BLASTN-short mode uses 11-mer seeds; reports matches based on e-value (expected hits by chance given DB size) |
| [MMseqs2](https://github.com/soedinglab/MMseqs2) | Optional | K-mer prefiltering | Yes (3-stage cascade) | E-value, bit score | 2017 | Sequence search | Double k-mer matching → vectorized ungapped → gapped SW; optimized for many-against-many searches |
| [Sassy](https://github.com/RagnarGrootKoerkamp/sassy) | No | Uses a bit-parallel algorithm based on Myers' bitpacking to perform exhaustive approximate string matching (ASM) | Exhaustive | Edit distance | 2025 | Versatile pattern matching, suggested for use in CRISPR (gene-editing) off target detection and raw-read alignments | Guarantees perfect recall; explores full edit distance landscape; supports arbitrary distances; high computational cost, requires SIMD instructions (AVX2 and NEON), which most modern CPUs support |
| [X-mapper](https://github.com/mathjeff/Mapper) | Yes | Gapped x-mer pyramid | Yes (dynamic seeds) | Edit distance | 2024 | Read mapping | **Preprint** - Dynamic-length gapped x-mers with "pyramid walking" to optimize seed specificity |
| [mummer4](https://github.com/mummer4/mummer) | Optional | 48-bit suffix array | Yes (MUM-based) | Edit distance | 2018 | Genome alignment | Identifies Maximal Unique Matches (MUMs) using enhanced suffix array; handles genomes up to 141 Tbp |

: Evaluated Tools and Their Characteristics. **Indexing:** "Yes" indicates a precomputed index is required (in our benchmark, if an index has to be pre-created the time to construct it is measured along the search/alignment step); "Optional" means a persistent index can be generated for reuse but the tool can also build it on-the-fly for single runs; "No" means no indexing is required or supported. **Heuristic/Exhaustive:** Exhaustive methods guarantee finding all matches within the specified distance threshold; heuristic methods use optimizations (seed-based indexing, chaining, k-mer filtering) to improve speed but may miss some matches. **Reporting/Limiting Threshold:** The primary metric used to report or filter alignments. Hamming distance counts only substitutions; Edit distance allows insertions and deletions; Affine distance uses gap penalties; E-value represents expected matches by chance given database size; Exact match requires perfect identity. Note that edit/affine-based algorithms will naturally report more matches than hamming-based ones when both use the same numeric threshold—this reflects different computational problems being solved rather than tool quality differences. **Tool Configuration:** All tools were configured to maximize sensitivity and avoid artificial limitations on multiple match detection. Exact commands, parameters, and tool versions are provided in Supplementary Table S1. {#tbl-tools}

### Data Generation and acquisition {#sec-data}

#### Dataset Overview {#sec-datasets}

We evaluated tool performance using three complementary apprches with varying levels of ground truth information:

| Dataset Type | Spacers | Spacer Stats | Contigs | Contig Stats | Notes |
|:-----------|:-----------|:-----------|:-----------|:-----------|:-----------|
| **Fully Synthetic** | 100,000 simulated | 132 Mbp total; 25-100 bp range; median 34 bp; GC 49% | 20,000 simulated | 2.0 Gbp total; 1,501-200,000 bp range; median 100,831 bp; GC 46% | Complete ground truth; planned spacer occurences allow differentiation of true vs non-planned matches |
| **Semi-Synthetic** | 3,835,942 real (iPHoP) | 132 Mbp total; 25-100 bp range; median 34 bp; GC 46.73% | 400,000 simulated | 40.3 Gbp total; 1,501-200,000 bp range; median 100,831 bp; GC 46% | Real spacers + synthetic contigs matching IMG/VR4; enables non-planned match rate estimation in realistic sequence composition |
| **IMG/VR4 Real Data** | 3,835,942 real (iPHoP) | 132 Mbp total; 25-100 bp range; median 34 bp; GC 46.73% | 421,431 HQ contigs | 18.9 Gbp total; 1,001-2,473,870 bp range; median 7,664 bp; GC 44.45% | Real-world performance; HQ subset from 5.1M filtered contigs; subsampled at 0.0005× (279 contigs, 7 Mbp), 0.001× (421 contigs, 10 Mbp), 0.005× (2.1k contigs, 57 Mbp), 0.01× (4.2k contigs, 124 Mbp), 0.05× (21k contigs, 715 Mbp), 0.1× (42k contigs, 1.5 Gbp), and 1× (421k contigs, 18.9 Gbp) using stratified sampling to maintain taxonomic distributions |

: Dataset characteristics and ground truth availability. Synthetic contigs were generated to match IMG/VR4 sequence characteristics (GC content, length distributions). Subsampling of IMG/VR4 HQ contigs enabled inclusion of exhaustive tools on smaller fractions while evaluating all tools on larger fractions and the full HQ dataset. {#tbl-datasets}

#### Real datasets {#sec-real-data}

To evaluate tool performance in real-world scenarios, we used predicted viral contigs and CRISPR spacers from recent comprehensive databases.

**Viral contigs:** We used the IMG/VR4 v1.1 high-confidence viral contigs [@camargo_img_vr4_2023], one of the most comprehensive databases of uncultured phage and viral genomes. These sequences are predicted primarily via geNomad [@camargo_genomad_2024] scans of metagenomic data, supplemented with sequences from NCBI's RefSeq and GenBank databases.

To focus on prokaryotic phages and exclude eukaryotic viruses (which typically lack CRISPR systems in their hosts), we applied taxonomic filtering based on ICTV classifications. Specifically, we removed contigs classified into eukaryotic virus families, orders, and classes, including but not limited to major groups such as Adenoviridae, Herpesviridae, Poxviridae, Coronaviridae (families); Herpesvirales, Picornavirales, Bunyavirales (orders); and Megaviricetes, Alsuviricetes, Pokkesviricetes (classes). We additionally filtered out contigs ≤1000 bp to ensure sufficient sequence length for reliable spacer-protospacer matching. After these filtering steps (starting from 5,457,198 high-confidence contigs), the final dataset contains 5,115,894 prokaryotic viral contigs with a total size of \~79 Gbp (range: 1,001 - 2,473,870 bp, median: 7,664 bp, GC%: 44.45%). See Supplementary table nnn for detailed statistics on the contig dataset and filtering steps.

For benchmarking, we selected a high-quality (HQ) subset of 421,431 contigs (\~18.9 Gbp) using stratified sampling to maintain taxonomic class label distributions while focusing on the most reliable viral sequences. This HQ subset served as the base set for subsampling experiments and derived performance analyses. This set is available in the zenodo deposit of this project (citn nnn DOI), and Steps to recreate this set are available in

**CRISPR spacers:** We used the curated spacer dataset from iPHoP (June 2025 release) [@Roux2023_iphop], which combines CRISPR spacers from both reference genomes and metagenomes. To our knowledge, this dataset represents the largest curated spacers extracted from assembled data and is used in existing host-prediction tools. The raw iphop set contain 3,882,812 unique spacers (length range: 25-40 bp, median: 34 bp, GC%: 47.6%) compiled from CRISPR arrays identified primarily via piler-cr [@edgar_piler_cr_2007] and CRT [@bland_crt_2007]. We applied minor additional filtering to remove 55833 spacers (\~1.4% of all) with low sequence complexity or ambiguity (see below "Complexity filtering"). The entire final set of 3,826,979 spacers was used in all benchmarking analyses pretaining to the "real data". See supplementary figure nnn tables nnn for detailed statistics on the spacer dataset composition and feature distribution.

**Complexity filtering:** In this work, our goal is to benchmark the different tools results when they are executed with the most suitable paramters and settings (often CLI arguments) for the task of spacer-protospacer matching. From that prespective, investigating the effects of differtent complexity filtering algorithms and implementation is outside the scope of this project. The different tools evaluted handle complexity and ambiguity differently - some have internal, hard-coded restrictions (e.g. blastn does not select seeds from regions with ambiguous (`N`) bases, but allows extending over them from another seed), or provide option to disable complexity filtering (such as `-dust no` CLI option in blastn). Some tools (like sassy) may allow all query sequences to contain Ns, but may allow restricting the target sequence to region with a maximal fraction of N positions. Previous uses of blastn for this task (such as in CRISPRTarget) tend to explicitly disable complexity filtering. Some host-assignment tools (such as Iphop) employ complexity filtering post-hoc (after collecting the search/alignment tool results). To provide a uniform starting position for all tools, so that the complexity handling is not a confunding factor, we applied a basic complexity filter to remove spacers with low sequence complexity or high ambiguity. We note that these sequences are likely not particularly informative from a biological perspective and may arise from incorrect CRISPR array prediction or extraction, and in some in-house tests for this project, we observed these disproportionately contribute to the computational resource issues associcated with a non-inforative matches (such as extremely massive output files detailing "potential" alignments to regions of Ns).\
The steps and code to reproduce the filtering are available in the project repository (spacer_inspection.ipynb). Briefly, we first calculated the fraction of each nucleotide (A, T, G, C, N) in each spacer sequence, as well as the GC%, Shannon entropy value, and the number of non-unique 6-mers (i.e., 6-mers that occur more than once in the spacer). We then filtered out spacers with any of the following characteristics: any ambiguous bases (N fraction \> 0), low sequence complexity (Shannon entropy ≤ 1), high homopolymer content (any of A, T, G, C fraction ≥ 0.95), or low k-mer diversity (≥4 non-unique 6-mers). This filtering removed 55,833 spacers (\~1.4% of all) and resulted in a final set of 3,826,979 spacers used in all benchmarking analyses pertaining to the "real data".

**Stratified Subsampling Strategy:** For benchmarking purposes, we selected an inital high-quality (HQ), representative subset of 421,431 contigs from the raw img/vr4 dataset as describied above, termed "fraction_1". In this benchmark, we measure the tool results on subsamples of this set for three reasons: first, we can only inclde the exustaive tools (Sassy, indelfree.sh bruteforce) on the smaller fraction as they are computioanlly expensive, secondly by comparing the fraction to fraction variation in each tool's result, we can estimate the tools performence consistency, and thirdly, we can investigate the effect of the dataset (fraction) size on the tools' resource usage (cpu time, memory). To creart these subsamples, we employed a "Representative Sampling" aimed at ensuring the samples reflect the entire sequence population characteristics and diversity. Specifically, each sampled subset had to include representatives from each taxonomic class, at the same proportional qunatities the classes had in the 421k contig set. We note that this is a crucial step as the majority of the prokaryotic viruses in IMG/VR4 belong to a handfull of classes causing random sampling with small sizes to have few or no representative for the various other viral lineages. Using the startified sampling method, we created subsets of several different fractions: 0.0005 (279 contigs, 7.04 Mbp), 0.001 (421 contigs, 9.75 Mbp), 0.005 (2,107 contigs, 57.06 Mbp), 0.01 (4,214 contigs, 123.67 Mbp), 0.05 (21,071 contigs, 715.07 Mbp), 0.1 (42,143 contigs, 1.50 Gbp), and 1.0 (full HQ set: 421,431 contigs, 18.87 Gbp). The same set of 3,826,979 spacers was used for all subsamples and the full HQ set. Note that even for the smaller subsamples (0.0005-0.01 fractions), not exhaustive tool completed within reasonable CPU time budgets. Note, in this project, we only include the alignments reported by any tool if that tool finished within the same time limit. A singular exception is blastn for the fraction_1 set, which was allowed to run completion, as it the main point of reference with regards to historical use of (any) tool for this task.

#### Synthetic dataset generation {#sec-synthetic-data}

To examine each tool's performance across diverse spacer-to-target matching scenarios, we developed a Rust-based simulation framework accessible through a Python CLI interface with fine-grained control over sequence characteristics. The simulator records the ground truth of all planned spacer occurences, enabling differentiation between true positives (planned matches) and non-planned matches (validated alignments occurring in unplanned regions but meeting distance thresholds).

**Customizable Sequence Characteristics:** The simulation framework provides extensive parameterization to generate realistic sequences. Users can specify nucleotide base composition independently for spacers and contigs through either GC content percentages or explicit base frequency parameters (A, T, C, G fractions). Additional parameters control contig and spacer length distributions (uniform or normal), the range of substitution mismatches to introduce when injecting a spacer into simulated contigs, the number or range of times each spacer would be "injected" into a simulated contigs, optional indel mutations (insertion and deletion ranges), and the proportion of spacers to reverse complement. Additionally, a semi-synthetic option is permitted - where either an external (existing) spacer or/and contig set is read from file.

**Comparison to real spacers:** Rather than using purely random sequences and uniform distributions, for the sets describied here, we configured the synthetic data generation to match key characteristics of the real-datasets (i.e. the filtered iphop spacer set and the HQ IMG/VR4 dataset noted above as "fraction_1"). Specifically, we set the GC content to approximately 49% (spacers), and 46% (contigs), and configured contig lengths to be selected under normal distribution from a realistic range (mostly, 1,501-200,000 bp, see supp. data nnn for more dtails). To demonstrate the ability of the simulated sequences to mimic the real data sets, we compared calculated several notable features (e.g. k-mer repeatability, entropy, base frequencies etc) (see Supplementary Figure S6 and notebooks: spacer_inspection.ipynb). Most analysed features appear similar for the simulated and real sequences, with the exception of that in some complexity measures (k-mer repeatability and LCC), the real spacers have slightly wider range of value, suggesting a minor amount of the real spacers have more extreme values in this regards.


#### Semi-Synthetic dataset {#sec-synthetic-data}

This set uses the existing simulation framework, but instead of generating both spacers and contigs from random sequences, we use the same filtered spacer set as the real-datasets, and generate synthetic contigs matching the sequence characteristics of the HQ IMG/VR4 dataset (fraction_1, see table nnn for details). Note, the spacers are not "injected" into the simulated contigs (using the `--number-spacer-insertions 0 0 ` option of the `simulate` command). Wr primarly use this set to estimate "non-planned" match rates in a realistic spacer set sequence composition context, and realistic search space context. In this context (0 planned spacer occurences) we expect all identified matches to reflect chance similarity. We note that this set is considered "large" (by design, similarly to the fraction_1), hence we are only able to use an aggregate of the verifid non-exhustive tools, and only under hamming distance <=3. This suggests that the actual count and rate may actually be larger. See the "Non-planned Match Rate Estimation" section below for details on the defintion of "non-planned" matches.



### Coordinate Tolerance and Unique Region Counting {#sec-coordinate-tolerance}

When aggregating results across tools, we implement coordinate tolerance matching to handle slight boundary differences in reported alignments. We observed tools may report alignments with minor variations in start/end coordinates (typically 1-5 bp) due to different handling of terminal mismatches or gaps (see example in supplementary note nnn). We use a default 5bp tolerance when merging alignments to count unique spacer-contig regions.
This approach reduces double-counting of essentially identical matches, accounts for valid algorithmic differences in gap versus substitution placement at alignment boundaries (or variation in tool-specific "clipping" behaviour), which enables fair comparison of tool results coverage (total unique regions detected).
All reported alignments from all such regions are verified seperatly, by extracting the reference contig region and realigning to the spacer sequence (see Alignment Verification section below). 

### Alignment Verification and Distance Metric Calculation {#sec-alignment-recalc}

For comparing alignments across tools, we use hamming distance (counting only substitutions) as our primary distance metric, with biological and computational justification provided below. Our benchmarking CLI tool supports setting thresholds for three distance metrics: (minimal) hamming distance, (minimal) edit distance, and gap-affine (by measuring the edit distance from an alignment with a user provided cost matrix and gap penalties). However, for the analyses presented here, we focus primarily on hamming distance ≤3 for most datasets, with hamming distance ≤5 used in datasets where computational resources permitted. As noted in the introduction, we recommend prioritizing hamming distance for spacer-protospacer matching in the context of phage-host interactions, as this better reflects the predominant mutation types observed in experimental studies of phage escape from CRISPR immunity. To clarify, in the context of gene-editing, a minimal edit distance might serve as a more appropriate metric for off-target prediction. Despite this recommendation, we have attempted to compare hamming and edit distance effects empirically in our analyses (when computionaly feasible). 
We note that while in practice, we observed near complete aggreement between the minimal edit and gap-affine distance metrics under the conditions tested, these are not identical measurements - a gap-affine distance metric allows for more flexible gap placement and scoring, aimed at capturing biologically relevant (i.e. sharing a common ancestor) relationships, while the minimal edit distance metric will priotise the minimal set of edits (substitutions and indels) regardless of their evolutionary likelihood (e.g. the higher rarity of indels compared to substitutions).

**Distance Verification Methodology:** To ensure consistent and accurate distance calculation across tools that use different internal alignment algorithms and scoring schemes, we independently recalculated distances for all reported alignments post-hoc. For the gap-affine distance metric, parasail's [@Daily2016_parasail] implementation of the Needleman-Wunsch global alignment algorithm is used, for the minimal edit distance metric, the python version of the edlib library [@Šošić_Šikić_2017_edlib] is used (`edlib.align`). For the hamming distance metric, we measure the number of non-identical residues in the aligned region. As hamming distancne can only be calculated for alignments of the same length, and as some tools report gapped alignments, we first pad the shorter sequence with non-matching characters (e.g. "@") and then calculate the hamming distance as the count of positions where the two sequences differ. This approch also address potential differnece in clipping behaviour between tools.


### Performance definitions and calculation {#sec-performance-calc}

We defined the ground truth and classified alignments slightly differently for synthetic and real datasets, as for the larger real-data sets we do not have the results of an exhustive tool to establish a complete set of alignment. 

Speficially, for synthetic datasets, we define three categories: 
1.  **positive_in_plan:** Alignments matching planned spacer occurence coordinates (±5bp tolerance, see §sec-coordinate-tolerance). These represent the intended ground truth matches, i.e. sequences were generated following the simulation pre-planned design at known coordinates, strands, and number of mismatches.
2.  **positive_not_in_plan:** Tool reported alignments within the allowed distance thershold passing our independent validation, but occurring outside planned regions. These represent chance similarities at the specified distance - not false positives in the technical sense (they are valid alignments), but non-planned matches that may indicate increased background noise under a certain condition (distance threshold, search space size, see the "Non-planned Match Rate Estimation" section below).
3.  **invalid_alignment:** Alignments that fail the independent alignment verification, or exceed quality thresholds. These are "true" (in the classical sense) false positives, and we note these tend to result from tool-specific reporting artifacts - not all tools support limiting reported alignments within a specific distance metric and thershold (e.g. strobealign do not have any explictly option to control what alignments are reported), and the parameters we provide to these tools can only approximate it (such as minimal identity or minimal query coverage). Note that the parameter choice is aiming to include at-least all matches within our analysis scope (hamming <=3) but often includes a larger range. In the context of this project, we discard these "invalid-alignments" and do not investigate them further.

We extend these defintions to the real datasets by treating all (validated) alignments reported by all tools as "positive_not_in_plan".  
In this project, we define a tool's "Recall" as the postive rate, or fraction of positives detected by tool. When this fraction is calculated out of the planned alignemnts only (not inclding the non-planned), we specify it by noting "non augmented" (e.g. "non-augmented recall"). This reflects our choice to consider the non-planned validated matches as positives. We argue that from a pure sequence alignment perspectve, they are as correct (within the distance threshold) as the planned, and that for the real datasets, where we do not have a complete set of planned matches, this is the only way to calculate recall in practice. Furthermore, we specifically recommened the distance metric and thershold choice (hamming<=3) as we estimate to by considerably low than the number of real alignments we observe in the real data-set (note, this is not the case for higher distance thresholds or for differnet distance metrics).
Additionally, we note that metrics such as precision (\(\frac{TruePositives}{TruePositives+FalsePositives}\)) are not applicable in this context - as were we to define the non-planned matches as false positives (for the synthetic sets),we could control this value by adjusting the simulation parameters (i.e. the number of planned spacer occurences). We also note that in this framework "true negatives" ("all correctly not reported matches that do not actually align") is not a sensical or useful defintion. By extension we can not compute certain common performance metrics such as specificity.  

We acknoledge that a limitation of this system is the lack of a complete set of all positives for datasets too large to be exhaustively searched (using sassy and indelfree.sh bruteforce mode). In such cases, the positive set is essentially the union of all valid tool reported alignments.

**Non-planned Match Rate Estimation:** Using the synthetic datasets allows us to estimate the frequency of these reported alignments as a dependency of distance metric and threshold, and of the search space size. As we control the exact details in the simualted runs, we generate a ground truth table, where the location, number of mismatchs, spacer and contig identifiers of each planned spacer occurence is recorded. By combining this ground truth with the different tool resutls (particularly the exhustive ones), we are able to identify (and subsequently verify) any reported alignment - and record the number of valid (within distance thershold) alignment not explictly planned (occurring in regions other than those in the simulation plan). We expect these non-planned matches to represent chance similarities arising from sequence composition and length. To estimate the rate of such non-planned matches, under a given distance threshold, we divide the number of validated non-planned matches by different representations of the total search space size: A metric "engulfing" both spacer and contig set sizes in bp (sum of spacer legnths \* sum of contig lengths), or the product of the number of spacers and contigs (e.g. per n spacer and m contigs). Realistically, total spacer length is negligible compared to total contig length, however the product of the number of spacers and contigs assumes every contig and spacer effect the search space equally.  
We quantified non-planned match rates using the exhustive search tools under varying hamming (indelfree bruteforce) and edit (sassy) distance thresholds (1 - 5). For large datasets (where the use of exhustive search tools is too computionaly prohibitive), we either reduced the thershold the range (1-3) if possible, altough for the full set (fraction_1, the semisytnthetic set, and the largest of the simulated runs) we resorted to use the aggregation of the non exustaive tools (namely Bowtie1 and blastn) results as proxy.


%%%%% maybe move this to results %%%%%
This methodology revealed that non-planned match rates increase dramatically with allowed distance, with substantially higher rates when allowing indels compared to substitutions only. At hamming distance \u22643, we observed 54,388 non-planned matches (approximately 0.36 per million spacer-bp × million contig-bp) in the semi-synthetic set. The non-planned match count decreased to 2,217 at hamming distance \u22642, 47 at hamming distance \u22641, and only 1 exact non-planned match at hamming distance = 0. 
For this rate under edit distance, the computional requirement of the exhustive tools restricted the analysis to the smaller simulated datasets only. Complete results, including per-distance breakdowns, validation analyses, and detailed comparison tables, are provided in Supplementary Note 3 and the distance_metric_analysis notebook in the project repository.
%%%%% maybe move this to results %%%%%

### Benchmarking framework {#sec-benchmark}

### Computational Resource and Runtime Tracking {#sec-resource-tracking}

While the primary focus of this study was to evaluate the ability of each tool to accurately identify spacer-protospacer matches, computational resource usage are a limiting factor for certain dataset sizes. This is particularly relevant for the exhustive tools (Sassy and indelfree.sh in bruteforce mode), which have high computational costs. 

The CLI benchmarking tool we developed utilises hyperfine [@Peter_hyperfine_2023] for local execution of tools (suitable for smaller datasets), and a SLURM (Simple Linux Utility for Resource Management) based method (suitable for larger datasets ran on HPC), where we captured the resource usage via SLURM's built-in accounting system using the sacct command, accessed through a custom Python wrapper (pyseff.py) [@pyseff]. For consistency, all analyses herein were performed using the SLURM tracking approch. Both approaches allow us to capture detailed resource usage metrics, including wall clock time, CPU time, and peak memory usage, which are critical for understanding the practical feasibility of each tool under different conditions. All SLURM job logs were retained for reproducibility and are available in the Zenodo repository. All tools were alocated the same CPU and memory resources (64 threads, 512 GB RAM) to ensure a fair comparison, and the same maximum wall time limit (72 hours) was applied to all runs. If a tool exceeded the wall time limit, it was terminated and marked as "timed out" for that dataset, and no results were recorded for that run. The only exception to this was blastn for the fraction_1 set, which was allowed to run to completion as it is the main point of reference with regards to historical use of (any) tool for this task.  
We note that memory (RAM) tracking for some java tools (particularly BBMap suite tools such as indelfree.sh) can seem to use all available memory as the java virtual machine may not explictly report cleared, unused memory, in a way visible to the SLURM accounting system. This means that the SLURM reported peak memory usage may not indicate the actual minimal requirement of a tool.
For tools that require generating an index file (or any additional obligatory steps and commands) prior the actual search/scan/alignment comomand, we include the index construction time (or the additional commands) in the total runtime of a tool as these represents real computational cost, though for production use with repeated searches, index construction might constitue a one-time cost. 


### Versioning and Reproducibility {#sec-reproducibility}

All tools were installed and managed using conda [@conda] (via the mamba [@mamba] package manager) in isolated environments. To prevent dependency conflicts and ensure reproducibility, most tools are installed in a dedicated environment. Environment activation time was excluded from performance measurements to focus on actual tool runtime.
The exact versions and configurations of all tools were recorded in environment files, allowing for exact replication of our testing environment. All benchmarks were performed on identical hardware configurations to ensure fair comparison.
%%% verbalise %%% 
``` json
{
    "os": "Linux",
    "kernel": "4.18.0-553.58.1.el8_10.x86_64",
    "architecture": "x86_64",
    "cpu": "AMD EPYC 7543 32-Core Processor X 2",
    "cpu_cores": "64",
    "cpu_threads": "64",
    "cpu_frequency": "3705.616 MHz",
    "cpu_cache_l3": "32768K",
    "ram": "512.0G",
    "ram_total_bytes": "549755813888",
    "filesystem": "nfs",
    "disk_model": "SAMSUNG MZ1LB1T9HALS-00007",
    "python_version": "3.10.19",
    "hyperfine_version": "1.19.0"
}
```
%%% verbalise %%%

### Extensibility

The framework is designed to be expandable through the integration of new tools. Each tool/software configuration is saved as a separate JSON file, which includes the exact commands and conda/mamba environment it uses. This configuration files can use placeholder variables which the main benchmarking software replaces (according to the users' CLI arguments) during execution (such as `{threads}`, `{contigs_file}`, `{spacers_file}`, `{output_dir}`, and `{results_dir}`). A new JSON file can be added manually or via bench.utils.tool_commands:add_tool function in a semi automated method. 



## Results

### Performance as a function of mismatch threshold {#sec-mismatch-performance}

![Performance of each tool as a function of mismatch threshold. The horizontal axis shows the number of allowed mismatches, while the vertical axis represents the mean detection fraction (0-1) aggregated across all spacer-contig pairs at a given mismatch threshold. Each color and shape indicates a different tool plot (shapes connected by lines for interpolation) Panel B shows the performance of the tools on the IMG/VR4 dataset, while panel A shows the performance of the tools on the synthetic dataset.](figures/main/tool_performance_by_mismatches.svg){#fig-tool-performance}

First we investigated potential tradeoffs and effects of the total edit distance (henceforth, interchangeable with mismatches) on the observed recall metric of the tools. Generally, the detection rate of each tool decreases as mismatch thresholds increase. Additionally, no single tool was able to identify all spacer occurrences, although at 0 mismatches the recall of bowtie1, bowtie2, blastn and mummer4 is approximately 0.99 (See supplementary table 2 for the recall values for each tool at different mismatch thresholds). At increased allowed mismatches, the tools showed more divergence, yet bowtie1 remained the single tool with the most unique matches by a considerable margin (@fig-tool-performance). Overall, the performance of the tools is similar between the synthetic and real datasets, albeit the overall lower sample size of the synthetic data should be considered when interpreting the results (see [table 1](#tbl-tools)).

### Performance as a function of query (spacer) abundance in reference database {#sec-abundance-performance}

![Comparison of recall (detection rate) across different mismatch thresholds and target abundance levels for IMG/VR v4 virus and spacer dataset. Top panel displays the subset of results with up to 1 mismatch, and the bottom panel displays the results with up to 3 mismatches. The horizontal axis shows the number of target occurrences on a logarithmic scale from 1 to 10^4^, while the vertical axis represents the mean detection fraction (0-1). Each color and shape indicates a different tool plot (shapes connected by lines for interpolation). The low-abundance region (1 - 1000 occurrences) is binned into logarithmically-spaced bins, while the high-abundance region (\>1000 occurrences) is divided into only 3 additional bins, as such ultra-high abundance sequences are rare. The detection fraction is the mean detection fraction across all spacer-contig pairs at a given mismatch threshold and target abundance level.](figures/main/recall_vs_occurrences_combined.svg){#fig-recall}

We then investigated if there are any potential effects for the number of times each protospacer sequence appears in the target set (i.e. the virus sequence set). For perfect matches (0 mismatches), bowtie1 demonstrates exceptional performance with recall rates consistently above 0.99 across all occurrence frequencies (Figure 2). Mummer4, bowtie2 and blastn all maintain a detection rate close to bowtie1. For low-occurrence spacers (1-10 occurrences), strobealign achieves detection rates of 95.44% but shows a systematic decline to approximately 20% for spacers occurring \>100 times, and further drops below 5% in the high occurrence range (\>1000).

When allowing one mismatch, the overall detection capabilities decrease across all tools, although Bowtie1 maintains its high performance. At up to three mismatches, the overall recall rates for all other tools further decrease, while Bowtie1 maintains detection rates above 97% throughout the occurrence spectrum.

The data shows a consistent pattern where detection rates generally decline for spacers with very high occurrence frequencies (\>1000), though this effect becomes less pronounced as more mismatches are permitted. Quantitatively, this decline is most evident in tools like strobealign and bbmap-skimmer, while bowtie1 maintains its high performance even with highly repetitive sequences. Detailed statistics and recall curves for exact mismatch values (rather than at a maximal value) can be found in the supplementary.

### Overall number of identified spacer-contig {#sec-pairwise}

![Tool vs Tool (pairwise) comparisons - set intersections and differences matrixes. The value of a cell(i,j) is number of spacer-contig pairs identified by the tool listed in row i, which were not identified by the tool listed in the j column. Panel A shows the results for the synthetic dataset, while panel B shows the results for the IMG/VR4 dataset.](figures/main/tool_comaprison_matrix.svg){#fig-pairwise}

The pairwise comparison of the tool results suggests (@fig-pairwise), reinforces the observation regarding bowtie1's unique ability to recover a maximum number of spacer matches. Generally, it appears that, when compared to any single other tool, the total number of contig-spacer pairs bowtie1 misses is relatively smaller than the number of pairs the compared tool identified which were not identified by bowtie1.

### Computational Resource Requirements and Scalability {#sec-resource-usage}

Computational resource requirements vary dramatically across tools, reflecting fundamental differences in algorithmic approaches and trade-offs between sensitivity and efficiency (@fig-resource-usage). Understanding these resource requirements is critical for practical tool selection, particularly as CRISPR spacer and viral databases continue to grow rapidly.

![Computational resource usage for selected tools on the synthetic dataset. Panel A shows peak memory usage (GB) vs wall clock time (minutes) on log-log scale, with point size indicating total CPU time. Panel B shows CPU time scaling with search space size (spacer-bp × contig-bp), demonstrating different computational complexity classes. Exhaustive tools (Sassy, indelfree bruteforce) are shown in red hues, while heuristic tools are in blue hues.](figures/main/resource_usage.svg){#fig-resource-usage}

**Exhaustive vs Heuristic Tool Performance:**

Unsuprisingly, we observed massive computational gap between exhaustive and heuristic approaches:

-   **Sassy (exhaustive, edit distance ≤5):** For the 5% IMG/VR4 HQ subsample (21,071 contigs, 715 Mbp), sassy required \~1M CPU seconds. Extrapolating to the full HQ benchmark dataset (421k contigs, 18.9 Gbp) would require \>20M CPU seconds (\~230 CPU-years) - computationally prohibitive for routine analysis.

-   **Indelfree.sh bruteforce mode (exhaustive, hamming distance):** while not as optimised or efficient as sassy, was only able to requiring \~100k CPU seconds for small subsamples. The indexed mode provides substantially better performance while maintaining hamming distance constraints.

-   **Heuristic tools (Bowtie1, Bowtie2, MMseqs2, BLASTn):** Orders of magnitude faster than exhaustive approaches. Bowtie1 completed the full IMG/VR4 HQ benchmark dataset (3.8M spacers × 421k contigs, 18.9 Gbp) in approximately 12 hours wall time with 16 cores, demonstrating practical scalability to large metagenomic datasets.

**Memory Requirements:**

Peak memory usage ranges from \<10GB (Bowtie1, StrobeAlign) to \>200GB (MMseqs2, Sassy on larger datasets). Some tools (particularly BBMap suite products like indelfree.sh) use available memory opportunistically rather than deterministically, making resource allocation challenging in shared computing environments.

**Scaling Behavior:**

Different tools exhibit different computational complexity patterns (@fig-resource-usage Panel B):

-   **Linear scaling:** Tools like Bowtie1 show near-linear scaling with search space size, making them suitable for ever-growing databases
-   **Superlinear scaling:** Some tools show worse-than-linear scaling, which may become prohibitive as databases continue doubling in size every few years
-   **Index construction overhead:** For indexed tools, the one-time cost of index construction should be considered separately from per-query search time

**Practical Implications:**

For large-scale metagenomic analyses involving millions of spacers and billions of bases of viral sequence:

1.  **Heuristic tools are essential:** Exhaustive approaches, while providing perfect recall and valuable for validation on small datasets, are computationally prohibitive for routine large-scale analysis.

2.  **Bowtie1 offers optimal balance:** Combining high recall (\>95% for ≤3 mismatches), low non-planned match rates at hamming distance ≤3, and excellent computational efficiency, making it the practical choice for most applications.

3.  **Exhaustive tools for validation:** Tools like Sassy and indelfree.sh bruteforce mode are valuable for establishing baseline performance on small subsamples and validating heuristic tool results, but not for production workflows.

These computational considerations reinforce our recommendation of Bowtie1 for most applications, with exhaustive tools reserved for specialized validation studies or small-scale analyses where their computational costs are acceptable.

## Discussion

### Tool Performance and Recommendations

Our analysis, combining synthetic datasets with known ground truth and real-world metagenomic data, reveals critical insights for CRISPR spacer-protospacer matching tool selection.

**Primary Finding - Hamming Distance vs Edit Distance Analysis:**

Our systematic comparison of hamming distance (substitutions only) versus edit distance (allowing indels) reveals profound implications for match specificity and biological relevance. Using both synthetic datasets with complete ground truth and semi-synthetic datasets combining real spacers with synthetic contigs, we quantified how distance metric choice affects non-planned match rates.

**Key Findings from Distance Metric Comparison:**

Using the semi-synthetic dataset (\~3.7M real spacers vs 400k synthetic contigs), we observed:

-   **Hamming distance ≤3:** 54,388 validated non-planned matches (\~0.36 per million spacer-bp × million contig-bp)
-   **Hamming distance ≤2:** 2,217 validated non-planned matches
-   **Hamming distance ≤1:** 47 validated non-planned matches
-   **Hamming distance = 0:** 1 exact match

When allowing indels (edit distance), non-planned match rates increase substantially, though the exact magnitude depends on allowed gap parameters. Using Sassy (the only tool with perfect recall supporting arbitrary edit distances) on the fully synthetic dataset, we confirmed that edit distance \>3 leads to dramatically higher rates of non-planned matches compared to hamming distance ≤3.

See Supplementary Note 3 and the distance_metric_analysis notebook for detailed methodology and complete results of this comparison.

**Biological Justification for Preferring Hamming Distance:**

Our preference for hamming distance is supported by multiple lines of evidence: (1) indels are \~4× rarer than substitutions in bacteria, (2) most experimental escape mutations are single substitutions, particularly in PAM-proximal regions, (3) phage genomes are coding-dense making frameshift-inducing indels often lethal, and (4) with sufficient sequencing depth, sequencing-induced indels should be rare in assembled contigs from Illumina data.

**Tool Selection Flowchart and Recommendations:**

Based on our comprehensive benchmarking, we provide the following evidence-based recommendations organized by use case. @fig-flowchart provides a decision flowchart to guide tool selection based on dataset characteristics and analysis goals.

```{mermaid}
%%| label: fig-flowchart
%%| fig-cap: "Tool selection decision flowchart for CRISPR spacer-protospacer matching. Decision nodes (diamonds) represent classification criteria based on dataset scale, computational resources, distance metric requirements, and biological constraints. Edge labels specify the conditions and reasoning for each path. Terminal nodes (rectangles) indicate recommended tools with performance characteristics and application domains. Bowtie1 (green) represents the primary recommendation for large-scale analyses with hamming distance ≤3. Indelfree.sh indexed (blue) provides extended hamming distance capability (>3 substitutions, up to ≤5). Sassy (orange) enables exhaustive edit distance search for small datasets or low-accuracy long-read data."

flowchart TD
    Start([Tool Selection for<br/>Spacer-Protospacer Matching]) --> Q1{Dataset Scale<br/>and Computational<br/>Resources}
    
    Q1 -->|Small experimental dataset<br/><1M spacers, <10 Gbp contigs<br/>Computational cost acceptable| Q2{Distance Metric<br/>Requirements}
    Q1 -->|Large-scale metagenomic<br/>>1M spacers or >10 Gbp contigs<br/>Efficiency critical| Q3{Distance Threshold<br/>and Biological<br/>Constraints}
    
    Q2 -->|Edit distance needed:<br/>Indel tolerance required for<br/>low-accuracy long reads: ONT R9, PacBio CLR<br/>or mutation type characterization| Sassy[<b>Sassy - Exhaustive Edit Distance</b><br/>✓ Perfect recall, arbitrary thresholds<br/>✓ Supports indels and substitutions<br/>⚠ ~1M CPU-seconds per 5% IMG/VR4<br/>Applications: Experimental phage-host studies,<br/>low-accuracy long-read assemblies,<br/>off-target analysis, methodological validation]
    Q2 -->|Hamming distance sufficient:<br/>Substitutions only<br/>High-quality assembled data: Illumina<br/>Historical infection inference| Q3
    
    Q3 -->|≤3 substitutions:<br/>Biologically relevant threshold<br/>Indels ~4× rarer than substitutions<br/>Most escape mutations ≤3 nt| Bowtie1[<b>Bowtie1 - Hamming ≤3</b><br/>✓ >99% recall at intended threshold<br/>✓ Scales to millions of spacers<br/>✓ Low non-planned match rate<br/>✓ Ungapped alignment only<br/><b>PRIMARY RECOMMENDATION</b><br/>Applications: Large-scale host prediction,<br/>metagenomic spacer-protospacer matching,<br/>high-throughput CRISPR target identification]
    Q3 -->|>3 substitutions:<br/>Extended divergence detection<br/>Accepts higher non-planned matches<br/>Conserved indel-free constraint| Indelfree[<b>Indelfree.sh Indexed - Hamming ≤5</b><br/>✓ Near-perfect recall for hamming ≤5<br/>✓ Maintains substitution-only constraint<br/>⚠ 10-100× slower than Bowtie1<br/>⚠ Higher non-planned match frequency<br/>Applications: Divergent phage detection,<br/>extended mismatch tolerance,<br/>sensitivity-prioritized analyses]
    
    style Bowtie1 fill:#90EE90,stroke:#228B22,stroke-width:3px
    style Sassy fill:#FFE4B5,stroke:#FF8C00,stroke-width:2px
    style Indelfree fill:#ADD8E6,stroke:#4169E1,stroke-width:2px
    style Start fill:#F0F0F0,stroke:#333,stroke-width:2px

```

**Detailed Recommendations by Use Case:**

1.  **Primary recommendation for most applications - Bowtie1 (hamming ≤3):**
    -   **Use for:** Large-scale metagenomic analyses, routine host-virus prediction, high-throughput spacer-protospacer matching
    -   **Performance:** \>95% recall for 0-3 mismatch spacers, \<1% false positive rate
    -   **Advantages:** Excellent computational efficiency, scales well to millions of spacers and billions of bases, maintains high performance even for high-abundance targets
    -   **Limitations:** Maximum 3 substitutions, does not support indels
    -   **Biological justification:** Most experimental escape mutations are ≤3 substitutions; higher thresholds increase false positives without substantial biological gain
2.  **Extended hamming distance - Indelfree.sh indexed mode (hamming ≤5):**
    -   **Use for:** Scenarios requiring detection up to 5 substitutions while maintaining hamming distance
    -   **Performance:** Good recall for 4-5 mismatch spacers without the false positive explosion of edit distance
    -   **Advantages:** Extends beyond bowtie1's 3-mismatch limitation while avoiding indel-associated false positives
    -   **Limitations:** More computationally intensive than bowtie1, still limited to substitutions only
    -   **When to use:** When analyzing divergent sequences or when increased sensitivity is needed beyond 3 mismatches
3.  **Sassy (edit distance) - Specific applications only:**
    -   **Use for:**
        -   Small datasets where computational cost is acceptable
        -   Experimental setups where mutation type (substitution vs indel) is of research interest
        -   Comparative studies of escape mutation types in controlled systems
        -   Methodological validation and establishing baseline performance
        -   Low-accuracy sequencing (e.g., Oxford Nanopore R9) where indel tolerance may be necessary
    -   **Performance:** Perfect recall (100%), supports arbitrary edit distances
    -   **Limitations:** Massive computational requirements (\~1M CPU seconds for 5% subsample),
    -   **Critical note:** We recommend sassy only for specialized applications where its unique capabilities (perfect recall + arbitrary edit distance) are essential and computational resources are available
4.  **BLASTn-short - Parameter-dependent performance:**
    -   **Use for:** Legacy compatibility, when familiarity with BLAST ecosystem is important
    -   **Critical considerations:** Performance heavily dependent on `-max_target_seqs` parameter; default value significantly impacts high-abundance spacer detection
    -   **Our analysis:** Used most sensitive parameters (lowest word size, high e-value, `-max_target_seqs 100000`)
    -   **Advantages:** Familiar to many researchers, well-documented
    -   **Limitations:** Lower recall than bowtie1, especially for high-abundance targets; parameter sensitivity requires careful configuration

**Algorithmic Insights:**

A key finding is that tools differ not because they are "bad" but because their algorithms solve different computational problems. Edit/affine-based algorithms (bowtie2, minimap2, BLAST, sassy) naturally report more matches than hamming-based ones (bowtie1, indelfree.sh) when using the same numeric threshold - this reflects fundamental algorithmic differences rather than tool quality. For CRISPR spacer-protospacer matching, where biological evidence strongly supports substitution-dominant mutation patterns, hamming-based approaches are more appropriate.

**Abundance Effects:**

Our findings reveal that within the mismatch thresholds tested (≤3), no single tool identified all spacer occurrences. The main performance differentiator is how tools handle high-abundance spacers (\>1000 occurrences). Bowtie1 maintains \>97% recall even for highly repetitive sequences, while other tools show systematic decline (e.g., strobealign drops to \<5% for \>1000 occurrences). This abundance sensitivity likely reflects tool-specific heuristics designed for different use cases (e.g., read mapping assumes reference sequences are not highly redundant).

**Practical Implications:**

Of specific concern is the relatively high number of alignments missed by blastn-short at default parameters, which is currently common in published analyses. Missing genuine spacer-protospacer pairs can significantly impact downstream conclusions about MGE host range, virus-host networks, and CRISPR system evolution. Our analysis suggests that many published studies using blastn-short may have substantially underestimated the number of spacer-protospacer matches, particularly for high-abundance targets.

**Context-Dependent Considerations:**

Experimental and analytical context must be considered when selecting tools:

-   **Large-scale meta-analyses:** Favor bowtie1 for high recall and computational efficiency. In these studies where spacers and targets may not co-occur (from different samples/environments), high recall is critical while acknowledging that similarity implies ancestral encounters rather than current infectivity.

-   **Experimental isolate studies:** When studying known phage-host pairs from isolates or temporally resolved samples, tools allowing higher mismatch tolerance may be appropriate, though biological justification for edit distance remains weak.

-   **CRISPR array context:** Spacers from complete arrays provide additional information (order, genomic location, host genome origin). Recent studies (Mitrofanov et al., Vink et al.) reveal system-specific spacer loss patterns and mismatch tolerance, which may inform post-search verification.

-   **Low-complexity filtering:** Regardless of tool choice, applying DUST masking or similar complexity filtering (e.g., ldust from minimap2, BBDuk from bbmap) prior to searching is prudent to reduce spurious matches from repetitive regions.

Another consideration should be the source of the spacer data: spacers sequences extracted from raw NGS data and spacers extracted from assembled CRISPR arrays (either from assembled or long read sequencing). Specifically, spacers from complete arrays present additional information, namely the location and order of the spacers within the array, and the observation they originate from the same host genome. Notably, a recent in-depth study by Mitrofanov et al @Mitrofanov2025, investigating the mutational landscape of repeats across many isolate prokaryote genomes, have identified patterns of spacer loss based on system sub/type and location. A similar meta-analysis of spacer mutations by Vink et al @Vink2021, have revealed that different CRISPR subtypes exhibit varying tolerance for mismatches within the spacer sequences, with most matched spacers containing three or fewer mismatched nucleotides. This aligns with our current general recommendation of using Bowtie1. Additionally, Vink et al observed that Type I-E and Type II systems preferentially target template strands while Type I-A, I-B, and Type III systems prefer coding strands, emphasizing system-specific characteristics which may also inform post-search verification methods (albeit this may require additional information, such as the sequences orientation or coding potential, and the CRISPR subtype of the spacer).

### Biological Interpretation and Potential False Positives

While our technical comparison focuses on tool performance, the biological interpretation of identified matches also requires careful consideration. The arms race between prokaryotes and MGEs creates a complex landscape where simple sequence matching may not directly translate to genuine host-parasite relationships. We identified several scenarios that could lead to false positive assignments:

1.  **Low Complexity Sequence Matches**: Independently of the tool choice, low-complexity (regions with highly skewed GC content, or composed of many repeated sequences) can be susceptible to spurious matches. Low complexity regions may be present in both the virus target set, or in the spacer set, where some non-CRISPR repeated sequences may have been misclassified as such. While certain tools employ filters and heuristics to mitigate the effect of low complexity regions, a prudent procedure should include a step separated from the search, to specifically identify, filter or mask the spacer and virus sets. Dustmasker [@Morgulis_2006], or a similar tool (e.g. ldust from minimap, or BBDuk from bbmap) could be used prior to the search.

2.  **Common Sequence Motifs**: Some matches may correspond to highly conserved sequences shared across various biological systems. For instance, horizontal gene transfer events can lead to the spread of similar sequences across diverse MGEs, potentially creating spurious matches that don't reflect direct host-MGE interactions. An example of potential HGT mediated matches was described by Kosmopoulos et al. 2023, where a transposon-mediated transferred of a phage lysin gene (to the host genome) created a true sequence similarity (which was verified by the authors using a combination of sequencing technologies) [@Kosmopoulos_2023]. Even if anecdotally observed, this suggests that an unknown number of observed "good" alignments may be due to HGT, which in the absence of additional information, could not be ruled out as a false positive.

3.  **Self-Targeting Events**: Some matches may represent CRISPR targeting of host genes [@Wimmer_2020]. Previous studies estimated a varying amount of these actually target sequences with putative exogenous origin such as prophages, ranging from \~50% [@Stern_2010], to \~80% [@Shmakov_2017]. In Shmakov et al., the authors estimate non-defence targeting is likely a rare event. So far, most observations of non-defence (or counter defence) molecular functions of CRISPRs did not directly involve the spacer sequences, but rather the Cas genes or related effectors. Some observed functions include genome remodelling and evolution, or temporal regulation of gene expression. For example, in *Francisella novicida*, Sampson et al. 2013 demonstrated that certain lipoprotein production is mediated by a CRISPR system [@Sampson_2013].

4.  **non-chromosomal replicon encoded CRISPRs**: Similarly to the potential of CRISPR systems to act in non-immune functions, in certain scenarios spacers may be acquired from non-MGE replicons, or be carried (even if partially) by mobile elements. While some types are known to be chromosomal, others are known to be carried entirely by plasmids (i.e. both the Cas proteins and the array loci are on the plasmid), for example in various halophilic archaea [@Maier_2018]. A recent study by Zhang et al [@Zhang_2025] observed similar phenomena in the human gut microbiome, specifically in *Bifidobacterium longum*. Another non-MGE targeting phenomena in archaea was described by Turgeman-Grott et al. [@Turgeman_Grott_2018], where inter-species spacers (targeting genes from related species) were demonstrated to be common in archaea, at least in the context of cellular mating. Another confounding factor is the potential of certain MGEs to target host genes, potentially for regulatory functions, or as counter-defense mechanisms[@Shmakov_2023]. Shmakov et al. 2023 have identified widespread CRISPR-derived phage-encoded mini-arrays, which can hijack and interfere with their host native system.

### Study Limitations

**Synthetic Dataset Characteristics:**

The synthetic data was generated with sequence characteristics matched to real biological sequences, including GC content matched to iPHoP spacer data (\~49%), simulated contig GC% and length distributions based on IMG/VR4 characteristics (\~46% GC), and verification that k-mer distributions and sequence complexity match real data (see supplementary notebook: spacer_inspection.ipynb). However, real biological sequences have additional complexities not fully captured, including locus-specific composition biases, regional variation in nucleotide frequencies, and evolutionary constraints that shape sequence structure. These differences should be considered when interpreting results, though our enhanced synthetic dataset provides substantially improved realism compared to purely random sequences.

**Distance Metric Focus:**

Our analysis primarily focuses on hamming distance (substitutions only) which we argue aligns with the underlying biological question. While we demonstrate that edit distance dramatically increases false positives (\>10% for edit \>3 vs \<1% for hamming ≤3), we acknowledge that rare cases exist where frame-preserving indels (multiples of 3 bp) could indicate genuine interactions under strong selection. We used sassy to comprehensively test edit distances up to 5, confirming our recommendation against routine use of edit distance for most applications due to prohibitive computational costs and false positive rates. For specialized applications requiring indel detection (e.g., low-accuracy long-read sequencing, experimental mutation-type studies), sassy provides perfect recall but at massive computational cost (\~1M CPU seconds for 5% subsample at 21k contigs, with ≤5 edits). We note that while long-read technologies are attractive for CRISPR arrays (which are repetitive and prone to short-read misassembly), alignment quality cannot exceed underlying sequencing accuracy.

**Sequencing Technology Considerations:**

Our recommendations are primarily based on Illumina-derived assembled sequences where sequencing-induced indels are rare with sufficient depth. For other technologies (Oxford Nanopore R9, PacBio CLR) or low-depth datasets using raw reads, different considerations may apply. However, we argue that even in these cases, biological indels remain rare compared to substitutions, and the false positive explosion with edit distance must be carefully weighed against potential benefits.

**Parameter Space and Tool Versions:**

Tools were not exhaustively tested across all possible parameter combinations, and further optimization may be possible for specific use cases. Several tools presented version-specific challenges: MMseqs2's latest release version had frequent crashes requiring use of the latest GitHub commit, and various index construction parameters (offset rate, seed selection) may affect performance in ways not fully explored. We focused on parameters maximizing sensitivity and avoiding artificial limitations on multiple match detection, representing realistic use cases rather than exhaustive parameter sweeps.

**Computational Resource Constraints:**

Exhaustive tools (Sassy, indelfree.sh bruteforce mode) were not run on the full HQ benchmark dataset due to prohibitive computational costs. Our extrapolations are based on smaller subsamples (ranging from 0.0005× to 0.1×, corresponding to 279 to 42,143 contigs), and actual performance on the full dataset might differ from these estimates. However, the consistent scaling patterns observed across subsample sizes suggest that our computational cost projections are reasonable approximations.

**Sample Size and Scale:**

The synthetic dataset is smaller than the real dataset, though this enabled controlled testing of rare scenarios like ultra-high spacer occurrence rates. We employed stratified subsampling of real data (IMG/VR4 HQ contigs at 0.0005, 0.001, 0.005, 0.01, 0.05, 0.1, and 1.0 fractions) to assess tool behavior across different database scales, finding consistent performance patterns suggesting our recommendations are robust across database sizes ranging from 279 contigs (7 Mbp) to 421,431 contigs (18.9 Gbp).

### Future Directions

Several research directions emerge from this work that could refine tool selection and improve spacer-protospacer matching accuracy. First, systematic experimental validation comparing substitution versus indel frequencies in phage escape mutations across diverse CRISPR-Cas systems would provide direct evidence for or against our hamming distance preference. While existing studies predominantly report substitutions [@Deveau2008; @Semenova2011; @Fineran2014], comprehensive quantitative comparisons across mutation types remain lacking.

Second, investigation of frame-preserving indels (3bp multiples) as indicators of strong selection in coding regions could reveal rare but biologically significant patterns. Such indels would maintain reading frame integrity while potentially disrupting CRISPR targeting, representing adaptations under particularly strong selective pressure.

Third, incorporation of CRISPR subtype information could refine matching strategies. Recent work by Mitrofanov et al. [@Mitrofanov2025] on spacer loss patterns and Vink et al. [@Vink2021] on mismatch tolerance differences across CRISPR subtypes suggests that system-specific characteristics could inform both search parameters and post-search verification. Type I-E and Type II systems preferentially target template strands while Type I-A, I-B, and Type III systems prefer coding strands, information that could enhance match validation when combined with strand orientation and coding potential analysis.

Fourth, development of purpose-built tools for spacer-protospacer matching that combine bowtie1-level computational efficiency with extended hamming distance support (≥4 substitutions) would fill a practical gap. Current options either sacrifice sensitivity (bowtie1 maximum 3 mismatches) or computational efficiency (indelfree.sh indexed mode).

Finally, integrated workflows incorporating complexity filtering, post-search verification using tools like parasail [@Daily2016_parasail], phylogenetic context analysis, and genomic location assessment would provide more robust inference frameworks. Such workflows could systematically address confounding factors including low-complexity sequences, horizontal gene transfer events [@Kosmopoulos_2023], self-targeting [@Wimmer_2020; @Stern_2010; @Shmakov_2017], non-defense functions [@Sampson_2013], plasmid-encoded CRISPR systems [@Maier_2018; @Zhang_2025], inter-species targeting [@Turgeman_Grott_2018], and phage-encoded mini-arrays [@Shmakov_2023].

## Conclusion

Our comprehensive comparison of spacer-protospacer search tools, combining synthetic datasets with known ground truth and real-world metagenomic data, reveals critical insights for tool selection and establishes evidence-based recommendations.

**Key Findings:**

1.  **Distance metric choice is critical:** Hamming distance (≤3 substitutions) maintains \<1% false positive rate while edit distance (\>3 edits) leads to \>10% false positives. Biological evidence strongly supports hamming distance: indels are \~4× rarer than substitutions, most escape mutations are single substitutions, and phage coding-dense genomes make frameshift-inducing indels often lethal.

2.  **Tool performance varies by algorithmic approach:** Tools differ not in quality but in the computational problems they solve. Edit/affine-based algorithms naturally report more matches than hamming-based ones - understanding these algorithmic differences is essential for appropriate tool selection.

3.  **Abundance sensitivity distinguishes tools:** The primary performance differentiator is handling high-abundance spacers (\>1000 occurrences). Bowtie1 maintains \>97% recall for highly repetitive sequences, while other tools show systematic decline due to heuristics designed for different use cases.

4.  **Current practices may miss many matches:** BLASTn-short at default parameters misses substantially more matches than bowtie1, particularly for high-abundance targets. Many published studies likely underestimate spacer-protospacer matches.

**Evidence-Based Recommendations:**

-   **Most applications:** Bowtie1 with hamming ≤3 (high recall, low false positives, excellent computational efficiency)
-   **Extended mismatches (4-5):** Indelfree.sh indexed mode (avoids edit distance false positive explosion)
-   **Specialized only:** Sassy for small datasets when mutation type matters or perfect recall is essential (computationally prohibitive for large-scale analysis)

**Broader Implications:**

The interpretation of spacer-protospacer matches requires careful consideration of biological context beyond tool performance: low-complexity sequences, horizontal gene transfer, self-targeting, and mobile element-encoded CRISPRs can all complicate straightforward interpretation. Proper workflow should include complexity filtering, post-search verification of alignment quality, and contextual analysis of genomic location and phylogeny.

Our findings emphasize that tool selection should be guided by understanding algorithmic assumptions and their alignment with biological expectations. As CRISPR spacer and viral databases continue growing rapidly (from 366,799 spacers in 2017 to 3,835,942 in 2023), choosing appropriate tools becomes increasingly critical for accurate host-MGE interaction inference.

We provide not just tool performance metrics but a framework for understanding why tools differ and how to select appropriate methods for specific research contexts. This work enables more accurate inference of virus-host relationships and CRISPR system evolution across diverse microbial ecosystems.

## Code and data availability

All code generated for this study can be found in the git repository: [code.jgi.doe.gov/spacersdb/spacer_matching_bench](http://code.jgi.doe.gov/spacersdb/spacer_matching_bench). All raw outputs (tool results on real and synthetic datasets, the simulated sequence files, the SLURM logs, and the hyperfine runtime measurements) are available on Zenodo [@zenodo_doi].

## Acknowledgements

Work conducted by the U.S. DOE Joint Genome Institute (https://ror.org/04xm1d337) (SR, UN, APC and BB), a DOE Office of Science User Facility, is supported by the Office of Science of the U.S. DOE operated under Contract DE-AC02-05CH11231.

We would like to thank the following people for their helpful feedback and suggestions: Uri Gophna, Georg Rath, and Ragnar Groot Koerkamp for valuable discussions on distance metrics and tool performance.